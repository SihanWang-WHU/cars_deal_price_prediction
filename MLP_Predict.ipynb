{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:44:33.422921500Z",
     "start_time": "2024-01-14T18:44:32.709923Z"
    }
   },
   "outputs": [],
   "source": [
    "# 绘图案例 an example of matplotlib\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import jn\n",
    "from IPython.display import display, clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#将kilometer当做类别变量处理试试,异常值用groupby处理,'匿名特征可以进一步处理一下'\n",
    "## 基础工具\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import jn\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "## 模型预测的\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "\n",
    "## 数据降维处理的\n",
    "from sklearn.decomposition import PCA,FastICA,FactorAnalysis,SparsePCA\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "## 参数搜索和评价的\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score,StratifiedKFold,train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import scipy.signal as signal"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:44:34.760011800Z",
     "start_time": "2024-01-14T18:44:32.719921800Z"
    }
   },
   "id": "960a4858d4e53369"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predefined Functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71d3ff038c610adf"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#处理异常值\n",
    "def smooth_cols(group,out_value,kind):\n",
    "    cols = ['power']\n",
    "    if kind == 'g':\n",
    "        for col in cols:\n",
    "            yes_no = (group[col]<out_value).astype('int')\n",
    "            new = yes_no * group[col]\n",
    "            group[col] = new.replace(0,group[col].quantile(q=0.995))\n",
    "        return group\n",
    "    if kind == 'l':\n",
    "        for col in cols:\n",
    "            yes_no = (group[col]>out_value).astype('int')\n",
    "            new = yes_no * group[col]\n",
    "            group[col] = new.replace(0,group[col].quantile(q=0.07))\n",
    "        return group   \n",
    "    \n",
    "def date_proc(x):\n",
    "    m = int(x[4:6])\n",
    "    if m == 0:\n",
    "        m = 1\n",
    "    return x[:4] + '-' + str(m) + '-' + x[6:]\n",
    "\n",
    "#定义日期提取函数\n",
    "def date_tran(df,fea_col):\n",
    "    for f in tqdm(fea_col):\n",
    "        df[f] = pd.to_datetime(df[f].astype('str').apply(date_proc))\n",
    "        df[f + '_year'] = df[f].dt.year\n",
    "        df[f + '_month'] = df[f].dt.month\n",
    "        df[f + '_day'] = df[f].dt.day\n",
    "        df[f + '_dayofweek'] = df[f].dt.dayofweek\n",
    "    return (df)\n",
    "\n",
    "#分桶操作\n",
    "def cut_group(df,cols,num_bins=50):\n",
    "    for col in cols:\n",
    "        all_range = int(df[col].max()-df[col].min())\n",
    "        bin = [i*all_range/num_bins for i in range(all_range)]\n",
    "        df[col+'_bin'] = pd.cut(df[col], bin, labels=False)\n",
    "    return df\n",
    "\n",
    "### count编码\n",
    "def count_coding(df,fea_col):\n",
    "    for f in fea_col:\n",
    "        df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "    return(df)\n",
    "\n",
    "#定义交叉特征统计\n",
    "def cross_cat_num(df,num_col,cat_col):\n",
    "    for f1 in tqdm(cat_col):\n",
    "        g = df.groupby(f1, as_index=False)\n",
    "        for f2 in tqdm(num_col):\n",
    "            feat = g[f2].agg({\n",
    "                '{}_{}_max'.format(f1, f2): 'max', '{}_{}_min'.format(f1, f2): 'min',\n",
    "                '{}_{}_median'.format(f1, f2): 'median',\n",
    "            })\n",
    "            df = df.merge(feat, on=f1, how='left')\n",
    "    return(df)\n",
    "\n",
    "### 类别特征的二阶交叉\n",
    "from scipy.stats import entropy\n",
    "def cross_qua_cat_num(df):\n",
    "    for f_pair in tqdm([\n",
    "        ['model', 'brand'], ['model', 'regionCode'], ['brand', 'regionCode']\n",
    "    ]):\n",
    "        ### 共现次数\n",
    "        df['_'.join(f_pair) + '_count'] = df.groupby(f_pair)['SaleID'].transform('count')\n",
    "        ### n unique、熵\n",
    "        df = df.merge(df.groupby(f_pair[0], as_index=False)[f_pair[1]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[0], f_pair[1]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[0], f_pair[1]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[0], how='left')\n",
    "        df = df.merge(df.groupby(f_pair[1], as_index=False)[f_pair[0]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[1], f_pair[0]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[1], f_pair[0]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[1], how='left')\n",
    "        ### 比例偏好\n",
    "        df['{}_in_{}_prop'.format(f_pair[0], f_pair[1])] = df['_'.join(f_pair) + '_count'] / df[f_pair[1] + '_count']\n",
    "        df['{}_in_{}_prop'.format(f_pair[1], f_pair[0])] = df['_'.join(f_pair) + '_count'] / df[f_pair[0] + '_count']\n",
    "    return (df)\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:44:34.764009900Z",
     "start_time": "2024-01-14T18:44:34.292966500Z"
    }
   },
   "id": "476f1db95fded050"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b6888b17d8d5c42"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 37200128.00 MB\n",
      "Memory usage after optimization is: 10200260.00 MB\n",
      "Decreased by 72.6%\n",
      "Memory usage of dataframe is 12000128.00 MB\n",
      "Memory usage after optimization is: 3200260.00 MB\n",
      "Decreased by 73.3%\n",
      "Train data shape: (150000, 31)\n",
      "TestA data shape: (50000, 30)\n",
      "concat_data shape: (200000, 31)\n"
     ]
    }
   ],
   "source": [
    "## 通过Pandas对于数据进行读取 (pandas是一个很友好的数据读取函数库)\n",
    "Train_data = reduce_mem_usage(pd.read_csv('./data/used_car_train_20200313.csv', sep=' '))\n",
    "TestA_data = reduce_mem_usage(pd.read_csv('./data/used_car_testB_20200421.csv', sep=' '))\n",
    "\n",
    "#Train_data = Train_data[Train_data['price']>100]\n",
    "#Train_data['price'] = np.log1p(Train_data['price'])\n",
    "## 输出数据的大小信息\n",
    "print('Train data shape:',Train_data.shape)\n",
    "print('TestA data shape:',TestA_data.shape)\n",
    "\n",
    "\n",
    "#合并数据集\n",
    "concat_data = pd.concat([Train_data,TestA_data])\n",
    "concat_data['notRepairedDamage'] = concat_data['notRepairedDamage'].replace('-',0).astype('float16')\n",
    "concat_data = concat_data.fillna(concat_data.mode().iloc[0,:])\n",
    "print('concat_data shape:',concat_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:44:38.807319Z",
     "start_time": "2024-01-14T18:44:34.564011900Z"
    }
   },
   "id": "4e26bd2a52d5f947"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#截断异常值\n",
    "concat_data['power'][concat_data['power']>600] = 600\n",
    "concat_data['power'][concat_data['power']<1] = 1\n",
    "\n",
    "concat_data['v_13'][concat_data['v_13']>6] = 6\n",
    "concat_data['v_14'][concat_data['v_14']>4] = 4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:44:38.992320800Z",
     "start_time": "2024-01-14T18:44:38.961316900Z"
    }
   },
   "id": "60d13082b4b937f6"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(200000, 353)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in ['v_' +str(i) for i in range(14)]:\n",
    "    for j in ['v_' +str(i) for i in range(14)]:\n",
    "        concat_data[str(i)+'+'+str(j)] = concat_data[str(i)]+concat_data[str(j)]\n",
    "for i in ['model','brand', 'bodyType', 'fuelType','gearbox', 'power', 'kilometer', 'notRepairedDamage', 'regionCode']:\n",
    "    for j in ['v_' +str(i) for i in range(14)]:\n",
    "        concat_data[str(i)+'*'+str(j)] = concat_data[i]*concat_data[j]    \n",
    "concat_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:44:49.698660100Z",
     "start_time": "2024-01-14T18:44:39.153318100Z"
    }
   },
   "id": "b538a870da7ff463"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "#提取日期信息\n",
    "date_cols = ['regDate', 'creatDate']\n",
    "concat_data = date_tran(concat_data,date_cols)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:44:50.966377500Z",
     "start_time": "2024-01-14T18:44:49.695125300Z"
    }
   },
   "id": "716ee76142c4f824"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "data = concat_data.copy()\n",
    "\n",
    "#count编码\n",
    "count_list = ['regDate', 'creatDate', 'model', 'brand', 'regionCode','bodyType','fuelType','name','regDate_year', 'regDate_month', 'regDate_day', 'regDate_dayofweek' , 'creatDate_month','creatDate_day', 'creatDate_dayofweek','kilometer']\n",
    "       \n",
    "data = count_coding(data,count_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:44:51.525372600Z",
     "start_time": "2024-01-14T18:44:50.961377400Z"
    }
   },
   "id": "bbc86cabfa5d939d"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#特征构造\n",
    "# 使用时间：data['creatDate'] - data['regDate']，反应汽车使用时间，一般来说价格与使用时间成反比\n",
    "# 不过要注意，数据里有时间出错的格式，所以我们需要 errors='coerce'\n",
    "data['used_time1'] = (pd.to_datetime(data['creatDate'], format='%Y%m%d', errors='coerce') - \n",
    "                            pd.to_datetime(data['regDate'], format='%Y%m%d', errors='coerce')).dt.days\n",
    "data['used_time2'] = (pd.datetime.now() - pd.to_datetime(data['regDate'], format='%Y%m%d', errors='coerce')).dt.days                        \n",
    "data['used_time3'] = (pd.datetime.now() - pd.to_datetime(data['creatDate'], format='%Y%m%d', errors='coerce') ).dt.days\n",
    "\n",
    "#分桶操作\n",
    "cut_cols = ['power']+['used_time1','used_time2','used_time3']\n",
    "data = cut_group(data,cut_cols,50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:44:51.681376900Z",
     "start_time": "2024-01-14T18:44:51.509377600Z"
    }
   },
   "id": "1a5b315466fe5137"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  1.71it/s]\u001B[A\n",
      " 33%|███▎      | 2/6 [00:01<00:02,  1.47it/s]\u001B[A\n",
      " 50%|█████     | 3/6 [00:02<00:02,  1.14it/s]\u001B[A\n",
      " 67%|██████▋   | 4/6 [00:03<00:01,  1.05it/s]\u001B[A\n",
      " 83%|████████▎ | 5/6 [00:05<00:01,  1.16s/it]\u001B[A\n",
      "100%|██████████| 6/6 [00:05<00:00,  1.05it/s]\u001B[A\n",
      " 33%|███▎      | 1/3 [00:05<00:11,  5.74s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  2.07it/s]\u001B[A\n",
      " 33%|███▎      | 2/6 [00:01<00:02,  1.80it/s]\u001B[A\n",
      " 50%|█████     | 3/6 [00:02<00:02,  1.06it/s]\u001B[A\n",
      " 67%|██████▋   | 4/6 [00:03<00:01,  1.22it/s]\u001B[A\n",
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.32it/s]\u001B[A\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.37it/s]\u001B[A\n",
      " 67%|██████▋   | 2/3 [00:10<00:04,  4.94s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  1.96it/s]\u001B[A\n",
      " 33%|███▎      | 2/6 [00:01<00:02,  1.86it/s]\u001B[A\n",
      " 50%|█████     | 3/6 [00:01<00:01,  1.79it/s]\u001B[A\n",
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.72it/s]\u001B[A\n",
      " 83%|████████▎ | 5/6 [00:02<00:00,  1.70it/s]\u001B[A\n",
      "100%|██████████| 6/6 [00:03<00:00,  1.73it/s]\u001B[A\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.54s/it]\n"
     ]
    }
   ],
   "source": [
    "### 用数值特征对类别特征做统计刻画，随便挑了几个跟price相关性最高的匿名特征\n",
    "cross_cat = ['model', 'brand','regDate_year']\n",
    "cross_num = ['v_0','v_3', 'v_4', 'v_8', 'v_12','power']\n",
    "data = cross_cat_num(data,cross_num,cross_cat)#一阶交叉\n",
    "#data = cross_qua_cat_num(data)#二阶交叉"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:45:05.356394700Z",
     "start_time": "2024-01-14T18:44:51.676376100Z"
    }
   },
   "id": "1e7018f8e7b1c1c6"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SaleID', 'name', 'regDate', 'model', 'brand', 'bodyType', 'fuelType',\n",
      "       'gearbox', 'power', 'kilometer',\n",
      "       ...\n",
      "       'regDate_year_v_4_median', 'regDate_year_v_8_max',\n",
      "       'regDate_year_v_8_min', 'regDate_year_v_8_median',\n",
      "       'regDate_year_v_12_max', 'regDate_year_v_12_min',\n",
      "       'regDate_year_v_12_median', 'regDate_year_power_max',\n",
      "       'regDate_year_power_min', 'regDate_year_power_median'],\n",
      "      dtype='object', length=438)\n"
     ]
    }
   ],
   "source": [
    "## 选择特征列\n",
    "numerical_cols = data.columns\n",
    "print(numerical_cols)\n",
    "\n",
    "cat_fea = ['SaleID','offerType','seller']\n",
    "feature_cols = [col for col in numerical_cols if col not in cat_fea]\n",
    "feature_cols = [col for col in feature_cols if col not in ['price']]\n",
    "\n",
    "## 提前特征列，标签列构造训练样本和测试样本\n",
    "X_data = data.iloc[:len(Train_data),:][feature_cols]\n",
    "Y_data = Train_data['price']\n",
    "X_test  = data.iloc[len(Train_data):,:][feature_cols]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:45:05.652611100Z",
     "start_time": "2024-01-14T18:45:05.355394100Z"
    }
   },
   "id": "8b9dc722ccd3c43"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from meanencoder import MeanEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:45:05.668610500Z",
     "start_time": "2024-01-14T18:45:05.655611800Z"
    }
   },
   "id": "e586f6128e46d1b4"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class_list = ['model','brand','name','regionCode']+date_cols\n",
    "MeanEnocodeFeature = class_list#声明需要平均数编码的特征\n",
    "ME = MeanEncoder(MeanEnocodeFeature,target_type='regression') #声明平均数编码的类\n",
    "X_data = ME.fit_transform(X_data,Y_data)#对训练数据集的X和y进行拟合\n",
    "#x_train_fav = ME.fit_transform(x_train,y_train_fav)#对训练数据集的X和y进行拟合\n",
    "X_test = ME.transform(X_test)#对测试集进行编码"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:45:37.863989100Z",
     "start_time": "2024-01-14T18:45:05.674612700Z"
    }
   },
   "id": "4ac8932222ce8d94"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "X_data['price'] = Train_data['price']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:45:37.886987600Z",
     "start_time": "2024-01-14T18:45:37.864988400Z"
    }
   },
   "id": "5d7fd3ad324321a7"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:35<00:00,  5.95s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "### target encoding目标编码，回归场景相对来说做目标编码的选择更多，不仅可以做均值编码，还可以做标准差编码、中位数编码等\n",
    "enc_cols = []\n",
    "stats_default_dict = {\n",
    "    'max': X_data['price'].max(),\n",
    "    'min': X_data['price'].min(),\n",
    "    'median': X_data['price'].median(),\n",
    "    'mean': X_data['price'].mean(),\n",
    "    'sum': X_data['price'].sum(),\n",
    "    'std': X_data['price'].std(),\n",
    "    'skew': X_data['price'].skew(),\n",
    "    'kurt': X_data['price'].kurt(),\n",
    "    'mad': X_data['price'].mad()\n",
    "}\n",
    "### 暂且选择这三种编码\n",
    "enc_stats = ['max','min','mean']\n",
    "skf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for f in tqdm(['regionCode','brand','regDate_year','creatDate_year','kilometer','model']):\n",
    "    enc_dict = {}\n",
    "    for stat in enc_stats:\n",
    "        enc_dict['{}_target_{}'.format(f, stat)] = stat\n",
    "        X_data['{}_target_{}'.format(f, stat)] = 0\n",
    "        X_test['{}_target_{}'.format(f, stat)] = 0\n",
    "        enc_cols.append('{}_target_{}'.format(f, stat))\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(X_data, Y_data)):\n",
    "        trn_x, val_x = X_data.iloc[trn_idx].reset_index(drop=True), X_data.iloc[val_idx].reset_index(drop=True)\n",
    "        enc_df = trn_x.groupby(f, as_index=False)['price'].agg(enc_dict)\n",
    "        val_x = val_x[[f]].merge(enc_df, on=f, how='left')\n",
    "        test_x = X_test[[f]].merge(enc_df, on=f, how='left')\n",
    "        for stat in enc_stats:\n",
    "            val_x['{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "            test_x['{}_target_{}'.format(f, stat)] = test_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "            X_data.loc[val_idx, '{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].values \n",
    "            X_test['{}_target_{}'.format(f, stat)] += test_x['{}_target_{}'.format(f, stat)].values / skf.n_splits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:46:13.635732400Z",
     "start_time": "2024-01-14T18:45:37.879987700Z"
    }
   },
   "id": "d32ff18bd1a69442"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(150000, 454)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_list = ['regDate', 'creatDate','brand_power_min', 'regDate_year_power_min']\n",
    "x_train = X_data.drop(drop_list+['price'],axis=1)\n",
    "x_test = X_test.drop(drop_list,axis=1)\n",
    "x_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:46:13.830734Z",
     "start_time": "2024-01-14T18:46:13.626712900Z"
    }
   },
   "id": "e15c481563a7b0ae"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:46:14.093761100Z",
     "start_time": "2024-01-14T18:46:13.833731Z"
    }
   },
   "id": "b9ad7c25893a68c4"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#特征归一化\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(pd.concat([x_train,x_test]).values)\n",
    "all_data = min_max_scaler.transform(pd.concat([x_train,x_test]).values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:46:16.127733600Z",
     "start_time": "2024-01-14T18:46:14.097731500Z"
    }
   },
   "id": "f49f25a990708417"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=146)\n",
    "all_pca = pca.fit_transform(all_data)\n",
    "X_pca = all_pca[:len(x_train)]\n",
    "test = all_pca[len(x_train):]\n",
    "y = Train_data['price'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:46:30.604973600Z",
     "start_time": "2024-01-14T18:46:16.134731800Z"
    }
   },
   "id": "8c0231af56293e43"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from model import NN_model\n",
    "from evaluation import Metric\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import LearningRateScheduler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:46:38.594846200Z",
     "start_time": "2024-01-14T18:46:30.604973600Z"
    }
   },
   "id": "9334634e54ecff44"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def scheduler(epoch):\n",
    "    # 每隔100个epoch，学习率减小为原来的1/10\n",
    "    if epoch % 20 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.6)\n",
    "        print(\"lr changed to {}\".format(lr * 0.6))\n",
    "    return K.get_value(model.optimizer.lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:46:38.614845100Z",
     "start_time": "2024-01-14T18:46:38.596844500Z"
    }
   },
   "id": "9fe99c4084510300"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.optimizers import Adam"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:48:32.053498200Z",
     "start_time": "2024-01-14T18:48:31.999495Z"
    }
   },
   "id": "5307ef69b8f7d9aa"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "X_train shape: (125000, 146)\n",
      "y_train shape: (125000,)\n",
      "X_val shape: (25000, 146)\n",
      "y_val shape: (25000,)\n",
      "Epoch 1/145\n",
      "63/63 - 1s - loss: 2500.7583 - mae: 2500.7583 - val_loss: 879.2311 - val_mae: 879.2311\n",
      "Epoch 2/145\n",
      "63/63 - 0s - loss: 764.5579 - mae: 764.5579 - val_loss: 670.8928 - val_mae: 670.8928\n",
      "Epoch 3/145\n",
      "63/63 - 0s - loss: 676.4959 - mae: 676.4959 - val_loss: 683.1513 - val_mae: 683.1513\n",
      "Epoch 4/145\n",
      "63/63 - 0s - loss: 638.1234 - mae: 638.1234 - val_loss: 581.0860 - val_mae: 581.0860\n",
      "Epoch 5/145\n",
      "63/63 - 0s - loss: 590.7853 - mae: 590.7853 - val_loss: 554.6421 - val_mae: 554.6421\n",
      "Epoch 6/145\n",
      "63/63 - 0s - loss: 625.6236 - mae: 625.6236 - val_loss: 592.7914 - val_mae: 592.7914\n",
      "Epoch 7/145\n",
      "63/63 - 0s - loss: 592.7852 - mae: 592.7852 - val_loss: 529.6412 - val_mae: 529.6412\n",
      "Epoch 8/145\n",
      "63/63 - 0s - loss: 538.6637 - mae: 538.6637 - val_loss: 534.0087 - val_mae: 534.0087\n",
      "Epoch 9/145\n",
      "63/63 - 0s - loss: 576.1797 - mae: 576.1797 - val_loss: 619.3561 - val_mae: 619.3561\n",
      "Epoch 10/145\n",
      "63/63 - 1s - loss: 515.5005 - mae: 515.5005 - val_loss: 493.7594 - val_mae: 493.7594\n",
      "Epoch 11/145\n",
      "63/63 - 1s - loss: 566.5837 - mae: 566.5837 - val_loss: 519.7325 - val_mae: 519.7325\n",
      "Epoch 12/145\n",
      "63/63 - 1s - loss: 551.7820 - mae: 551.7820 - val_loss: 481.8512 - val_mae: 481.8512\n",
      "Epoch 13/145\n",
      "63/63 - 1s - loss: 492.5789 - mae: 492.5789 - val_loss: 500.7578 - val_mae: 500.7578\n",
      "Epoch 14/145\n",
      "63/63 - 1s - loss: 498.9525 - mae: 498.9525 - val_loss: 483.5874 - val_mae: 483.5874\n",
      "Epoch 15/145\n",
      "63/63 - 1s - loss: 490.8061 - mae: 490.8061 - val_loss: 503.9397 - val_mae: 503.9397\n",
      "Epoch 16/145\n",
      "63/63 - 1s - loss: 482.6483 - mae: 482.6483 - val_loss: 476.7973 - val_mae: 476.7973\n",
      "Epoch 17/145\n",
      "63/63 - 1s - loss: 515.9646 - mae: 515.9646 - val_loss: 537.8856 - val_mae: 537.8856\n",
      "Epoch 18/145\n",
      "63/63 - 1s - loss: 505.8472 - mae: 505.8472 - val_loss: 473.8336 - val_mae: 473.8336\n",
      "Epoch 19/145\n",
      "63/63 - 0s - loss: 477.4512 - mae: 477.4512 - val_loss: 495.8503 - val_mae: 495.8503\n",
      "Epoch 20/145\n",
      "63/63 - 0s - loss: 476.6831 - mae: 476.6831 - val_loss: 495.7874 - val_mae: 495.7874\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 0s - loss: 454.2778 - mae: 454.2778 - val_loss: 446.2306 - val_mae: 446.2306\n",
      "Epoch 22/145\n",
      "63/63 - 0s - loss: 445.7646 - mae: 445.7646 - val_loss: 445.0555 - val_mae: 445.0555\n",
      "Epoch 23/145\n",
      "63/63 - 0s - loss: 443.3643 - mae: 443.3643 - val_loss: 453.4269 - val_mae: 453.4269\n",
      "Epoch 24/145\n",
      "63/63 - 0s - loss: 447.4929 - mae: 447.4929 - val_loss: 440.3438 - val_mae: 440.3438\n",
      "Epoch 25/145\n",
      "63/63 - 0s - loss: 446.8319 - mae: 446.8319 - val_loss: 442.1158 - val_mae: 442.1158\n",
      "Epoch 26/145\n",
      "63/63 - 0s - loss: 444.6861 - mae: 444.6861 - val_loss: 438.1890 - val_mae: 438.1890\n",
      "Epoch 27/145\n",
      "63/63 - 0s - loss: 442.7704 - mae: 442.7704 - val_loss: 448.5179 - val_mae: 448.5179\n",
      "Epoch 28/145\n",
      "63/63 - 0s - loss: 442.8922 - mae: 442.8922 - val_loss: 455.6725 - val_mae: 455.6725\n",
      "Epoch 29/145\n",
      "63/63 - 0s - loss: 459.8318 - mae: 459.8318 - val_loss: 445.4896 - val_mae: 445.4896\n",
      "Epoch 30/145\n",
      "63/63 - 0s - loss: 440.9444 - mae: 440.9444 - val_loss: 444.8864 - val_mae: 444.8864\n",
      "Epoch 31/145\n",
      "63/63 - 0s - loss: 438.9554 - mae: 438.9554 - val_loss: 468.6265 - val_mae: 468.6265\n",
      "Epoch 32/145\n",
      "63/63 - 0s - loss: 444.5299 - mae: 444.5299 - val_loss: 474.6803 - val_mae: 474.6803\n",
      "Epoch 33/145\n",
      "63/63 - 0s - loss: 448.4815 - mae: 448.4815 - val_loss: 463.3756 - val_mae: 463.3756\n",
      "Epoch 34/145\n",
      "63/63 - 0s - loss: 445.2890 - mae: 445.2890 - val_loss: 444.7886 - val_mae: 444.7886\n",
      "Epoch 35/145\n",
      "63/63 - 0s - loss: 435.3122 - mae: 435.3122 - val_loss: 439.5027 - val_mae: 439.5027\n",
      "Epoch 36/145\n",
      "63/63 - 0s - loss: 454.6312 - mae: 454.6312 - val_loss: 441.1696 - val_mae: 441.1696\n",
      "Epoch 37/145\n",
      "63/63 - 0s - loss: 439.0497 - mae: 439.0497 - val_loss: 452.5379 - val_mae: 452.5379\n",
      "Epoch 38/145\n",
      "63/63 - 0s - loss: 445.7774 - mae: 445.7774 - val_loss: 432.9557 - val_mae: 432.9557\n",
      "Epoch 39/145\n",
      "63/63 - 0s - loss: 449.1752 - mae: 449.1752 - val_loss: 485.9541 - val_mae: 485.9541\n",
      "Epoch 40/145\n",
      "63/63 - 0s - loss: 448.9429 - mae: 448.9429 - val_loss: 438.5130 - val_mae: 438.5130\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 1s - loss: 418.6490 - mae: 418.6490 - val_loss: 428.4955 - val_mae: 428.4955\n",
      "Epoch 42/145\n",
      "63/63 - 0s - loss: 414.7099 - mae: 414.7099 - val_loss: 434.7173 - val_mae: 434.7173\n",
      "Epoch 43/145\n",
      "63/63 - 0s - loss: 414.9615 - mae: 414.9615 - val_loss: 435.8259 - val_mae: 435.8259\n",
      "Epoch 44/145\n",
      "63/63 - 0s - loss: 415.7556 - mae: 415.7556 - val_loss: 431.9384 - val_mae: 431.9384\n",
      "Epoch 45/145\n",
      "63/63 - 1s - loss: 413.6724 - mae: 413.6724 - val_loss: 427.8058 - val_mae: 427.8058\n",
      "Epoch 46/145\n",
      "63/63 - 0s - loss: 412.8196 - mae: 412.8196 - val_loss: 426.6877 - val_mae: 426.6877\n",
      "Epoch 47/145\n",
      "63/63 - 0s - loss: 414.9850 - mae: 414.9850 - val_loss: 427.9342 - val_mae: 427.9342\n",
      "Epoch 48/145\n",
      "63/63 - 1s - loss: 415.8923 - mae: 415.8923 - val_loss: 421.6732 - val_mae: 421.6732\n",
      "Epoch 49/145\n",
      "63/63 - 0s - loss: 409.9840 - mae: 409.9840 - val_loss: 422.2043 - val_mae: 422.2043\n",
      "Epoch 50/145\n",
      "63/63 - 0s - loss: 409.8280 - mae: 409.8280 - val_loss: 428.6851 - val_mae: 428.6851\n",
      "Epoch 51/145\n",
      "63/63 - 0s - loss: 428.1906 - mae: 428.1906 - val_loss: 425.7345 - val_mae: 425.7345\n",
      "Epoch 52/145\n",
      "63/63 - 0s - loss: 409.6419 - mae: 409.6419 - val_loss: 422.6743 - val_mae: 422.6743\n",
      "Epoch 53/145\n",
      "63/63 - 1s - loss: 408.8162 - mae: 408.8162 - val_loss: 425.2577 - val_mae: 425.2577\n",
      "Epoch 54/145\n",
      "63/63 - 1s - loss: 406.7291 - mae: 406.7291 - val_loss: 432.3286 - val_mae: 432.3286\n",
      "Epoch 55/145\n",
      "63/63 - 1s - loss: 406.7917 - mae: 406.7917 - val_loss: 424.8878 - val_mae: 424.8878\n",
      "Epoch 56/145\n",
      "63/63 - 1s - loss: 407.8917 - mae: 407.8917 - val_loss: 422.9707 - val_mae: 422.9707\n",
      "Epoch 57/145\n",
      "63/63 - 1s - loss: 410.3436 - mae: 410.3436 - val_loss: 514.8458 - val_mae: 514.8458\n",
      "Epoch 58/145\n",
      "63/63 - 1s - loss: 419.0243 - mae: 419.0243 - val_loss: 418.5896 - val_mae: 418.5896\n",
      "Epoch 59/145\n",
      "63/63 - 0s - loss: 402.3003 - mae: 402.3003 - val_loss: 423.9207 - val_mae: 423.9207\n",
      "Epoch 60/145\n",
      "63/63 - 0s - loss: 404.3351 - mae: 404.3351 - val_loss: 422.7102 - val_mae: 422.7102\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 0s - loss: 397.7352 - mae: 397.7352 - val_loss: 414.8972 - val_mae: 414.8972\n",
      "Epoch 62/145\n",
      "63/63 - 0s - loss: 395.5131 - mae: 395.5131 - val_loss: 414.5469 - val_mae: 414.5469\n",
      "Epoch 63/145\n",
      "63/63 - 0s - loss: 399.6024 - mae: 399.6024 - val_loss: 419.3710 - val_mae: 419.3710\n",
      "Epoch 64/145\n",
      "63/63 - 0s - loss: 397.0000 - mae: 397.0000 - val_loss: 416.8503 - val_mae: 416.8503\n",
      "Epoch 65/145\n",
      "63/63 - 0s - loss: 393.2121 - mae: 393.2121 - val_loss: 413.8925 - val_mae: 413.8925\n",
      "Epoch 66/145\n",
      "63/63 - 0s - loss: 393.7795 - mae: 393.7795 - val_loss: 419.6320 - val_mae: 419.6320\n",
      "Epoch 67/145\n",
      "63/63 - 0s - loss: 393.0579 - mae: 393.0579 - val_loss: 417.0415 - val_mae: 417.0415\n",
      "Epoch 68/145\n",
      "63/63 - 0s - loss: 392.0052 - mae: 392.0052 - val_loss: 415.8204 - val_mae: 415.8204\n",
      "Epoch 69/145\n",
      "63/63 - 0s - loss: 390.7809 - mae: 390.7809 - val_loss: 430.2420 - val_mae: 430.2420\n",
      "Epoch 70/145\n",
      "63/63 - 0s - loss: 394.0241 - mae: 394.0241 - val_loss: 416.7377 - val_mae: 416.7377\n",
      "Epoch 71/145\n",
      "63/63 - 0s - loss: 392.7841 - mae: 392.7841 - val_loss: 415.3221 - val_mae: 415.3221\n",
      "Epoch 72/145\n",
      "63/63 - 0s - loss: 390.1023 - mae: 390.1023 - val_loss: 416.9193 - val_mae: 416.9193\n",
      "Epoch 73/145\n",
      "63/63 - 0s - loss: 390.4483 - mae: 390.4483 - val_loss: 415.2493 - val_mae: 415.2493\n",
      "Epoch 74/145\n",
      "63/63 - 0s - loss: 390.2420 - mae: 390.2420 - val_loss: 419.5525 - val_mae: 419.5525\n",
      "Epoch 75/145\n",
      "63/63 - 0s - loss: 389.8968 - mae: 389.8968 - val_loss: 415.3680 - val_mae: 415.3680\n",
      "Epoch 76/145\n",
      "63/63 - 0s - loss: 389.4974 - mae: 389.4974 - val_loss: 416.1592 - val_mae: 416.1592\n",
      "Epoch 77/145\n",
      "63/63 - 0s - loss: 387.8366 - mae: 387.8366 - val_loss: 414.2773 - val_mae: 414.2773\n",
      "Epoch 78/145\n",
      "63/63 - 0s - loss: 388.1432 - mae: 388.1432 - val_loss: 418.1412 - val_mae: 418.1412\n",
      "Epoch 79/145\n",
      "63/63 - 0s - loss: 396.2336 - mae: 396.2336 - val_loss: 415.1093 - val_mae: 415.1093\n",
      "Epoch 80/145\n",
      "63/63 - 0s - loss: 386.3291 - mae: 386.3291 - val_loss: 412.5098 - val_mae: 412.5098\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 0s - loss: 382.7949 - mae: 382.7949 - val_loss: 413.4888 - val_mae: 413.4888\n",
      "Epoch 82/145\n",
      "63/63 - 0s - loss: 380.9090 - mae: 380.9090 - val_loss: 411.6526 - val_mae: 411.6526\n",
      "Epoch 83/145\n",
      "63/63 - 0s - loss: 380.3912 - mae: 380.3912 - val_loss: 412.5295 - val_mae: 412.5295\n",
      "Epoch 84/145\n",
      "63/63 - 0s - loss: 379.6614 - mae: 379.6614 - val_loss: 414.4718 - val_mae: 414.4718\n",
      "Epoch 85/145\n",
      "63/63 - 0s - loss: 382.0412 - mae: 382.0412 - val_loss: 413.1436 - val_mae: 413.1436\n",
      "Epoch 86/145\n",
      "63/63 - 0s - loss: 379.8955 - mae: 379.8955 - val_loss: 412.3015 - val_mae: 412.3015\n",
      "Epoch 87/145\n",
      "63/63 - 0s - loss: 378.6543 - mae: 378.6543 - val_loss: 416.3456 - val_mae: 416.3456\n",
      "Epoch 88/145\n",
      "63/63 - 0s - loss: 379.8648 - mae: 379.8648 - val_loss: 412.7031 - val_mae: 412.7031\n",
      "Epoch 89/145\n",
      "63/63 - 0s - loss: 378.4882 - mae: 378.4882 - val_loss: 412.8245 - val_mae: 412.8245\n",
      "Epoch 90/145\n",
      "63/63 - 0s - loss: 378.3702 - mae: 378.3702 - val_loss: 411.1466 - val_mae: 411.1466\n",
      "Epoch 91/145\n",
      "63/63 - 0s - loss: 377.4368 - mae: 377.4368 - val_loss: 412.5784 - val_mae: 412.5784\n",
      "Epoch 92/145\n",
      "63/63 - 0s - loss: 377.1590 - mae: 377.1590 - val_loss: 412.0374 - val_mae: 412.0374\n",
      "Epoch 93/145\n",
      "63/63 - 0s - loss: 377.1797 - mae: 377.1797 - val_loss: 413.6550 - val_mae: 413.6550\n",
      "Epoch 94/145\n",
      "63/63 - 0s - loss: 376.6686 - mae: 376.6686 - val_loss: 412.9671 - val_mae: 412.9671\n",
      "Epoch 95/145\n",
      "63/63 - 0s - loss: 375.4857 - mae: 375.4857 - val_loss: 410.9343 - val_mae: 410.9343\n",
      "Epoch 96/145\n",
      "63/63 - 0s - loss: 375.8777 - mae: 375.8777 - val_loss: 411.2538 - val_mae: 411.2538\n",
      "Epoch 97/145\n",
      "63/63 - 1s - loss: 376.1248 - mae: 376.1248 - val_loss: 415.9355 - val_mae: 415.9355\n",
      "Epoch 98/145\n",
      "63/63 - 1s - loss: 375.7972 - mae: 375.7972 - val_loss: 412.0735 - val_mae: 412.0735\n",
      "Epoch 99/145\n",
      "63/63 - 1s - loss: 377.5910 - mae: 377.5910 - val_loss: 411.9707 - val_mae: 411.9707\n",
      "Epoch 100/145\n",
      "63/63 - 1s - loss: 374.0767 - mae: 374.0767 - val_loss: 411.8573 - val_mae: 411.8573\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 1s - loss: 371.7635 - mae: 371.7635 - val_loss: 411.1265 - val_mae: 411.1265\n",
      "Epoch 102/145\n",
      "63/63 - 1s - loss: 371.1601 - mae: 371.1601 - val_loss: 409.8100 - val_mae: 409.8100\n",
      "Epoch 103/145\n",
      "63/63 - 0s - loss: 370.8426 - mae: 370.8426 - val_loss: 411.5973 - val_mae: 411.5973\n",
      "Epoch 104/145\n",
      "63/63 - 0s - loss: 370.5647 - mae: 370.5647 - val_loss: 409.2244 - val_mae: 409.2244\n",
      "Epoch 105/145\n",
      "63/63 - 0s - loss: 369.7722 - mae: 369.7722 - val_loss: 410.6980 - val_mae: 410.6980\n",
      "Epoch 106/145\n",
      "63/63 - 0s - loss: 369.7857 - mae: 369.7857 - val_loss: 411.0491 - val_mae: 411.0491\n",
      "Epoch 107/145\n",
      "63/63 - 0s - loss: 369.0164 - mae: 369.0164 - val_loss: 409.4950 - val_mae: 409.4950\n",
      "Epoch 108/145\n",
      "63/63 - 0s - loss: 370.1663 - mae: 370.1663 - val_loss: 414.7873 - val_mae: 414.7873\n",
      "Epoch 109/145\n",
      "63/63 - 0s - loss: 368.3300 - mae: 368.3300 - val_loss: 409.3785 - val_mae: 409.3785\n",
      "Epoch 110/145\n",
      "63/63 - 0s - loss: 368.1809 - mae: 368.1809 - val_loss: 413.4977 - val_mae: 413.4977\n",
      "Epoch 111/145\n",
      "63/63 - 0s - loss: 368.6352 - mae: 368.6352 - val_loss: 411.1734 - val_mae: 411.1734\n",
      "Epoch 112/145\n",
      "63/63 - 0s - loss: 368.0842 - mae: 368.0842 - val_loss: 411.2179 - val_mae: 411.2179\n",
      "Epoch 113/145\n",
      "63/63 - 0s - loss: 368.7838 - mae: 368.7838 - val_loss: 412.2349 - val_mae: 412.2349\n",
      "Epoch 114/145\n",
      "63/63 - 1s - loss: 367.6459 - mae: 367.6459 - val_loss: 410.1244 - val_mae: 410.1244\n",
      "Epoch 115/145\n",
      "63/63 - 1s - loss: 366.9647 - mae: 366.9647 - val_loss: 412.3217 - val_mae: 412.3217\n",
      "Epoch 116/145\n",
      "63/63 - 1s - loss: 367.4979 - mae: 367.4979 - val_loss: 410.3122 - val_mae: 410.3122\n",
      "Epoch 117/145\n",
      "63/63 - 1s - loss: 367.7560 - mae: 367.7560 - val_loss: 410.2104 - val_mae: 410.2104\n",
      "Epoch 118/145\n",
      "63/63 - 1s - loss: 366.5132 - mae: 366.5132 - val_loss: 410.5883 - val_mae: 410.5883\n",
      "Epoch 119/145\n",
      "63/63 - 1s - loss: 366.8755 - mae: 366.8755 - val_loss: 412.0545 - val_mae: 412.0545\n",
      "Epoch 120/145\n",
      "63/63 - 1s - loss: 366.2846 - mae: 366.2846 - val_loss: 409.8928 - val_mae: 409.8928\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 1s - loss: 363.7927 - mae: 363.7927 - val_loss: 409.3795 - val_mae: 409.3795\n",
      "Epoch 122/145\n",
      "63/63 - 0s - loss: 363.3573 - mae: 363.3573 - val_loss: 410.4790 - val_mae: 410.4790\n",
      "Epoch 123/145\n",
      "63/63 - 0s - loss: 363.5783 - mae: 363.5783 - val_loss: 410.7937 - val_mae: 410.7937\n",
      "Epoch 124/145\n",
      "63/63 - 0s - loss: 363.3272 - mae: 363.3272 - val_loss: 409.6319 - val_mae: 409.6319\n",
      "Epoch 125/145\n",
      "63/63 - 0s - loss: 363.3372 - mae: 363.3372 - val_loss: 409.9391 - val_mae: 409.9391\n",
      "Epoch 126/145\n",
      "63/63 - 0s - loss: 362.7761 - mae: 362.7761 - val_loss: 410.3473 - val_mae: 410.3473\n",
      "Epoch 127/145\n",
      "63/63 - 0s - loss: 362.2483 - mae: 362.2483 - val_loss: 409.2256 - val_mae: 409.2256\n",
      "Epoch 128/145\n",
      "63/63 - 1s - loss: 362.2672 - mae: 362.2672 - val_loss: 410.2848 - val_mae: 410.2848\n",
      "Epoch 129/145\n",
      "63/63 - 0s - loss: 362.2147 - mae: 362.2147 - val_loss: 410.0188 - val_mae: 410.0188\n",
      "Epoch 130/145\n",
      "63/63 - 0s - loss: 362.0292 - mae: 362.0292 - val_loss: 411.4254 - val_mae: 411.4254\n",
      "Epoch 131/145\n",
      "63/63 - 0s - loss: 362.0531 - mae: 362.0531 - val_loss: 409.9923 - val_mae: 409.9923\n",
      "Epoch 132/145\n",
      "63/63 - 0s - loss: 362.1345 - mae: 362.1345 - val_loss: 409.7983 - val_mae: 409.7983\n",
      "Epoch 133/145\n",
      "63/63 - 0s - loss: 361.4721 - mae: 361.4721 - val_loss: 409.6707 - val_mae: 409.6707\n",
      "Epoch 134/145\n",
      "63/63 - 0s - loss: 361.3278 - mae: 361.3278 - val_loss: 410.4285 - val_mae: 410.4285\n",
      "Epoch 135/145\n",
      "63/63 - 1s - loss: 361.2665 - mae: 361.2665 - val_loss: 409.9538 - val_mae: 409.9538\n",
      "Epoch 136/145\n",
      "63/63 - 1s - loss: 361.5744 - mae: 361.5744 - val_loss: 409.2576 - val_mae: 409.2576\n",
      "Epoch 137/145\n",
      "63/63 - 1s - loss: 360.7348 - mae: 360.7348 - val_loss: 411.7415 - val_mae: 411.7415\n",
      "Epoch 138/145\n",
      "63/63 - 1s - loss: 361.3376 - mae: 361.3376 - val_loss: 410.1651 - val_mae: 410.1651\n",
      "Epoch 139/145\n",
      "63/63 - 1s - loss: 360.9075 - mae: 360.9075 - val_loss: 410.7455 - val_mae: 410.7455\n",
      "Epoch 140/145\n",
      "63/63 - 1s - loss: 360.0739 - mae: 360.0739 - val_loss: 410.3864 - val_mae: 410.3864\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 1s - loss: 359.4817 - mae: 359.4817 - val_loss: 409.5827 - val_mae: 409.5827\n",
      "Epoch 142/145\n",
      "63/63 - 1s - loss: 358.5081 - mae: 358.5081 - val_loss: 409.3502 - val_mae: 409.3502\n",
      "Epoch 143/145\n",
      "63/63 - 0s - loss: 358.5277 - mae: 358.5277 - val_loss: 409.7468 - val_mae: 409.7468\n",
      "Epoch 144/145\n",
      "63/63 - 0s - loss: 358.8772 - mae: 358.8772 - val_loss: 411.2601 - val_mae: 411.2601\n",
      "Epoch 145/145\n",
      "63/63 - 0s - loss: 358.4572 - mae: 358.4572 - val_loss: 409.2619 - val_mae: 409.2619\n",
      "\n",
      "val_mae is:409.26190550001144\n",
      "\n",
      "fold: 1\n",
      "X_train shape: (125000, 146)\n",
      "y_train shape: (125000,)\n",
      "X_val shape: (25000, 146)\n",
      "y_val shape: (25000,)\n",
      "Epoch 1/145\n",
      "63/63 - 2s - loss: 2661.8081 - mae: 2661.8081 - val_loss: 869.2037 - val_mae: 869.2037\n",
      "Epoch 2/145\n",
      "63/63 - 1s - loss: 794.9562 - mae: 794.9562 - val_loss: 693.3900 - val_mae: 693.3900\n",
      "Epoch 3/145\n",
      "63/63 - 1s - loss: 671.0715 - mae: 671.0715 - val_loss: 644.4481 - val_mae: 644.4481\n",
      "Epoch 4/145\n",
      "63/63 - 1s - loss: 622.7531 - mae: 622.7531 - val_loss: 558.2037 - val_mae: 558.2037\n",
      "Epoch 5/145\n",
      "63/63 - 1s - loss: 587.8108 - mae: 587.8108 - val_loss: 553.4617 - val_mae: 553.4617\n",
      "Epoch 6/145\n",
      "63/63 - 0s - loss: 563.3266 - mae: 563.3266 - val_loss: 528.9822 - val_mae: 528.9822\n",
      "Epoch 7/145\n",
      "63/63 - 0s - loss: 582.5322 - mae: 582.5322 - val_loss: 611.7047 - val_mae: 611.7047\n",
      "Epoch 8/145\n",
      "63/63 - 0s - loss: 537.4916 - mae: 537.4916 - val_loss: 543.3302 - val_mae: 543.3302\n",
      "Epoch 9/145\n",
      "63/63 - 0s - loss: 567.8269 - mae: 567.8269 - val_loss: 518.6069 - val_mae: 518.6069\n",
      "Epoch 10/145\n",
      "63/63 - 0s - loss: 499.7076 - mae: 499.7076 - val_loss: 555.0252 - val_mae: 555.0252\n",
      "Epoch 11/145\n",
      "63/63 - 0s - loss: 518.5595 - mae: 518.5595 - val_loss: 504.2445 - val_mae: 504.2445\n",
      "Epoch 12/145\n",
      "63/63 - 0s - loss: 500.5277 - mae: 500.5277 - val_loss: 516.3027 - val_mae: 516.3027\n",
      "Epoch 13/145\n",
      "63/63 - 0s - loss: 505.9144 - mae: 505.9144 - val_loss: 511.6322 - val_mae: 511.6322\n",
      "Epoch 14/145\n",
      "63/63 - 0s - loss: 520.1527 - mae: 520.1527 - val_loss: 477.7323 - val_mae: 477.7323\n",
      "Epoch 15/145\n",
      "63/63 - 0s - loss: 488.1042 - mae: 488.1042 - val_loss: 471.2813 - val_mae: 471.2813\n",
      "Epoch 16/145\n",
      "63/63 - 0s - loss: 508.6341 - mae: 508.6341 - val_loss: 479.1036 - val_mae: 479.1036\n",
      "Epoch 17/145\n",
      "63/63 - 0s - loss: 517.7459 - mae: 517.7459 - val_loss: 466.3688 - val_mae: 466.3688\n",
      "Epoch 18/145\n",
      "63/63 - 0s - loss: 491.3181 - mae: 491.3181 - val_loss: 485.9202 - val_mae: 485.9202\n",
      "Epoch 19/145\n",
      "63/63 - 0s - loss: 475.3345 - mae: 475.3345 - val_loss: 471.8134 - val_mae: 471.8134\n",
      "Epoch 20/145\n",
      "63/63 - 0s - loss: 522.2771 - mae: 522.2771 - val_loss: 488.6846 - val_mae: 488.6846\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 0s - loss: 450.6236 - mae: 450.6236 - val_loss: 449.4912 - val_mae: 449.4912\n",
      "Epoch 22/145\n",
      "63/63 - 0s - loss: 445.0324 - mae: 445.0324 - val_loss: 446.7594 - val_mae: 446.7594\n",
      "Epoch 23/145\n",
      "63/63 - 0s - loss: 440.5216 - mae: 440.5216 - val_loss: 451.4940 - val_mae: 451.4940\n",
      "Epoch 24/145\n",
      "63/63 - 0s - loss: 439.0841 - mae: 439.0841 - val_loss: 440.5428 - val_mae: 440.5428\n",
      "Epoch 25/145\n",
      "63/63 - 0s - loss: 445.1935 - mae: 445.1935 - val_loss: 450.0494 - val_mae: 450.0494\n",
      "Epoch 26/145\n",
      "63/63 - 0s - loss: 441.3273 - mae: 441.3273 - val_loss: 451.8184 - val_mae: 451.8184\n",
      "Epoch 27/145\n",
      "63/63 - 0s - loss: 440.5977 - mae: 440.5977 - val_loss: 457.3952 - val_mae: 457.3952\n",
      "Epoch 28/145\n",
      "63/63 - 0s - loss: 439.4050 - mae: 439.4050 - val_loss: 437.8145 - val_mae: 437.8145\n",
      "Epoch 29/145\n",
      "63/63 - 0s - loss: 439.0522 - mae: 439.0522 - val_loss: 455.3661 - val_mae: 455.3661\n",
      "Epoch 30/145\n",
      "63/63 - 0s - loss: 448.5141 - mae: 448.5141 - val_loss: 439.9270 - val_mae: 439.9270\n",
      "Epoch 31/145\n",
      "63/63 - 0s - loss: 440.1228 - mae: 440.1228 - val_loss: 439.0224 - val_mae: 439.0224\n",
      "Epoch 32/145\n",
      "63/63 - 0s - loss: 454.9274 - mae: 454.9274 - val_loss: 435.5608 - val_mae: 435.5608\n",
      "Epoch 33/145\n",
      "63/63 - 0s - loss: 433.6370 - mae: 433.6370 - val_loss: 465.2893 - val_mae: 465.2893\n",
      "Epoch 34/145\n",
      "63/63 - 0s - loss: 438.1965 - mae: 438.1965 - val_loss: 450.2208 - val_mae: 450.2208\n",
      "Epoch 35/145\n",
      "63/63 - 0s - loss: 430.8989 - mae: 430.8989 - val_loss: 433.0340 - val_mae: 433.0340\n",
      "Epoch 36/145\n",
      "63/63 - 0s - loss: 443.0085 - mae: 443.0085 - val_loss: 448.3749 - val_mae: 448.3749\n",
      "Epoch 37/145\n",
      "63/63 - 0s - loss: 430.4055 - mae: 430.4055 - val_loss: 451.0414 - val_mae: 451.0414\n",
      "Epoch 38/145\n",
      "63/63 - 0s - loss: 433.0370 - mae: 433.0370 - val_loss: 433.3346 - val_mae: 433.3346\n",
      "Epoch 39/145\n",
      "63/63 - 0s - loss: 430.5072 - mae: 430.5072 - val_loss: 434.5363 - val_mae: 434.5363\n",
      "Epoch 40/145\n",
      "63/63 - 1s - loss: 425.3216 - mae: 425.3216 - val_loss: 463.0697 - val_mae: 463.0697\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 1s - loss: 425.0117 - mae: 425.0117 - val_loss: 428.1730 - val_mae: 428.1730\n",
      "Epoch 42/145\n",
      "63/63 - 1s - loss: 412.0900 - mae: 412.0900 - val_loss: 429.9734 - val_mae: 429.9734\n",
      "Epoch 43/145\n",
      "63/63 - 1s - loss: 412.1079 - mae: 412.1079 - val_loss: 433.2791 - val_mae: 433.2791\n",
      "Epoch 44/145\n",
      "63/63 - 1s - loss: 409.9497 - mae: 409.9497 - val_loss: 424.9251 - val_mae: 424.9251\n",
      "Epoch 45/145\n",
      "63/63 - 1s - loss: 408.9551 - mae: 408.9551 - val_loss: 422.1630 - val_mae: 422.1630\n",
      "Epoch 46/145\n",
      "63/63 - 1s - loss: 409.3269 - mae: 409.3269 - val_loss: 423.0632 - val_mae: 423.0632\n",
      "Epoch 47/145\n",
      "63/63 - 0s - loss: 406.8495 - mae: 406.8495 - val_loss: 426.3185 - val_mae: 426.3185\n",
      "Epoch 48/145\n",
      "63/63 - 0s - loss: 407.5173 - mae: 407.5173 - val_loss: 428.2984 - val_mae: 428.2984\n",
      "Epoch 49/145\n",
      "63/63 - 0s - loss: 408.9722 - mae: 408.9722 - val_loss: 442.7547 - val_mae: 442.7547\n",
      "Epoch 50/145\n",
      "63/63 - 0s - loss: 409.9281 - mae: 409.9281 - val_loss: 425.0118 - val_mae: 425.0118\n",
      "Epoch 51/145\n",
      "63/63 - 0s - loss: 409.6764 - mae: 409.6764 - val_loss: 423.0950 - val_mae: 423.0950\n",
      "Epoch 52/145\n",
      "63/63 - 0s - loss: 408.4420 - mae: 408.4420 - val_loss: 423.1930 - val_mae: 423.1930\n",
      "Epoch 53/145\n",
      "63/63 - 0s - loss: 405.9357 - mae: 405.9357 - val_loss: 431.8306 - val_mae: 431.8306\n",
      "Epoch 54/145\n",
      "63/63 - 0s - loss: 409.5036 - mae: 409.5036 - val_loss: 422.8443 - val_mae: 422.8443\n",
      "Epoch 55/145\n",
      "63/63 - 0s - loss: 408.4013 - mae: 408.4013 - val_loss: 421.0673 - val_mae: 421.0673\n",
      "Epoch 56/145\n",
      "63/63 - 0s - loss: 403.5537 - mae: 403.5537 - val_loss: 444.4276 - val_mae: 444.4276\n",
      "Epoch 57/145\n",
      "63/63 - 1s - loss: 427.1496 - mae: 427.1496 - val_loss: 442.1587 - val_mae: 442.1587\n",
      "Epoch 58/145\n",
      "63/63 - 0s - loss: 407.5411 - mae: 407.5411 - val_loss: 419.5257 - val_mae: 419.5257\n",
      "Epoch 59/145\n",
      "63/63 - 1s - loss: 398.5775 - mae: 398.5775 - val_loss: 415.9074 - val_mae: 415.9074\n",
      "Epoch 60/145\n",
      "63/63 - 1s - loss: 400.2635 - mae: 400.2635 - val_loss: 422.4673 - val_mae: 422.4673\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 1s - loss: 392.4633 - mae: 392.4633 - val_loss: 415.9308 - val_mae: 415.9308\n",
      "Epoch 62/145\n",
      "63/63 - 1s - loss: 391.8492 - mae: 391.8492 - val_loss: 416.6076 - val_mae: 416.6076\n",
      "Epoch 63/145\n",
      "63/63 - 1s - loss: 392.6205 - mae: 392.6205 - val_loss: 415.3763 - val_mae: 415.3763\n",
      "Epoch 64/145\n",
      "63/63 - 1s - loss: 391.0325 - mae: 391.0325 - val_loss: 412.9710 - val_mae: 412.9710\n",
      "Epoch 65/145\n",
      "63/63 - 1s - loss: 389.2005 - mae: 389.2005 - val_loss: 413.1570 - val_mae: 413.1570\n",
      "Epoch 66/145\n",
      "63/63 - 0s - loss: 389.0948 - mae: 389.0948 - val_loss: 414.4077 - val_mae: 414.4077\n",
      "Epoch 67/145\n",
      "63/63 - 0s - loss: 389.5917 - mae: 389.5917 - val_loss: 414.6165 - val_mae: 414.6165\n",
      "Epoch 68/145\n",
      "63/63 - 0s - loss: 389.5953 - mae: 389.5953 - val_loss: 411.6512 - val_mae: 411.6512\n",
      "Epoch 69/145\n",
      "63/63 - 0s - loss: 389.2481 - mae: 389.2481 - val_loss: 411.4542 - val_mae: 411.4542\n",
      "Epoch 70/145\n",
      "63/63 - 0s - loss: 385.2853 - mae: 385.2853 - val_loss: 412.9290 - val_mae: 412.9290\n",
      "Epoch 71/145\n",
      "63/63 - 0s - loss: 389.0935 - mae: 389.0935 - val_loss: 436.8652 - val_mae: 436.8652\n",
      "Epoch 72/145\n",
      "63/63 - 0s - loss: 388.0971 - mae: 388.0971 - val_loss: 419.9079 - val_mae: 419.9079\n",
      "Epoch 73/145\n",
      "63/63 - 0s - loss: 385.2016 - mae: 385.2016 - val_loss: 413.4944 - val_mae: 413.4944\n",
      "Epoch 74/145\n",
      "63/63 - 0s - loss: 382.5054 - mae: 382.5054 - val_loss: 410.5372 - val_mae: 410.5372\n",
      "Epoch 75/145\n",
      "63/63 - 0s - loss: 384.6966 - mae: 384.6966 - val_loss: 413.7981 - val_mae: 413.7981\n",
      "Epoch 76/145\n",
      "63/63 - 0s - loss: 383.1500 - mae: 383.1500 - val_loss: 411.6232 - val_mae: 411.6232\n",
      "Epoch 77/145\n",
      "63/63 - 0s - loss: 384.6176 - mae: 384.6176 - val_loss: 413.0907 - val_mae: 413.0907\n",
      "Epoch 78/145\n",
      "63/63 - 1s - loss: 385.1092 - mae: 385.1092 - val_loss: 411.2380 - val_mae: 411.2380\n",
      "Epoch 79/145\n",
      "63/63 - 2s - loss: 382.8242 - mae: 382.8242 - val_loss: 409.0268 - val_mae: 409.0268\n",
      "Epoch 80/145\n",
      "63/63 - 1s - loss: 381.6048 - mae: 381.6048 - val_loss: 411.1703 - val_mae: 411.1703\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 2s - loss: 377.4442 - mae: 377.4442 - val_loss: 415.8308 - val_mae: 415.8308\n",
      "Epoch 82/145\n",
      "63/63 - 0s - loss: 376.7643 - mae: 376.7643 - val_loss: 407.9246 - val_mae: 407.9246\n",
      "Epoch 83/145\n",
      "63/63 - 0s - loss: 375.3922 - mae: 375.3922 - val_loss: 410.5789 - val_mae: 410.5789\n",
      "Epoch 84/145\n",
      "63/63 - 0s - loss: 377.6835 - mae: 377.6835 - val_loss: 408.3820 - val_mae: 408.3820\n",
      "Epoch 85/145\n",
      "63/63 - 0s - loss: 375.6592 - mae: 375.6592 - val_loss: 407.7834 - val_mae: 407.7834\n",
      "Epoch 86/145\n",
      "63/63 - 0s - loss: 374.7575 - mae: 374.7575 - val_loss: 409.5102 - val_mae: 409.5102\n",
      "Epoch 87/145\n",
      "63/63 - 0s - loss: 373.8728 - mae: 373.8728 - val_loss: 410.6771 - val_mae: 410.6771\n",
      "Epoch 88/145\n",
      "63/63 - 0s - loss: 373.5713 - mae: 373.5713 - val_loss: 412.4344 - val_mae: 412.4344\n",
      "Epoch 89/145\n",
      "63/63 - 0s - loss: 375.5069 - mae: 375.5069 - val_loss: 409.4319 - val_mae: 409.4319\n",
      "Epoch 90/145\n",
      "63/63 - 0s - loss: 373.3175 - mae: 373.3175 - val_loss: 409.1463 - val_mae: 409.1463\n",
      "Epoch 91/145\n",
      "63/63 - 0s - loss: 372.9510 - mae: 372.9510 - val_loss: 412.0648 - val_mae: 412.0648\n",
      "Epoch 92/145\n",
      "63/63 - 0s - loss: 373.8023 - mae: 373.8023 - val_loss: 410.6583 - val_mae: 410.6583\n",
      "Epoch 93/145\n",
      "63/63 - 0s - loss: 372.3685 - mae: 372.3685 - val_loss: 408.3963 - val_mae: 408.3963\n",
      "Epoch 94/145\n",
      "63/63 - 0s - loss: 372.4253 - mae: 372.4253 - val_loss: 409.9443 - val_mae: 409.9443\n",
      "Epoch 95/145\n",
      "63/63 - 1s - loss: 372.5360 - mae: 372.5360 - val_loss: 408.8898 - val_mae: 408.8898\n",
      "Epoch 96/145\n",
      "63/63 - 1s - loss: 372.8194 - mae: 372.8194 - val_loss: 410.4386 - val_mae: 410.4386\n",
      "Epoch 97/145\n",
      "63/63 - 1s - loss: 372.7890 - mae: 372.7890 - val_loss: 407.0720 - val_mae: 407.0720\n",
      "Epoch 98/145\n",
      "63/63 - 1s - loss: 370.6038 - mae: 370.6038 - val_loss: 409.3992 - val_mae: 409.3992\n",
      "Epoch 99/145\n",
      "63/63 - 1s - loss: 370.1132 - mae: 370.1132 - val_loss: 408.4514 - val_mae: 408.4514\n",
      "Epoch 100/145\n",
      "63/63 - 1s - loss: 371.7740 - mae: 371.7740 - val_loss: 414.1866 - val_mae: 414.1866\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 1s - loss: 368.2785 - mae: 368.2785 - val_loss: 406.8233 - val_mae: 406.8233\n",
      "Epoch 102/145\n",
      "63/63 - 1s - loss: 367.2805 - mae: 367.2805 - val_loss: 408.6205 - val_mae: 408.6205\n",
      "Epoch 103/145\n",
      "63/63 - 1s - loss: 366.6502 - mae: 366.6502 - val_loss: 406.0900 - val_mae: 406.0900\n",
      "Epoch 104/145\n",
      "63/63 - 0s - loss: 366.4709 - mae: 366.4709 - val_loss: 409.0750 - val_mae: 409.0750\n",
      "Epoch 105/145\n",
      "63/63 - 0s - loss: 366.6668 - mae: 366.6668 - val_loss: 406.8871 - val_mae: 406.8871\n",
      "Epoch 106/145\n",
      "63/63 - 0s - loss: 365.9452 - mae: 365.9452 - val_loss: 407.1451 - val_mae: 407.1451\n",
      "Epoch 107/145\n",
      "63/63 - 0s - loss: 365.7827 - mae: 365.7827 - val_loss: 407.1371 - val_mae: 407.1371\n",
      "Epoch 108/145\n",
      "63/63 - 0s - loss: 365.6643 - mae: 365.6643 - val_loss: 406.0956 - val_mae: 406.0956\n",
      "Epoch 109/145\n",
      "63/63 - 0s - loss: 366.5061 - mae: 366.5061 - val_loss: 410.1868 - val_mae: 410.1868\n",
      "Epoch 110/145\n",
      "63/63 - 0s - loss: 364.8684 - mae: 364.8684 - val_loss: 407.4020 - val_mae: 407.4020\n",
      "Epoch 111/145\n",
      "63/63 - 0s - loss: 365.0518 - mae: 365.0518 - val_loss: 407.0422 - val_mae: 407.0422\n",
      "Epoch 112/145\n",
      "63/63 - 0s - loss: 365.3029 - mae: 365.3029 - val_loss: 407.6749 - val_mae: 407.6749\n",
      "Epoch 113/145\n",
      "63/63 - 0s - loss: 364.8676 - mae: 364.8676 - val_loss: 406.1452 - val_mae: 406.1452\n",
      "Epoch 114/145\n",
      "63/63 - 0s - loss: 364.0658 - mae: 364.0658 - val_loss: 408.6418 - val_mae: 408.6418\n",
      "Epoch 115/145\n",
      "63/63 - 0s - loss: 363.5648 - mae: 363.5648 - val_loss: 406.6400 - val_mae: 406.6400\n",
      "Epoch 116/145\n",
      "63/63 - 0s - loss: 364.6299 - mae: 364.6299 - val_loss: 407.1666 - val_mae: 407.1666\n",
      "Epoch 117/145\n",
      "63/63 - 0s - loss: 364.3042 - mae: 364.3042 - val_loss: 405.9933 - val_mae: 405.9933\n",
      "Epoch 118/145\n",
      "63/63 - 0s - loss: 364.2988 - mae: 364.2988 - val_loss: 411.5782 - val_mae: 411.5782\n",
      "Epoch 119/145\n",
      "63/63 - 0s - loss: 363.8182 - mae: 363.8182 - val_loss: 408.6955 - val_mae: 408.6955\n",
      "Epoch 120/145\n",
      "63/63 - 0s - loss: 362.3428 - mae: 362.3428 - val_loss: 407.3923 - val_mae: 407.3923\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 0s - loss: 360.9842 - mae: 360.9842 - val_loss: 408.4224 - val_mae: 408.4224\n",
      "Epoch 122/145\n",
      "63/63 - 0s - loss: 360.6451 - mae: 360.6451 - val_loss: 406.8852 - val_mae: 406.8852\n",
      "Epoch 123/145\n",
      "63/63 - 0s - loss: 360.7666 - mae: 360.7666 - val_loss: 409.6068 - val_mae: 409.6068\n",
      "Epoch 124/145\n",
      "63/63 - 0s - loss: 360.5542 - mae: 360.5542 - val_loss: 407.6232 - val_mae: 407.6232\n",
      "Epoch 125/145\n",
      "63/63 - 0s - loss: 360.6293 - mae: 360.6293 - val_loss: 406.9408 - val_mae: 406.9408\n",
      "Epoch 126/145\n",
      "63/63 - 0s - loss: 360.6035 - mae: 360.6035 - val_loss: 407.8037 - val_mae: 407.8037\n",
      "Epoch 127/145\n",
      "63/63 - 0s - loss: 359.8287 - mae: 359.8287 - val_loss: 407.0826 - val_mae: 407.0826\n",
      "Epoch 128/145\n",
      "63/63 - 0s - loss: 360.6876 - mae: 360.6876 - val_loss: 407.0372 - val_mae: 407.0372\n",
      "Epoch 129/145\n",
      "63/63 - 0s - loss: 359.9737 - mae: 359.9737 - val_loss: 407.0500 - val_mae: 407.0500\n",
      "Epoch 130/145\n",
      "63/63 - 0s - loss: 360.2882 - mae: 360.2882 - val_loss: 407.7797 - val_mae: 407.7797\n",
      "Epoch 131/145\n",
      "63/63 - 0s - loss: 359.9150 - mae: 359.9150 - val_loss: 406.4219 - val_mae: 406.4219\n",
      "Epoch 132/145\n",
      "63/63 - 0s - loss: 359.8676 - mae: 359.8676 - val_loss: 407.9538 - val_mae: 407.9538\n",
      "Epoch 133/145\n",
      "63/63 - 0s - loss: 359.8507 - mae: 359.8507 - val_loss: 408.0861 - val_mae: 408.0861\n",
      "Epoch 134/145\n",
      "63/63 - 0s - loss: 359.5445 - mae: 359.5445 - val_loss: 407.6956 - val_mae: 407.6956\n",
      "Epoch 135/145\n",
      "63/63 - 0s - loss: 359.1768 - mae: 359.1768 - val_loss: 407.1577 - val_mae: 407.1577\n",
      "Epoch 136/145\n",
      "63/63 - 0s - loss: 359.0640 - mae: 359.0640 - val_loss: 407.4239 - val_mae: 407.4239\n",
      "Epoch 137/145\n",
      "63/63 - 0s - loss: 358.5310 - mae: 358.5310 - val_loss: 408.6383 - val_mae: 408.6383\n",
      "Epoch 138/145\n",
      "63/63 - 0s - loss: 358.3808 - mae: 358.3808 - val_loss: 406.9691 - val_mae: 406.9691\n",
      "Epoch 139/145\n",
      "63/63 - 0s - loss: 358.6718 - mae: 358.6718 - val_loss: 406.2363 - val_mae: 406.2363\n",
      "Epoch 140/145\n",
      "63/63 - 0s - loss: 358.7868 - mae: 358.7868 - val_loss: 406.6275 - val_mae: 406.6275\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 1s - loss: 357.5544 - mae: 357.5544 - val_loss: 406.5960 - val_mae: 406.5960\n",
      "Epoch 142/145\n",
      "63/63 - 1s - loss: 357.0032 - mae: 357.0032 - val_loss: 407.4787 - val_mae: 407.4787\n",
      "Epoch 143/145\n",
      "63/63 - 1s - loss: 356.7364 - mae: 356.7364 - val_loss: 407.1389 - val_mae: 407.1389\n",
      "Epoch 144/145\n",
      "63/63 - 1s - loss: 356.6218 - mae: 356.6218 - val_loss: 406.5042 - val_mae: 406.5042\n",
      "Epoch 145/145\n",
      "63/63 - 1s - loss: 356.6936 - mae: 356.6936 - val_loss: 406.3914 - val_mae: 406.3914\n",
      "\n",
      "val_mae is:406.39142397758485\n",
      "\n",
      "fold: 2\n",
      "X_train shape: (125000, 146)\n",
      "y_train shape: (125000,)\n",
      "X_val shape: (25000, 146)\n",
      "y_val shape: (25000,)\n",
      "Epoch 1/145\n",
      "63/63 - 1s - loss: 2456.4062 - mae: 2456.4062 - val_loss: 879.7570 - val_mae: 879.7570\n",
      "Epoch 2/145\n",
      "63/63 - 0s - loss: 795.5287 - mae: 795.5287 - val_loss: 702.7767 - val_mae: 702.7767\n",
      "Epoch 3/145\n",
      "63/63 - 0s - loss: 668.1859 - mae: 668.1859 - val_loss: 809.0608 - val_mae: 809.0608\n",
      "Epoch 4/145\n",
      "63/63 - 0s - loss: 655.9696 - mae: 655.9696 - val_loss: 589.0646 - val_mae: 589.0646\n",
      "Epoch 5/145\n",
      "63/63 - 0s - loss: 599.7524 - mae: 599.7524 - val_loss: 564.2747 - val_mae: 564.2747\n",
      "Epoch 6/145\n",
      "63/63 - 0s - loss: 549.3470 - mae: 549.3470 - val_loss: 554.8128 - val_mae: 554.8128\n",
      "Epoch 7/145\n",
      "63/63 - 0s - loss: 595.1450 - mae: 595.1450 - val_loss: 620.6707 - val_mae: 620.6707\n",
      "Epoch 8/145\n",
      "63/63 - 1s - loss: 527.7392 - mae: 527.7392 - val_loss: 502.4400 - val_mae: 502.4400\n",
      "Epoch 9/145\n",
      "63/63 - 1s - loss: 517.1775 - mae: 517.1775 - val_loss: 510.8464 - val_mae: 510.8464\n",
      "Epoch 10/145\n",
      "63/63 - 1s - loss: 516.7612 - mae: 516.7612 - val_loss: 511.1399 - val_mae: 511.1399\n",
      "Epoch 11/145\n",
      "63/63 - 1s - loss: 523.5963 - mae: 523.5963 - val_loss: 610.2776 - val_mae: 610.2776\n",
      "Epoch 12/145\n",
      "63/63 - 1s - loss: 549.7741 - mae: 549.7741 - val_loss: 509.2796 - val_mae: 509.2796\n",
      "Epoch 13/145\n",
      "63/63 - 1s - loss: 510.1229 - mae: 510.1229 - val_loss: 581.0715 - val_mae: 581.0715\n",
      "Epoch 14/145\n",
      "63/63 - 1s - loss: 529.0267 - mae: 529.0267 - val_loss: 636.8224 - val_mae: 636.8224\n",
      "Epoch 15/145\n",
      "63/63 - 1s - loss: 502.0510 - mae: 502.0510 - val_loss: 509.5937 - val_mae: 509.5937\n",
      "Epoch 16/145\n",
      "63/63 - 1s - loss: 506.7503 - mae: 506.7503 - val_loss: 495.2549 - val_mae: 495.2549\n",
      "Epoch 17/145\n",
      "63/63 - 0s - loss: 479.1441 - mae: 479.1441 - val_loss: 488.1545 - val_mae: 488.1545\n",
      "Epoch 18/145\n",
      "63/63 - 0s - loss: 500.1549 - mae: 500.1549 - val_loss: 527.3447 - val_mae: 527.3447\n",
      "Epoch 19/145\n",
      "63/63 - 0s - loss: 503.8596 - mae: 503.8596 - val_loss: 519.2692 - val_mae: 519.2692\n",
      "Epoch 20/145\n",
      "63/63 - 0s - loss: 545.7678 - mae: 545.7678 - val_loss: 529.0370 - val_mae: 529.0370\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 0s - loss: 451.2386 - mae: 451.2386 - val_loss: 463.0436 - val_mae: 463.0436\n",
      "Epoch 22/145\n",
      "63/63 - 0s - loss: 448.4165 - mae: 448.4165 - val_loss: 462.8528 - val_mae: 462.8528\n",
      "Epoch 23/145\n",
      "63/63 - 0s - loss: 446.3911 - mae: 446.3911 - val_loss: 460.5230 - val_mae: 460.5230\n",
      "Epoch 24/145\n",
      "63/63 - 0s - loss: 460.0437 - mae: 460.0437 - val_loss: 458.5798 - val_mae: 458.5798\n",
      "Epoch 25/145\n",
      "63/63 - 0s - loss: 440.8677 - mae: 440.8677 - val_loss: 468.2904 - val_mae: 468.2904\n",
      "Epoch 26/145\n",
      "63/63 - 0s - loss: 441.5197 - mae: 441.5197 - val_loss: 453.8066 - val_mae: 453.8066\n",
      "Epoch 27/145\n",
      "63/63 - 0s - loss: 441.7621 - mae: 441.7621 - val_loss: 459.8761 - val_mae: 459.8761\n",
      "Epoch 28/145\n",
      "63/63 - 0s - loss: 441.7285 - mae: 441.7285 - val_loss: 457.0542 - val_mae: 457.0542\n",
      "Epoch 29/145\n",
      "63/63 - 0s - loss: 440.9764 - mae: 440.9764 - val_loss: 451.1057 - val_mae: 451.1057\n",
      "Epoch 30/145\n",
      "63/63 - 1s - loss: 469.1033 - mae: 469.1033 - val_loss: 457.8042 - val_mae: 457.8042\n",
      "Epoch 31/145\n",
      "63/63 - 1s - loss: 441.6968 - mae: 441.6968 - val_loss: 475.9189 - val_mae: 475.9189\n",
      "Epoch 32/145\n",
      "63/63 - 1s - loss: 436.8674 - mae: 436.8674 - val_loss: 449.3608 - val_mae: 449.3608\n",
      "Epoch 33/145\n",
      "63/63 - 1s - loss: 460.2954 - mae: 460.2954 - val_loss: 466.2527 - val_mae: 466.2527\n",
      "Epoch 34/145\n",
      "63/63 - 1s - loss: 436.6958 - mae: 436.6958 - val_loss: 451.0530 - val_mae: 451.0530\n",
      "Epoch 35/145\n",
      "63/63 - 1s - loss: 441.7853 - mae: 441.7853 - val_loss: 460.8593 - val_mae: 460.8593\n",
      "Epoch 36/145\n",
      "63/63 - 1s - loss: 429.9176 - mae: 429.9176 - val_loss: 453.6143 - val_mae: 453.6143\n",
      "Epoch 37/145\n",
      "63/63 - 1s - loss: 431.0629 - mae: 431.0629 - val_loss: 446.2092 - val_mae: 446.2092\n",
      "Epoch 38/145\n",
      "63/63 - 0s - loss: 429.1107 - mae: 429.1107 - val_loss: 460.5315 - val_mae: 460.5315\n",
      "Epoch 39/145\n",
      "63/63 - 0s - loss: 427.1703 - mae: 427.1703 - val_loss: 460.0814 - val_mae: 460.0814\n",
      "Epoch 40/145\n",
      "63/63 - 0s - loss: 427.7662 - mae: 427.7662 - val_loss: 454.8675 - val_mae: 454.8675\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 0s - loss: 414.9156 - mae: 414.9156 - val_loss: 444.9149 - val_mae: 444.9149\n",
      "Epoch 42/145\n",
      "63/63 - 0s - loss: 412.1266 - mae: 412.1266 - val_loss: 441.3461 - val_mae: 441.3461\n",
      "Epoch 43/145\n",
      "63/63 - 0s - loss: 411.1232 - mae: 411.1232 - val_loss: 441.9992 - val_mae: 441.9992\n",
      "Epoch 44/145\n",
      "63/63 - 0s - loss: 408.9766 - mae: 408.9766 - val_loss: 439.8246 - val_mae: 439.8246\n",
      "Epoch 45/145\n",
      "63/63 - 0s - loss: 407.3504 - mae: 407.3504 - val_loss: 439.4813 - val_mae: 439.4813\n",
      "Epoch 46/145\n",
      "63/63 - 0s - loss: 411.4414 - mae: 411.4414 - val_loss: 443.1107 - val_mae: 443.1107\n",
      "Epoch 47/145\n",
      "63/63 - 0s - loss: 409.2679 - mae: 409.2679 - val_loss: 447.8140 - val_mae: 447.8140\n",
      "Epoch 48/145\n",
      "63/63 - 0s - loss: 408.2372 - mae: 408.2372 - val_loss: 449.6600 - val_mae: 449.6600\n",
      "Epoch 49/145\n",
      "63/63 - 0s - loss: 409.3523 - mae: 409.3523 - val_loss: 444.6969 - val_mae: 444.6969\n",
      "Epoch 50/145\n",
      "63/63 - 0s - loss: 406.3908 - mae: 406.3908 - val_loss: 440.6844 - val_mae: 440.6844\n",
      "Epoch 51/145\n",
      "63/63 - 0s - loss: 409.0027 - mae: 409.0027 - val_loss: 436.5777 - val_mae: 436.5777\n",
      "Epoch 52/145\n",
      "63/63 - 1s - loss: 405.3948 - mae: 405.3948 - val_loss: 448.6133 - val_mae: 448.6133\n",
      "Epoch 53/145\n",
      "63/63 - 1s - loss: 403.8364 - mae: 403.8364 - val_loss: 443.4649 - val_mae: 443.4649\n",
      "Epoch 54/145\n",
      "63/63 - 1s - loss: 405.3981 - mae: 405.3981 - val_loss: 435.8523 - val_mae: 435.8523\n",
      "Epoch 55/145\n",
      "63/63 - 1s - loss: 410.8305 - mae: 410.8305 - val_loss: 435.4686 - val_mae: 435.4686\n",
      "Epoch 56/145\n",
      "63/63 - 1s - loss: 404.6503 - mae: 404.6503 - val_loss: 448.7003 - val_mae: 448.7003\n",
      "Epoch 57/145\n",
      "63/63 - 1s - loss: 405.3537 - mae: 405.3537 - val_loss: 447.5424 - val_mae: 447.5424\n",
      "Epoch 58/145\n",
      "63/63 - 1s - loss: 404.2162 - mae: 404.2162 - val_loss: 441.6248 - val_mae: 441.6248\n",
      "Epoch 59/145\n",
      "63/63 - 1s - loss: 398.6425 - mae: 398.6425 - val_loss: 436.8579 - val_mae: 436.8579\n",
      "Epoch 60/145\n",
      "63/63 - 0s - loss: 403.7428 - mae: 403.7428 - val_loss: 437.8620 - val_mae: 437.8620\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 0s - loss: 390.1978 - mae: 390.1978 - val_loss: 433.4957 - val_mae: 433.4957\n",
      "Epoch 62/145\n",
      "63/63 - 0s - loss: 388.5306 - mae: 388.5306 - val_loss: 431.7274 - val_mae: 431.7274\n",
      "Epoch 63/145\n",
      "63/63 - 0s - loss: 387.7391 - mae: 387.7391 - val_loss: 433.6172 - val_mae: 433.6172\n",
      "Epoch 64/145\n",
      "63/63 - 0s - loss: 387.2973 - mae: 387.2973 - val_loss: 437.7397 - val_mae: 437.7397\n",
      "Epoch 65/145\n",
      "63/63 - 0s - loss: 388.3484 - mae: 388.3484 - val_loss: 434.4231 - val_mae: 434.4231\n",
      "Epoch 66/145\n",
      "63/63 - 0s - loss: 387.1206 - mae: 387.1206 - val_loss: 432.7338 - val_mae: 432.7338\n",
      "Epoch 67/145\n",
      "63/63 - 0s - loss: 385.4566 - mae: 385.4566 - val_loss: 430.0447 - val_mae: 430.0447\n",
      "Epoch 68/145\n",
      "63/63 - 0s - loss: 384.6496 - mae: 384.6496 - val_loss: 428.2712 - val_mae: 428.2712\n",
      "Epoch 69/145\n",
      "63/63 - 0s - loss: 383.3810 - mae: 383.3810 - val_loss: 430.4380 - val_mae: 430.4380\n",
      "Epoch 70/145\n",
      "63/63 - 0s - loss: 383.0705 - mae: 383.0705 - val_loss: 428.4212 - val_mae: 428.4212\n",
      "Epoch 71/145\n",
      "63/63 - 0s - loss: 384.4919 - mae: 384.4919 - val_loss: 437.6794 - val_mae: 437.6794\n",
      "Epoch 72/145\n",
      "63/63 - 0s - loss: 387.4082 - mae: 387.4082 - val_loss: 430.5097 - val_mae: 430.5097\n",
      "Epoch 73/145\n",
      "63/63 - 0s - loss: 382.8666 - mae: 382.8666 - val_loss: 434.3827 - val_mae: 434.3827\n",
      "Epoch 74/145\n",
      "63/63 - 0s - loss: 382.8494 - mae: 382.8494 - val_loss: 433.7736 - val_mae: 433.7736\n",
      "Epoch 75/145\n",
      "63/63 - 0s - loss: 384.3494 - mae: 384.3494 - val_loss: 430.1379 - val_mae: 430.1379\n",
      "Epoch 76/145\n",
      "63/63 - 0s - loss: 383.7126 - mae: 383.7126 - val_loss: 433.2410 - val_mae: 433.2410\n",
      "Epoch 77/145\n",
      "63/63 - 0s - loss: 384.9332 - mae: 384.9332 - val_loss: 438.4863 - val_mae: 438.4863\n",
      "Epoch 78/145\n",
      "63/63 - 0s - loss: 381.1206 - mae: 381.1206 - val_loss: 433.8958 - val_mae: 433.8958\n",
      "Epoch 79/145\n",
      "63/63 - 0s - loss: 379.1978 - mae: 379.1978 - val_loss: 429.0375 - val_mae: 429.0375\n",
      "Epoch 80/145\n",
      "63/63 - 0s - loss: 378.8748 - mae: 378.8748 - val_loss: 429.7457 - val_mae: 429.7457\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 0s - loss: 375.9436 - mae: 375.9436 - val_loss: 428.4372 - val_mae: 428.4372\n",
      "Epoch 82/145\n",
      "63/63 - 0s - loss: 373.8166 - mae: 373.8166 - val_loss: 427.8840 - val_mae: 427.8840\n",
      "Epoch 83/145\n",
      "63/63 - 0s - loss: 374.7690 - mae: 374.7690 - val_loss: 426.5029 - val_mae: 426.5029\n",
      "Epoch 84/145\n",
      "63/63 - 0s - loss: 373.9846 - mae: 373.9846 - val_loss: 427.1501 - val_mae: 427.1501\n",
      "Epoch 85/145\n",
      "63/63 - 0s - loss: 372.0172 - mae: 372.0172 - val_loss: 426.1282 - val_mae: 426.1282\n",
      "Epoch 86/145\n",
      "63/63 - 0s - loss: 373.2406 - mae: 373.2406 - val_loss: 430.4385 - val_mae: 430.4385\n",
      "Epoch 87/145\n",
      "63/63 - 0s - loss: 373.0349 - mae: 373.0349 - val_loss: 429.0587 - val_mae: 429.0587\n",
      "Epoch 88/145\n",
      "63/63 - 0s - loss: 373.2439 - mae: 373.2439 - val_loss: 429.1204 - val_mae: 429.1204\n",
      "Epoch 89/145\n",
      "63/63 - 0s - loss: 372.3845 - mae: 372.3845 - val_loss: 429.1099 - val_mae: 429.1099\n",
      "Epoch 90/145\n",
      "63/63 - 0s - loss: 371.0741 - mae: 371.0741 - val_loss: 430.1599 - val_mae: 430.1599\n",
      "Epoch 91/145\n",
      "63/63 - 0s - loss: 371.4582 - mae: 371.4582 - val_loss: 429.2721 - val_mae: 429.2721\n",
      "Epoch 92/145\n",
      "63/63 - 0s - loss: 369.8128 - mae: 369.8128 - val_loss: 428.1922 - val_mae: 428.1922\n",
      "Epoch 93/145\n",
      "63/63 - 0s - loss: 370.3489 - mae: 370.3489 - val_loss: 427.8477 - val_mae: 427.8477\n",
      "Epoch 94/145\n",
      "63/63 - 0s - loss: 370.9975 - mae: 370.9975 - val_loss: 428.8763 - val_mae: 428.8763\n",
      "Epoch 95/145\n",
      "63/63 - 0s - loss: 369.7594 - mae: 369.7594 - val_loss: 426.7301 - val_mae: 426.7301\n",
      "Epoch 96/145\n",
      "63/63 - 0s - loss: 369.0779 - mae: 369.0779 - val_loss: 428.5630 - val_mae: 428.5630\n",
      "Epoch 97/145\n",
      "63/63 - 0s - loss: 368.9304 - mae: 368.9304 - val_loss: 428.7755 - val_mae: 428.7755\n",
      "Epoch 98/145\n",
      "63/63 - 1s - loss: 368.6703 - mae: 368.6703 - val_loss: 435.3703 - val_mae: 435.3703\n",
      "Epoch 99/145\n",
      "63/63 - 1s - loss: 369.2556 - mae: 369.2556 - val_loss: 427.7695 - val_mae: 427.7695\n",
      "Epoch 100/145\n",
      "63/63 - 1s - loss: 367.6260 - mae: 367.6260 - val_loss: 431.0694 - val_mae: 431.0694\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 1s - loss: 366.5325 - mae: 366.5325 - val_loss: 427.9166 - val_mae: 427.9166\n",
      "Epoch 102/145\n",
      "63/63 - 1s - loss: 364.8617 - mae: 364.8617 - val_loss: 428.3477 - val_mae: 428.3477\n",
      "Epoch 103/145\n",
      "63/63 - 1s - loss: 363.2006 - mae: 363.2006 - val_loss: 426.2838 - val_mae: 426.2838\n",
      "Epoch 104/145\n",
      "63/63 - 1s - loss: 363.5271 - mae: 363.5271 - val_loss: 427.3194 - val_mae: 427.3194\n",
      "Epoch 105/145\n",
      "63/63 - 1s - loss: 362.7516 - mae: 362.7516 - val_loss: 426.0180 - val_mae: 426.0180\n",
      "Epoch 106/145\n",
      "63/63 - 1s - loss: 362.9709 - mae: 362.9709 - val_loss: 427.0027 - val_mae: 427.0027\n",
      "Epoch 107/145\n",
      "63/63 - 0s - loss: 362.5475 - mae: 362.5475 - val_loss: 426.8946 - val_mae: 426.8946\n",
      "Epoch 108/145\n",
      "63/63 - 0s - loss: 363.0114 - mae: 363.0114 - val_loss: 427.0596 - val_mae: 427.0596\n",
      "Epoch 109/145\n",
      "63/63 - 0s - loss: 362.3742 - mae: 362.3742 - val_loss: 427.5517 - val_mae: 427.5517\n",
      "Epoch 110/145\n",
      "63/63 - 0s - loss: 362.2932 - mae: 362.2932 - val_loss: 427.2301 - val_mae: 427.2301\n",
      "Epoch 111/145\n",
      "63/63 - 0s - loss: 362.2275 - mae: 362.2275 - val_loss: 428.2841 - val_mae: 428.2841\n",
      "Epoch 112/145\n",
      "63/63 - 0s - loss: 361.3472 - mae: 361.3472 - val_loss: 427.6218 - val_mae: 427.6218\n",
      "Epoch 113/145\n",
      "63/63 - 0s - loss: 361.1428 - mae: 361.1428 - val_loss: 427.9906 - val_mae: 427.9906\n",
      "Epoch 114/145\n",
      "63/63 - 0s - loss: 361.3463 - mae: 361.3463 - val_loss: 428.6797 - val_mae: 428.6797\n",
      "Epoch 115/145\n",
      "63/63 - 0s - loss: 361.0837 - mae: 361.0837 - val_loss: 429.3503 - val_mae: 429.3503\n",
      "Epoch 116/145\n",
      "63/63 - 0s - loss: 361.7896 - mae: 361.7896 - val_loss: 426.5500 - val_mae: 426.5500\n",
      "Epoch 117/145\n",
      "63/63 - 0s - loss: 359.9052 - mae: 359.9052 - val_loss: 426.5999 - val_mae: 426.5999\n",
      "Epoch 118/145\n",
      "63/63 - 0s - loss: 359.4306 - mae: 359.4306 - val_loss: 428.3232 - val_mae: 428.3232\n",
      "Epoch 119/145\n",
      "63/63 - 0s - loss: 360.4109 - mae: 360.4109 - val_loss: 426.1530 - val_mae: 426.1530\n",
      "Epoch 120/145\n",
      "63/63 - 0s - loss: 359.6917 - mae: 359.6917 - val_loss: 427.2194 - val_mae: 427.2194\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 1s - loss: 357.9400 - mae: 357.9400 - val_loss: 426.9781 - val_mae: 426.9781\n",
      "Epoch 122/145\n",
      "63/63 - 1s - loss: 358.0390 - mae: 358.0390 - val_loss: 426.7883 - val_mae: 426.7883\n",
      "Epoch 123/145\n",
      "63/63 - 1s - loss: 357.3690 - mae: 357.3690 - val_loss: 426.5934 - val_mae: 426.5934\n",
      "Epoch 124/145\n",
      "63/63 - 1s - loss: 356.9856 - mae: 356.9856 - val_loss: 427.0130 - val_mae: 427.0130\n",
      "Epoch 125/145\n",
      "63/63 - 1s - loss: 357.4408 - mae: 357.4408 - val_loss: 426.6511 - val_mae: 426.6511\n",
      "Epoch 126/145\n",
      "63/63 - 1s - loss: 356.7267 - mae: 356.7267 - val_loss: 426.7189 - val_mae: 426.7189\n",
      "Epoch 127/145\n",
      "63/63 - 1s - loss: 356.3965 - mae: 356.3965 - val_loss: 426.4835 - val_mae: 426.4835\n",
      "Epoch 128/145\n",
      "63/63 - 1s - loss: 355.9749 - mae: 355.9749 - val_loss: 426.9060 - val_mae: 426.9060\n",
      "Epoch 129/145\n",
      "63/63 - 0s - loss: 355.9357 - mae: 355.9357 - val_loss: 426.4308 - val_mae: 426.4308\n",
      "Epoch 130/145\n",
      "63/63 - 0s - loss: 356.0622 - mae: 356.0622 - val_loss: 426.6781 - val_mae: 426.6781\n",
      "Epoch 131/145\n",
      "63/63 - 0s - loss: 356.9329 - mae: 356.9329 - val_loss: 427.3644 - val_mae: 427.3644\n",
      "Epoch 132/145\n",
      "63/63 - 0s - loss: 356.0067 - mae: 356.0067 - val_loss: 426.3917 - val_mae: 426.3917\n",
      "Epoch 133/145\n",
      "63/63 - 0s - loss: 355.8894 - mae: 355.8894 - val_loss: 427.2176 - val_mae: 427.2176\n",
      "Epoch 134/145\n",
      "63/63 - 0s - loss: 355.5195 - mae: 355.5195 - val_loss: 427.6021 - val_mae: 427.6021\n",
      "Epoch 135/145\n",
      "63/63 - 0s - loss: 355.7336 - mae: 355.7336 - val_loss: 428.2281 - val_mae: 428.2281\n",
      "Epoch 136/145\n",
      "63/63 - 0s - loss: 355.3058 - mae: 355.3058 - val_loss: 426.7179 - val_mae: 426.7179\n",
      "Epoch 137/145\n",
      "63/63 - 0s - loss: 355.5517 - mae: 355.5517 - val_loss: 426.7687 - val_mae: 426.7687\n",
      "Epoch 138/145\n",
      "63/63 - 0s - loss: 355.3168 - mae: 355.3168 - val_loss: 427.5846 - val_mae: 427.5846\n",
      "Epoch 139/145\n",
      "63/63 - 0s - loss: 354.4692 - mae: 354.4692 - val_loss: 427.3346 - val_mae: 427.3346\n",
      "Epoch 140/145\n",
      "63/63 - 0s - loss: 354.3927 - mae: 354.3927 - val_loss: 427.0586 - val_mae: 427.0586\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 0s - loss: 353.2679 - mae: 353.2679 - val_loss: 426.8543 - val_mae: 426.8543\n",
      "Epoch 142/145\n",
      "63/63 - 0s - loss: 353.0759 - mae: 353.0759 - val_loss: 426.4466 - val_mae: 426.4466\n",
      "Epoch 143/145\n",
      "63/63 - 0s - loss: 352.8917 - mae: 352.8917 - val_loss: 426.6670 - val_mae: 426.6670\n",
      "Epoch 144/145\n",
      "63/63 - 0s - loss: 353.0048 - mae: 353.0048 - val_loss: 427.3792 - val_mae: 427.3792\n",
      "Epoch 145/145\n",
      "63/63 - 0s - loss: 352.8050 - mae: 352.8050 - val_loss: 426.7675 - val_mae: 426.7675\n",
      "\n",
      "val_mae is:426.76743871631624\n",
      "\n",
      "fold: 3\n",
      "X_train shape: (125000, 146)\n",
      "y_train shape: (125000,)\n",
      "X_val shape: (25000, 146)\n",
      "y_val shape: (25000,)\n",
      "Epoch 1/145\n",
      "63/63 - 1s - loss: 2526.3396 - mae: 2526.3396 - val_loss: 901.9788 - val_mae: 901.9788\n",
      "Epoch 2/145\n",
      "63/63 - 0s - loss: 774.5460 - mae: 774.5460 - val_loss: 721.5999 - val_mae: 721.5999\n",
      "Epoch 3/145\n",
      "63/63 - 0s - loss: 677.6797 - mae: 677.6797 - val_loss: 706.9620 - val_mae: 706.9620\n",
      "Epoch 4/145\n",
      "63/63 - 0s - loss: 642.6874 - mae: 642.6874 - val_loss: 602.0726 - val_mae: 602.0726\n",
      "Epoch 5/145\n",
      "63/63 - 0s - loss: 597.1071 - mae: 597.1071 - val_loss: 735.0563 - val_mae: 735.0563\n",
      "Epoch 6/145\n",
      "63/63 - 0s - loss: 632.2402 - mae: 632.2402 - val_loss: 533.2485 - val_mae: 533.2485\n",
      "Epoch 7/145\n",
      "63/63 - 0s - loss: 567.1635 - mae: 567.1636 - val_loss: 529.0619 - val_mae: 529.0619\n",
      "Epoch 8/145\n",
      "63/63 - 0s - loss: 545.2172 - mae: 545.2172 - val_loss: 542.8141 - val_mae: 542.8141\n",
      "Epoch 9/145\n",
      "63/63 - 0s - loss: 531.0614 - mae: 531.0614 - val_loss: 628.7796 - val_mae: 628.7796\n",
      "Epoch 10/145\n",
      "63/63 - 0s - loss: 522.2379 - mae: 522.2379 - val_loss: 492.9241 - val_mae: 492.9241\n",
      "Epoch 11/145\n",
      "63/63 - 0s - loss: 503.4200 - mae: 503.4200 - val_loss: 526.7283 - val_mae: 526.7283\n",
      "Epoch 12/145\n",
      "63/63 - 0s - loss: 500.2989 - mae: 500.2989 - val_loss: 491.9368 - val_mae: 491.9368\n",
      "Epoch 13/145\n",
      "63/63 - 0s - loss: 492.8885 - mae: 492.8885 - val_loss: 492.5197 - val_mae: 492.5197\n",
      "Epoch 14/145\n",
      "63/63 - 0s - loss: 493.2229 - mae: 493.2229 - val_loss: 525.8315 - val_mae: 525.8315\n",
      "Epoch 15/145\n",
      "63/63 - 0s - loss: 526.8436 - mae: 526.8436 - val_loss: 546.8302 - val_mae: 546.8302\n",
      "Epoch 16/145\n",
      "63/63 - 0s - loss: 508.1032 - mae: 508.1032 - val_loss: 595.6614 - val_mae: 595.6614\n",
      "Epoch 17/145\n",
      "63/63 - 0s - loss: 575.8214 - mae: 575.8214 - val_loss: 511.9237 - val_mae: 511.9237\n",
      "Epoch 18/145\n",
      "63/63 - 0s - loss: 480.3573 - mae: 480.3573 - val_loss: 482.1093 - val_mae: 482.1093\n",
      "Epoch 19/145\n",
      "63/63 - 0s - loss: 475.0079 - mae: 475.0079 - val_loss: 507.9718 - val_mae: 507.9718\n",
      "Epoch 20/145\n",
      "63/63 - 0s - loss: 473.4138 - mae: 473.4138 - val_loss: 475.8751 - val_mae: 475.8751\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 0s - loss: 448.8957 - mae: 448.8957 - val_loss: 462.5564 - val_mae: 462.5564\n",
      "Epoch 22/145\n",
      "63/63 - 0s - loss: 445.4658 - mae: 445.4658 - val_loss: 458.8790 - val_mae: 458.8790\n",
      "Epoch 23/145\n",
      "63/63 - 0s - loss: 457.2866 - mae: 457.2866 - val_loss: 470.4141 - val_mae: 470.4141\n",
      "Epoch 24/145\n",
      "63/63 - 0s - loss: 476.2789 - mae: 476.2789 - val_loss: 471.6170 - val_mae: 471.6170\n",
      "Epoch 25/145\n",
      "63/63 - 0s - loss: 442.5258 - mae: 442.5258 - val_loss: 459.9444 - val_mae: 459.9444\n",
      "Epoch 26/145\n",
      "63/63 - 0s - loss: 438.0634 - mae: 438.0634 - val_loss: 459.4064 - val_mae: 459.4064\n",
      "Epoch 27/145\n",
      "63/63 - 0s - loss: 461.1868 - mae: 461.1868 - val_loss: 490.6774 - val_mae: 490.6774\n",
      "Epoch 28/145\n",
      "63/63 - 0s - loss: 484.5980 - mae: 484.5980 - val_loss: 465.7246 - val_mae: 465.7246\n",
      "Epoch 29/145\n",
      "63/63 - 0s - loss: 467.3418 - mae: 467.3418 - val_loss: 460.3248 - val_mae: 460.3248\n",
      "Epoch 30/145\n",
      "63/63 - 0s - loss: 438.7363 - mae: 438.7363 - val_loss: 472.6877 - val_mae: 472.6877\n",
      "Epoch 31/145\n",
      "63/63 - 0s - loss: 437.3373 - mae: 437.3373 - val_loss: 457.7414 - val_mae: 457.7414\n",
      "Epoch 32/145\n",
      "63/63 - 0s - loss: 435.1303 - mae: 435.1303 - val_loss: 459.1900 - val_mae: 459.1900\n",
      "Epoch 33/145\n",
      "63/63 - 0s - loss: 431.6396 - mae: 431.6396 - val_loss: 466.7291 - val_mae: 466.7291\n",
      "Epoch 34/145\n",
      "63/63 - 0s - loss: 435.2760 - mae: 435.2760 - val_loss: 454.5844 - val_mae: 454.5844\n",
      "Epoch 35/145\n",
      "63/63 - 1s - loss: 431.4798 - mae: 431.4798 - val_loss: 458.5829 - val_mae: 458.5829\n",
      "Epoch 36/145\n",
      "63/63 - 1s - loss: 451.1434 - mae: 451.1434 - val_loss: 453.4622 - val_mae: 453.4622\n",
      "Epoch 37/145\n",
      "63/63 - 1s - loss: 435.0463 - mae: 435.0463 - val_loss: 463.8178 - val_mae: 463.8178\n",
      "Epoch 38/145\n",
      "63/63 - 1s - loss: 437.8107 - mae: 437.8107 - val_loss: 467.7785 - val_mae: 467.7785\n",
      "Epoch 39/145\n",
      "63/63 - 1s - loss: 431.6259 - mae: 431.6259 - val_loss: 489.0072 - val_mae: 489.0072\n",
      "Epoch 40/145\n",
      "63/63 - 1s - loss: 438.2899 - mae: 438.2899 - val_loss: 481.6945 - val_mae: 481.6945\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 1s - loss: 417.5757 - mae: 417.5757 - val_loss: 441.1312 - val_mae: 441.1312\n",
      "Epoch 42/145\n",
      "63/63 - 1s - loss: 414.0195 - mae: 414.0195 - val_loss: 444.1726 - val_mae: 444.1726\n",
      "Epoch 43/145\n",
      "63/63 - 1s - loss: 414.0090 - mae: 414.0090 - val_loss: 441.3395 - val_mae: 441.3395\n",
      "Epoch 44/145\n",
      "63/63 - 0s - loss: 411.7033 - mae: 411.7033 - val_loss: 441.4127 - val_mae: 441.4127\n",
      "Epoch 45/145\n",
      "63/63 - 0s - loss: 410.9531 - mae: 410.9531 - val_loss: 439.6985 - val_mae: 439.6985\n",
      "Epoch 46/145\n",
      "63/63 - 0s - loss: 413.6787 - mae: 413.6787 - val_loss: 439.5583 - val_mae: 439.5583\n",
      "Epoch 47/145\n",
      "63/63 - 0s - loss: 410.2753 - mae: 410.2753 - val_loss: 440.3437 - val_mae: 440.3437\n",
      "Epoch 48/145\n",
      "63/63 - 0s - loss: 410.9977 - mae: 410.9977 - val_loss: 447.3756 - val_mae: 447.3756\n",
      "Epoch 49/145\n",
      "63/63 - 0s - loss: 412.0533 - mae: 412.0533 - val_loss: 444.5254 - val_mae: 444.5254\n",
      "Epoch 50/145\n",
      "63/63 - 0s - loss: 418.9772 - mae: 418.9772 - val_loss: 486.8801 - val_mae: 486.8801\n",
      "Epoch 51/145\n",
      "63/63 - 0s - loss: 414.9375 - mae: 414.9375 - val_loss: 437.3635 - val_mae: 437.3635\n",
      "Epoch 52/145\n",
      "63/63 - 0s - loss: 411.5793 - mae: 411.5793 - val_loss: 492.2746 - val_mae: 492.2746\n",
      "Epoch 53/145\n",
      "63/63 - 0s - loss: 411.8875 - mae: 411.8875 - val_loss: 444.6948 - val_mae: 444.6948\n",
      "Epoch 54/145\n",
      "63/63 - 0s - loss: 414.2024 - mae: 414.2024 - val_loss: 443.1396 - val_mae: 443.1396\n",
      "Epoch 55/145\n",
      "63/63 - 0s - loss: 406.1247 - mae: 406.1247 - val_loss: 436.2957 - val_mae: 436.2957\n",
      "Epoch 56/145\n",
      "63/63 - 0s - loss: 404.2741 - mae: 404.2741 - val_loss: 443.3080 - val_mae: 443.3080\n",
      "Epoch 57/145\n",
      "63/63 - 1s - loss: 405.8996 - mae: 405.8996 - val_loss: 476.0197 - val_mae: 476.0197\n",
      "Epoch 58/145\n",
      "63/63 - 1s - loss: 412.0423 - mae: 412.0423 - val_loss: 502.7306 - val_mae: 502.7306\n",
      "Epoch 59/145\n",
      "63/63 - 1s - loss: 414.8626 - mae: 414.8626 - val_loss: 459.4828 - val_mae: 459.4828\n",
      "Epoch 60/145\n",
      "63/63 - 1s - loss: 421.5541 - mae: 421.5541 - val_loss: 434.1521 - val_mae: 434.1521\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 1s - loss: 392.3661 - mae: 392.3661 - val_loss: 432.1089 - val_mae: 432.1089\n",
      "Epoch 62/145\n",
      "63/63 - 1s - loss: 392.0717 - mae: 392.0717 - val_loss: 429.6165 - val_mae: 429.6165\n",
      "Epoch 63/145\n",
      "63/63 - 1s - loss: 390.5546 - mae: 390.5546 - val_loss: 431.3052 - val_mae: 431.3052\n",
      "Epoch 64/145\n",
      "63/63 - 1s - loss: 391.1625 - mae: 391.1625 - val_loss: 429.5481 - val_mae: 429.5481\n",
      "Epoch 65/145\n",
      "63/63 - 1s - loss: 390.2727 - mae: 390.2727 - val_loss: 439.4042 - val_mae: 439.4042\n",
      "Epoch 66/145\n",
      "63/63 - 0s - loss: 390.9844 - mae: 390.9844 - val_loss: 432.0644 - val_mae: 432.0644\n",
      "Epoch 67/145\n",
      "63/63 - 0s - loss: 389.2367 - mae: 389.2367 - val_loss: 428.9064 - val_mae: 428.9064\n",
      "Epoch 68/145\n",
      "63/63 - 0s - loss: 388.4562 - mae: 388.4562 - val_loss: 432.6017 - val_mae: 432.6017\n",
      "Epoch 69/145\n",
      "63/63 - 0s - loss: 388.4574 - mae: 388.4574 - val_loss: 430.8000 - val_mae: 430.8000\n",
      "Epoch 70/145\n",
      "63/63 - 0s - loss: 388.8727 - mae: 388.8727 - val_loss: 430.6299 - val_mae: 430.6299\n",
      "Epoch 71/145\n",
      "63/63 - 0s - loss: 386.7859 - mae: 386.7859 - val_loss: 429.5226 - val_mae: 429.5226\n",
      "Epoch 72/145\n",
      "63/63 - 0s - loss: 388.0216 - mae: 388.0216 - val_loss: 455.8911 - val_mae: 455.8911\n",
      "Epoch 73/145\n",
      "63/63 - 0s - loss: 394.4422 - mae: 394.4422 - val_loss: 427.5085 - val_mae: 427.5085\n",
      "Epoch 74/145\n",
      "63/63 - 0s - loss: 385.6597 - mae: 385.6597 - val_loss: 429.2788 - val_mae: 429.2788\n",
      "Epoch 75/145\n",
      "63/63 - 0s - loss: 385.7150 - mae: 385.7150 - val_loss: 430.5779 - val_mae: 430.5779\n",
      "Epoch 76/145\n",
      "63/63 - 0s - loss: 386.9400 - mae: 386.9400 - val_loss: 427.6733 - val_mae: 427.6733\n",
      "Epoch 77/145\n",
      "63/63 - 0s - loss: 384.4739 - mae: 384.4739 - val_loss: 435.1497 - val_mae: 435.1497\n",
      "Epoch 78/145\n",
      "63/63 - 0s - loss: 385.5070 - mae: 385.5070 - val_loss: 430.8248 - val_mae: 430.8248\n",
      "Epoch 79/145\n",
      "63/63 - 0s - loss: 383.8269 - mae: 383.8269 - val_loss: 428.9174 - val_mae: 428.9174\n",
      "Epoch 80/145\n",
      "63/63 - 1s - loss: 383.7196 - mae: 383.7196 - val_loss: 433.5145 - val_mae: 433.5145\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 1s - loss: 379.5131 - mae: 379.5131 - val_loss: 426.9185 - val_mae: 426.9185\n",
      "Epoch 82/145\n",
      "63/63 - 1s - loss: 378.2475 - mae: 378.2475 - val_loss: 428.9751 - val_mae: 428.9751\n",
      "Epoch 83/145\n",
      "63/63 - 1s - loss: 377.7391 - mae: 377.7391 - val_loss: 427.1541 - val_mae: 427.1541\n",
      "Epoch 84/145\n",
      "63/63 - 1s - loss: 378.0617 - mae: 378.0617 - val_loss: 425.5030 - val_mae: 425.5030\n",
      "Epoch 85/145\n",
      "63/63 - 1s - loss: 376.6641 - mae: 376.6641 - val_loss: 427.0078 - val_mae: 427.0078\n",
      "Epoch 86/145\n",
      "63/63 - 1s - loss: 376.5163 - mae: 376.5163 - val_loss: 427.1869 - val_mae: 427.1869\n",
      "Epoch 87/145\n",
      "63/63 - 1s - loss: 376.9428 - mae: 376.9428 - val_loss: 426.8604 - val_mae: 426.8604\n",
      "Epoch 88/145\n",
      "63/63 - 0s - loss: 375.6519 - mae: 375.6519 - val_loss: 428.4950 - val_mae: 428.4950\n",
      "Epoch 89/145\n",
      "63/63 - 0s - loss: 374.8029 - mae: 374.8029 - val_loss: 427.7511 - val_mae: 427.7511\n",
      "Epoch 90/145\n",
      "63/63 - 0s - loss: 374.5808 - mae: 374.5808 - val_loss: 426.2162 - val_mae: 426.2162\n",
      "Epoch 91/145\n",
      "63/63 - 0s - loss: 373.9897 - mae: 373.9897 - val_loss: 425.9107 - val_mae: 425.9107\n",
      "Epoch 92/145\n",
      "63/63 - 0s - loss: 373.9987 - mae: 373.9987 - val_loss: 427.0314 - val_mae: 427.0314\n",
      "Epoch 93/145\n",
      "63/63 - 0s - loss: 375.1004 - mae: 375.1004 - val_loss: 429.8133 - val_mae: 429.8133\n",
      "Epoch 94/145\n",
      "63/63 - 0s - loss: 373.9583 - mae: 373.9583 - val_loss: 430.5565 - val_mae: 430.5565\n",
      "Epoch 95/145\n",
      "63/63 - 0s - loss: 376.1073 - mae: 376.1073 - val_loss: 430.1929 - val_mae: 430.1929\n",
      "Epoch 96/145\n",
      "63/63 - 0s - loss: 374.3669 - mae: 374.3669 - val_loss: 425.9956 - val_mae: 425.9956\n",
      "Epoch 97/145\n",
      "63/63 - 0s - loss: 372.5273 - mae: 372.5273 - val_loss: 430.7626 - val_mae: 430.7626\n",
      "Epoch 98/145\n",
      "63/63 - 0s - loss: 373.0495 - mae: 373.0495 - val_loss: 429.5643 - val_mae: 429.5643\n",
      "Epoch 99/145\n",
      "63/63 - 0s - loss: 372.6595 - mae: 372.6595 - val_loss: 427.5478 - val_mae: 427.5478\n",
      "Epoch 100/145\n",
      "63/63 - 0s - loss: 372.7070 - mae: 372.7070 - val_loss: 429.8918 - val_mae: 429.8918\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 0s - loss: 368.5735 - mae: 368.5735 - val_loss: 426.0396 - val_mae: 426.0396\n",
      "Epoch 102/145\n",
      "63/63 - 1s - loss: 368.2080 - mae: 368.2080 - val_loss: 425.8278 - val_mae: 425.8278\n",
      "Epoch 103/145\n",
      "63/63 - 1s - loss: 368.1301 - mae: 368.1301 - val_loss: 425.6729 - val_mae: 425.6729\n",
      "Epoch 104/145\n",
      "63/63 - 1s - loss: 367.8136 - mae: 367.8136 - val_loss: 427.1241 - val_mae: 427.1241\n",
      "Epoch 105/145\n",
      "63/63 - 1s - loss: 368.3660 - mae: 368.3660 - val_loss: 425.2079 - val_mae: 425.2079\n",
      "Epoch 106/145\n",
      "63/63 - 1s - loss: 367.4725 - mae: 367.4725 - val_loss: 425.4347 - val_mae: 425.4347\n",
      "Epoch 107/145\n",
      "63/63 - 1s - loss: 366.4677 - mae: 366.4677 - val_loss: 426.8736 - val_mae: 426.8736\n",
      "Epoch 108/145\n",
      "63/63 - 1s - loss: 366.5790 - mae: 366.5790 - val_loss: 425.7145 - val_mae: 425.7145\n",
      "Epoch 109/145\n",
      "63/63 - 1s - loss: 366.3314 - mae: 366.3314 - val_loss: 429.2029 - val_mae: 429.2029\n",
      "Epoch 110/145\n",
      "63/63 - 1s - loss: 366.1161 - mae: 366.1161 - val_loss: 426.9355 - val_mae: 426.9355\n",
      "Epoch 111/145\n",
      "63/63 - 0s - loss: 366.2139 - mae: 366.2139 - val_loss: 427.3392 - val_mae: 427.3392\n",
      "Epoch 112/145\n",
      "63/63 - 0s - loss: 366.0192 - mae: 366.0192 - val_loss: 426.8128 - val_mae: 426.8128\n",
      "Epoch 113/145\n",
      "63/63 - 0s - loss: 365.0508 - mae: 365.0508 - val_loss: 426.1666 - val_mae: 426.1666\n",
      "Epoch 114/145\n",
      "63/63 - 0s - loss: 365.1402 - mae: 365.1402 - val_loss: 426.0545 - val_mae: 426.0545\n",
      "Epoch 115/145\n",
      "63/63 - 0s - loss: 364.5522 - mae: 364.5522 - val_loss: 425.7898 - val_mae: 425.7898\n",
      "Epoch 116/145\n",
      "63/63 - 0s - loss: 364.6739 - mae: 364.6739 - val_loss: 426.2690 - val_mae: 426.2690\n",
      "Epoch 117/145\n",
      "63/63 - 0s - loss: 364.7804 - mae: 364.7804 - val_loss: 425.9986 - val_mae: 425.9986\n",
      "Epoch 118/145\n",
      "63/63 - 0s - loss: 364.0583 - mae: 364.0583 - val_loss: 427.5777 - val_mae: 427.5777\n",
      "Epoch 119/145\n",
      "63/63 - 0s - loss: 363.9386 - mae: 363.9386 - val_loss: 426.3204 - val_mae: 426.3204\n",
      "Epoch 120/145\n",
      "63/63 - 0s - loss: 363.7588 - mae: 363.7588 - val_loss: 426.9761 - val_mae: 426.9761\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 0s - loss: 361.7215 - mae: 361.7215 - val_loss: 426.1308 - val_mae: 426.1308\n",
      "Epoch 122/145\n",
      "63/63 - 0s - loss: 361.2696 - mae: 361.2696 - val_loss: 425.3146 - val_mae: 425.3146\n",
      "Epoch 123/145\n",
      "63/63 - 0s - loss: 361.1797 - mae: 361.1797 - val_loss: 425.6407 - val_mae: 425.6407\n",
      "Epoch 124/145\n",
      "63/63 - 0s - loss: 361.3724 - mae: 361.3724 - val_loss: 426.1517 - val_mae: 426.1517\n",
      "Epoch 125/145\n",
      "63/63 - 0s - loss: 361.0026 - mae: 361.0026 - val_loss: 426.3588 - val_mae: 426.3588\n",
      "Epoch 126/145\n",
      "63/63 - 0s - loss: 361.1775 - mae: 361.1775 - val_loss: 425.5240 - val_mae: 425.5240\n",
      "Epoch 127/145\n",
      "63/63 - 0s - loss: 360.4627 - mae: 360.4627 - val_loss: 425.1528 - val_mae: 425.1528\n",
      "Epoch 128/145\n",
      "63/63 - 0s - loss: 360.6259 - mae: 360.6259 - val_loss: 426.1379 - val_mae: 426.1379\n",
      "Epoch 129/145\n",
      "63/63 - 0s - loss: 360.4948 - mae: 360.4948 - val_loss: 425.9572 - val_mae: 425.9572\n",
      "Epoch 130/145\n",
      "63/63 - 0s - loss: 360.2597 - mae: 360.2597 - val_loss: 426.0123 - val_mae: 426.0123\n",
      "Epoch 131/145\n",
      "63/63 - 0s - loss: 359.8073 - mae: 359.8073 - val_loss: 426.9502 - val_mae: 426.9502\n",
      "Epoch 132/145\n",
      "63/63 - 0s - loss: 360.5064 - mae: 360.5064 - val_loss: 426.1579 - val_mae: 426.1579\n",
      "Epoch 133/145\n",
      "63/63 - 0s - loss: 359.7211 - mae: 359.7211 - val_loss: 427.2234 - val_mae: 427.2234\n",
      "Epoch 134/145\n",
      "63/63 - 0s - loss: 359.8080 - mae: 359.8080 - val_loss: 426.0860 - val_mae: 426.0860\n",
      "Epoch 135/145\n",
      "63/63 - 0s - loss: 359.1483 - mae: 359.1483 - val_loss: 426.9602 - val_mae: 426.9602\n",
      "Epoch 136/145\n",
      "63/63 - 0s - loss: 359.7097 - mae: 359.7097 - val_loss: 427.3687 - val_mae: 427.3687\n",
      "Epoch 137/145\n",
      "63/63 - 0s - loss: 359.4593 - mae: 359.4593 - val_loss: 426.6484 - val_mae: 426.6484\n",
      "Epoch 138/145\n",
      "63/63 - 0s - loss: 359.2219 - mae: 359.2219 - val_loss: 426.0606 - val_mae: 426.0606\n",
      "Epoch 139/145\n",
      "63/63 - 0s - loss: 358.7607 - mae: 358.7607 - val_loss: 426.2102 - val_mae: 426.2102\n",
      "Epoch 140/145\n",
      "63/63 - 0s - loss: 358.3198 - mae: 358.3198 - val_loss: 426.2921 - val_mae: 426.2921\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 0s - loss: 357.8972 - mae: 357.8972 - val_loss: 425.6168 - val_mae: 425.6168\n",
      "Epoch 142/145\n",
      "63/63 - 0s - loss: 357.6331 - mae: 357.6331 - val_loss: 426.2405 - val_mae: 426.2405\n",
      "Epoch 143/145\n",
      "63/63 - 0s - loss: 356.8888 - mae: 356.8888 - val_loss: 425.2714 - val_mae: 425.2714\n",
      "Epoch 144/145\n",
      "63/63 - 0s - loss: 356.8324 - mae: 356.8324 - val_loss: 425.5851 - val_mae: 425.5851\n",
      "Epoch 145/145\n",
      "63/63 - 0s - loss: 356.6783 - mae: 356.6783 - val_loss: 425.5753 - val_mae: 425.5753\n",
      "\n",
      "val_mae is:425.5753302679443\n",
      "\n",
      "fold: 4\n",
      "X_train shape: (125000, 146)\n",
      "y_train shape: (125000,)\n",
      "X_val shape: (25000, 146)\n",
      "y_val shape: (25000,)\n",
      "Epoch 1/145\n",
      "63/63 - 1s - loss: 2606.3167 - mae: 2606.3167 - val_loss: 903.6755 - val_mae: 903.6755\n",
      "Epoch 2/145\n",
      "63/63 - 0s - loss: 785.3157 - mae: 785.3157 - val_loss: 687.9163 - val_mae: 687.9163\n",
      "Epoch 3/145\n",
      "63/63 - 0s - loss: 670.5891 - mae: 670.5891 - val_loss: 663.1284 - val_mae: 663.1284\n",
      "Epoch 4/145\n",
      "63/63 - 0s - loss: 613.3296 - mae: 613.3296 - val_loss: 568.6252 - val_mae: 568.6252\n",
      "Epoch 5/145\n",
      "63/63 - 0s - loss: 581.8427 - mae: 581.8427 - val_loss: 617.2349 - val_mae: 617.2349\n",
      "Epoch 6/145\n",
      "63/63 - 0s - loss: 562.2985 - mae: 562.2985 - val_loss: 546.1033 - val_mae: 546.1033\n",
      "Epoch 7/145\n",
      "63/63 - 0s - loss: 540.9581 - mae: 540.9581 - val_loss: 530.4934 - val_mae: 530.4934\n",
      "Epoch 8/145\n",
      "63/63 - 0s - loss: 525.6918 - mae: 525.6918 - val_loss: 537.5127 - val_mae: 537.5127\n",
      "Epoch 9/145\n",
      "63/63 - 0s - loss: 510.4125 - mae: 510.4125 - val_loss: 582.8464 - val_mae: 582.8464\n",
      "Epoch 10/145\n",
      "63/63 - 0s - loss: 596.9184 - mae: 596.9184 - val_loss: 504.7874 - val_mae: 504.7874\n",
      "Epoch 11/145\n",
      "63/63 - 0s - loss: 508.0094 - mae: 508.0094 - val_loss: 552.0566 - val_mae: 552.0566\n",
      "Epoch 12/145\n",
      "63/63 - 0s - loss: 575.1154 - mae: 575.1154 - val_loss: 589.1150 - val_mae: 589.1150\n",
      "Epoch 13/145\n",
      "63/63 - 0s - loss: 500.7225 - mae: 500.7225 - val_loss: 513.5013 - val_mae: 513.5013\n",
      "Epoch 14/145\n",
      "63/63 - 1s - loss: 528.1358 - mae: 528.1358 - val_loss: 486.2797 - val_mae: 486.2797\n",
      "Epoch 15/145\n",
      "63/63 - 1s - loss: 509.0222 - mae: 509.0222 - val_loss: 484.8974 - val_mae: 484.8974\n",
      "Epoch 16/145\n",
      "63/63 - 1s - loss: 480.6957 - mae: 480.6957 - val_loss: 477.9325 - val_mae: 477.9325\n",
      "Epoch 17/145\n",
      "63/63 - 1s - loss: 532.3176 - mae: 532.3176 - val_loss: 517.8864 - val_mae: 517.8864\n",
      "Epoch 18/145\n",
      "63/63 - 1s - loss: 505.9726 - mae: 505.9726 - val_loss: 463.9791 - val_mae: 463.9791\n",
      "Epoch 19/145\n",
      "63/63 - 1s - loss: 467.4860 - mae: 467.4860 - val_loss: 494.7550 - val_mae: 494.7550\n",
      "Epoch 20/145\n",
      "63/63 - 1s - loss: 469.1778 - mae: 469.1778 - val_loss: 469.1699 - val_mae: 469.1699\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 1s - loss: 445.6463 - mae: 445.6463 - val_loss: 455.1478 - val_mae: 455.1478\n",
      "Epoch 22/145\n",
      "63/63 - 1s - loss: 441.6266 - mae: 441.6266 - val_loss: 456.8508 - val_mae: 456.8508\n",
      "Epoch 23/145\n",
      "63/63 - 0s - loss: 440.5081 - mae: 440.5081 - val_loss: 450.9288 - val_mae: 450.9288\n",
      "Epoch 24/145\n",
      "63/63 - 0s - loss: 439.8147 - mae: 439.8147 - val_loss: 444.5167 - val_mae: 444.5167\n",
      "Epoch 25/145\n",
      "63/63 - 0s - loss: 442.0824 - mae: 442.0824 - val_loss: 449.1488 - val_mae: 449.1488\n",
      "Epoch 26/145\n",
      "63/63 - 0s - loss: 440.6158 - mae: 440.6158 - val_loss: 450.2426 - val_mae: 450.2426\n",
      "Epoch 27/145\n",
      "63/63 - 0s - loss: 440.9930 - mae: 440.9930 - val_loss: 443.3192 - val_mae: 443.3192\n",
      "Epoch 28/145\n",
      "63/63 - 0s - loss: 441.6837 - mae: 441.6837 - val_loss: 490.9578 - val_mae: 490.9578\n",
      "Epoch 29/145\n",
      "63/63 - 0s - loss: 461.2230 - mae: 461.2230 - val_loss: 458.5074 - val_mae: 458.5074\n",
      "Epoch 30/145\n",
      "63/63 - 0s - loss: 436.3619 - mae: 436.3619 - val_loss: 445.3085 - val_mae: 445.3085\n",
      "Epoch 31/145\n",
      "63/63 - 0s - loss: 438.8896 - mae: 438.8896 - val_loss: 448.1891 - val_mae: 448.1891\n",
      "Epoch 32/145\n",
      "63/63 - 0s - loss: 434.6600 - mae: 434.6600 - val_loss: 448.1930 - val_mae: 448.1929\n",
      "Epoch 33/145\n",
      "63/63 - 0s - loss: 443.5113 - mae: 443.5113 - val_loss: 442.5030 - val_mae: 442.5030\n",
      "Epoch 34/145\n",
      "63/63 - 0s - loss: 432.8030 - mae: 432.8030 - val_loss: 453.9404 - val_mae: 453.9404\n",
      "Epoch 35/145\n",
      "63/63 - 0s - loss: 430.0883 - mae: 430.0883 - val_loss: 452.5984 - val_mae: 452.5984\n",
      "Epoch 36/145\n",
      "63/63 - 0s - loss: 427.8203 - mae: 427.8203 - val_loss: 443.2240 - val_mae: 443.2240\n",
      "Epoch 37/145\n",
      "63/63 - 1s - loss: 427.2528 - mae: 427.2528 - val_loss: 439.8589 - val_mae: 439.8589\n",
      "Epoch 38/145\n",
      "63/63 - 1s - loss: 427.1155 - mae: 427.1155 - val_loss: 465.4375 - val_mae: 465.4375\n",
      "Epoch 39/145\n",
      "63/63 - 1s - loss: 428.1693 - mae: 428.1693 - val_loss: 450.4756 - val_mae: 450.4756\n",
      "Epoch 40/145\n",
      "63/63 - 1s - loss: 426.6734 - mae: 426.6734 - val_loss: 434.1105 - val_mae: 434.1105\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 1s - loss: 413.6025 - mae: 413.6025 - val_loss: 459.2402 - val_mae: 459.2402\n",
      "Epoch 42/145\n",
      "63/63 - 1s - loss: 415.3654 - mae: 415.3654 - val_loss: 429.6249 - val_mae: 429.6249\n",
      "Epoch 43/145\n",
      "63/63 - 1s - loss: 409.3140 - mae: 409.3140 - val_loss: 430.0632 - val_mae: 430.0632\n",
      "Epoch 44/145\n",
      "63/63 - 1s - loss: 407.0626 - mae: 407.0626 - val_loss: 437.8959 - val_mae: 437.8959\n",
      "Epoch 45/145\n",
      "63/63 - 0s - loss: 414.1097 - mae: 414.1097 - val_loss: 429.0818 - val_mae: 429.0818\n",
      "Epoch 46/145\n",
      "63/63 - 0s - loss: 410.0918 - mae: 410.0918 - val_loss: 431.6776 - val_mae: 431.6776\n",
      "Epoch 47/145\n",
      "63/63 - 0s - loss: 410.1487 - mae: 410.1487 - val_loss: 430.8378 - val_mae: 430.8378\n",
      "Epoch 48/145\n",
      "63/63 - 0s - loss: 408.8487 - mae: 408.8487 - val_loss: 434.4538 - val_mae: 434.4538\n",
      "Epoch 49/145\n",
      "63/63 - 0s - loss: 406.4148 - mae: 406.4148 - val_loss: 425.9476 - val_mae: 425.9476\n",
      "Epoch 50/145\n",
      "63/63 - 0s - loss: 408.1052 - mae: 408.1052 - val_loss: 429.3073 - val_mae: 429.3073\n",
      "Epoch 51/145\n",
      "63/63 - 0s - loss: 406.0056 - mae: 406.0056 - val_loss: 428.5905 - val_mae: 428.5905\n",
      "Epoch 52/145\n",
      "63/63 - 0s - loss: 404.6096 - mae: 404.6096 - val_loss: 426.1406 - val_mae: 426.1406\n",
      "Epoch 53/145\n",
      "63/63 - 0s - loss: 407.3294 - mae: 407.3294 - val_loss: 428.8862 - val_mae: 428.8862\n",
      "Epoch 54/145\n",
      "63/63 - 0s - loss: 402.6687 - mae: 402.6687 - val_loss: 428.9077 - val_mae: 428.9077\n",
      "Epoch 55/145\n",
      "63/63 - 0s - loss: 423.7852 - mae: 423.7852 - val_loss: 456.6349 - val_mae: 456.6349\n",
      "Epoch 56/145\n",
      "63/63 - 0s - loss: 409.5631 - mae: 409.5631 - val_loss: 425.1708 - val_mae: 425.1708\n",
      "Epoch 57/145\n",
      "63/63 - 0s - loss: 403.4062 - mae: 403.4062 - val_loss: 428.9440 - val_mae: 428.9440\n",
      "Epoch 58/145\n",
      "63/63 - 1s - loss: 404.3599 - mae: 404.3599 - val_loss: 436.6047 - val_mae: 436.6047\n",
      "Epoch 59/145\n",
      "63/63 - 1s - loss: 411.5882 - mae: 411.5882 - val_loss: 443.0984 - val_mae: 443.0984\n",
      "Epoch 60/145\n",
      "63/63 - 1s - loss: 406.3178 - mae: 406.3178 - val_loss: 429.0678 - val_mae: 429.0678\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 1s - loss: 397.2438 - mae: 397.2438 - val_loss: 421.9780 - val_mae: 421.9780\n",
      "Epoch 62/145\n",
      "63/63 - 1s - loss: 392.1827 - mae: 392.1827 - val_loss: 426.0463 - val_mae: 426.0463\n",
      "Epoch 63/145\n",
      "63/63 - 1s - loss: 393.1009 - mae: 393.1009 - val_loss: 430.6800 - val_mae: 430.6800\n",
      "Epoch 64/145\n",
      "63/63 - 1s - loss: 392.3915 - mae: 392.3915 - val_loss: 422.1824 - val_mae: 422.1824\n",
      "Epoch 65/145\n",
      "63/63 - 1s - loss: 393.4715 - mae: 393.4715 - val_loss: 422.6719 - val_mae: 422.6719\n",
      "Epoch 66/145\n",
      "63/63 - 1s - loss: 389.6128 - mae: 389.6128 - val_loss: 421.9966 - val_mae: 421.9966\n",
      "Epoch 67/145\n",
      "63/63 - 0s - loss: 388.8790 - mae: 388.8790 - val_loss: 423.2822 - val_mae: 423.2822\n",
      "Epoch 68/145\n",
      "63/63 - 0s - loss: 391.3417 - mae: 391.3417 - val_loss: 423.1046 - val_mae: 423.1046\n",
      "Epoch 69/145\n",
      "63/63 - 0s - loss: 389.9221 - mae: 389.9221 - val_loss: 420.0972 - val_mae: 420.0972\n",
      "Epoch 70/145\n",
      "63/63 - 0s - loss: 389.4125 - mae: 389.4125 - val_loss: 424.7122 - val_mae: 424.7122\n",
      "Epoch 71/145\n",
      "63/63 - 0s - loss: 388.1041 - mae: 388.1041 - val_loss: 423.8577 - val_mae: 423.8577\n",
      "Epoch 72/145\n",
      "63/63 - 0s - loss: 389.3551 - mae: 389.3551 - val_loss: 421.3615 - val_mae: 421.3615\n",
      "Epoch 73/145\n",
      "63/63 - 0s - loss: 389.3029 - mae: 389.3029 - val_loss: 456.4651 - val_mae: 456.4651\n",
      "Epoch 74/145\n",
      "63/63 - 0s - loss: 397.0164 - mae: 397.0164 - val_loss: 420.8822 - val_mae: 420.8822\n",
      "Epoch 75/145\n",
      "63/63 - 0s - loss: 386.1073 - mae: 386.1073 - val_loss: 421.7647 - val_mae: 421.7647\n",
      "Epoch 76/145\n",
      "63/63 - 0s - loss: 387.3684 - mae: 387.3684 - val_loss: 428.0771 - val_mae: 428.0771\n",
      "Epoch 77/145\n",
      "63/63 - 0s - loss: 385.9978 - mae: 385.9978 - val_loss: 424.6068 - val_mae: 424.6068\n",
      "Epoch 78/145\n",
      "63/63 - 0s - loss: 385.1582 - mae: 385.1582 - val_loss: 421.8331 - val_mae: 421.8331\n",
      "Epoch 79/145\n",
      "63/63 - 0s - loss: 384.0645 - mae: 384.0645 - val_loss: 420.8752 - val_mae: 420.8752\n",
      "Epoch 80/145\n",
      "63/63 - 0s - loss: 383.0192 - mae: 383.0192 - val_loss: 421.8190 - val_mae: 421.8190\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 0s - loss: 379.6663 - mae: 379.6663 - val_loss: 418.0599 - val_mae: 418.0599\n",
      "Epoch 82/145\n",
      "63/63 - 0s - loss: 378.8696 - mae: 378.8696 - val_loss: 417.3973 - val_mae: 417.3973\n",
      "Epoch 83/145\n",
      "63/63 - 0s - loss: 378.7564 - mae: 378.7564 - val_loss: 419.9498 - val_mae: 419.9498\n",
      "Epoch 84/145\n",
      "63/63 - 0s - loss: 379.1812 - mae: 379.1812 - val_loss: 418.3307 - val_mae: 418.3307\n",
      "Epoch 85/145\n",
      "63/63 - 0s - loss: 379.0130 - mae: 379.0130 - val_loss: 418.0627 - val_mae: 418.0627\n",
      "Epoch 86/145\n",
      "63/63 - 0s - loss: 378.3615 - mae: 378.3615 - val_loss: 417.5383 - val_mae: 417.5383\n",
      "Epoch 87/145\n",
      "63/63 - 0s - loss: 376.7824 - mae: 376.7824 - val_loss: 418.8448 - val_mae: 418.8448\n",
      "Epoch 88/145\n",
      "63/63 - 0s - loss: 376.8289 - mae: 376.8289 - val_loss: 418.8491 - val_mae: 418.8491\n",
      "Epoch 89/145\n",
      "63/63 - 0s - loss: 378.8823 - mae: 378.8823 - val_loss: 418.0681 - val_mae: 418.0681\n",
      "Epoch 90/145\n",
      "63/63 - 0s - loss: 376.9208 - mae: 376.9208 - val_loss: 418.0632 - val_mae: 418.0632\n",
      "Epoch 91/145\n",
      "63/63 - 0s - loss: 376.3423 - mae: 376.3423 - val_loss: 420.6080 - val_mae: 420.6080\n",
      "Epoch 92/145\n",
      "63/63 - 0s - loss: 375.3262 - mae: 375.3262 - val_loss: 418.7508 - val_mae: 418.7508\n",
      "Epoch 93/145\n",
      "63/63 - 0s - loss: 374.9628 - mae: 374.9628 - val_loss: 417.7998 - val_mae: 417.7998\n",
      "Epoch 94/145\n",
      "63/63 - 0s - loss: 375.6590 - mae: 375.6590 - val_loss: 418.8916 - val_mae: 418.8916\n",
      "Epoch 95/145\n",
      "63/63 - 0s - loss: 374.9642 - mae: 374.9642 - val_loss: 419.2690 - val_mae: 419.2690\n",
      "Epoch 96/145\n",
      "63/63 - 0s - loss: 375.1781 - mae: 375.1781 - val_loss: 419.0931 - val_mae: 419.0931\n",
      "Epoch 97/145\n",
      "63/63 - 0s - loss: 375.7655 - mae: 375.7655 - val_loss: 430.2276 - val_mae: 430.2276\n",
      "Epoch 98/145\n",
      "63/63 - 0s - loss: 374.4388 - mae: 374.4388 - val_loss: 416.6481 - val_mae: 416.6481\n",
      "Epoch 99/145\n",
      "63/63 - 0s - loss: 375.5677 - mae: 375.5677 - val_loss: 432.8423 - val_mae: 432.8423\n",
      "Epoch 100/145\n",
      "63/63 - 0s - loss: 376.4583 - mae: 376.4583 - val_loss: 416.9606 - val_mae: 416.9606\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 0s - loss: 370.4540 - mae: 370.4540 - val_loss: 416.2686 - val_mae: 416.2686\n",
      "Epoch 102/145\n",
      "63/63 - 0s - loss: 370.5203 - mae: 370.5203 - val_loss: 416.3419 - val_mae: 416.3419\n",
      "Epoch 103/145\n",
      "63/63 - 0s - loss: 369.2961 - mae: 369.2961 - val_loss: 415.8657 - val_mae: 415.8657\n",
      "Epoch 104/145\n",
      "63/63 - 0s - loss: 368.7408 - mae: 368.7408 - val_loss: 416.8442 - val_mae: 416.8442\n",
      "Epoch 105/145\n",
      "63/63 - 0s - loss: 368.4393 - mae: 368.4393 - val_loss: 416.9305 - val_mae: 416.9305\n",
      "Epoch 106/145\n",
      "63/63 - 0s - loss: 370.1970 - mae: 370.1970 - val_loss: 416.5528 - val_mae: 416.5528\n",
      "Epoch 107/145\n",
      "63/63 - 0s - loss: 368.5418 - mae: 368.5418 - val_loss: 418.9048 - val_mae: 418.9048\n",
      "Epoch 108/145\n",
      "63/63 - 0s - loss: 368.5937 - mae: 368.5937 - val_loss: 416.4250 - val_mae: 416.4250\n",
      "Epoch 109/145\n",
      "63/63 - 0s - loss: 368.0300 - mae: 368.0300 - val_loss: 416.6854 - val_mae: 416.6854\n",
      "Epoch 110/145\n",
      "63/63 - 0s - loss: 368.1603 - mae: 368.1603 - val_loss: 417.4938 - val_mae: 417.4938\n",
      "Epoch 111/145\n",
      "63/63 - 0s - loss: 367.9124 - mae: 367.9124 - val_loss: 417.9038 - val_mae: 417.9038\n",
      "Epoch 112/145\n",
      "63/63 - 0s - loss: 367.6425 - mae: 367.6425 - val_loss: 417.0634 - val_mae: 417.0634\n",
      "Epoch 113/145\n",
      "63/63 - 0s - loss: 366.9030 - mae: 366.9030 - val_loss: 416.0773 - val_mae: 416.0773\n",
      "Epoch 114/145\n",
      "63/63 - 0s - loss: 366.4915 - mae: 366.4915 - val_loss: 417.6116 - val_mae: 417.6116\n",
      "Epoch 115/145\n",
      "63/63 - 0s - loss: 367.7000 - mae: 367.7000 - val_loss: 417.4294 - val_mae: 417.4294\n",
      "Epoch 116/145\n",
      "63/63 - 0s - loss: 366.5717 - mae: 366.5717 - val_loss: 416.7119 - val_mae: 416.7119\n",
      "Epoch 117/145\n",
      "63/63 - 0s - loss: 366.2017 - mae: 366.2017 - val_loss: 418.2656 - val_mae: 418.2656\n",
      "Epoch 118/145\n",
      "63/63 - 0s - loss: 367.0343 - mae: 367.0343 - val_loss: 416.5629 - val_mae: 416.5629\n",
      "Epoch 119/145\n",
      "63/63 - 0s - loss: 367.1006 - mae: 367.1006 - val_loss: 416.9730 - val_mae: 416.9730\n",
      "Epoch 120/145\n",
      "63/63 - 0s - loss: 365.8661 - mae: 365.8661 - val_loss: 418.9628 - val_mae: 418.9628\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 0s - loss: 363.5946 - mae: 363.5946 - val_loss: 415.7379 - val_mae: 415.7379\n",
      "Epoch 122/145\n",
      "63/63 - 0s - loss: 363.2267 - mae: 363.2267 - val_loss: 415.7698 - val_mae: 415.7698\n",
      "Epoch 123/145\n",
      "63/63 - 0s - loss: 363.1392 - mae: 363.1392 - val_loss: 416.3129 - val_mae: 416.3129\n",
      "Epoch 124/145\n",
      "63/63 - 0s - loss: 362.9366 - mae: 362.9366 - val_loss: 416.1890 - val_mae: 416.1890\n",
      "Epoch 125/145\n",
      "63/63 - 0s - loss: 362.8682 - mae: 362.8682 - val_loss: 415.8624 - val_mae: 415.8624\n",
      "Epoch 126/145\n",
      "63/63 - 0s - loss: 362.4830 - mae: 362.4830 - val_loss: 416.0204 - val_mae: 416.0204\n",
      "Epoch 127/145\n",
      "63/63 - 0s - loss: 362.3977 - mae: 362.3977 - val_loss: 416.1985 - val_mae: 416.1985\n",
      "Epoch 128/145\n",
      "63/63 - 0s - loss: 362.2759 - mae: 362.2759 - val_loss: 416.0502 - val_mae: 416.0502\n",
      "Epoch 129/145\n",
      "63/63 - 0s - loss: 362.7353 - mae: 362.7353 - val_loss: 416.8044 - val_mae: 416.8044\n",
      "Epoch 130/145\n",
      "63/63 - 0s - loss: 362.1237 - mae: 362.1237 - val_loss: 417.1185 - val_mae: 417.1185\n",
      "Epoch 131/145\n",
      "63/63 - 1s - loss: 362.2928 - mae: 362.2928 - val_loss: 417.0444 - val_mae: 417.0444\n",
      "Epoch 132/145\n",
      "63/63 - 1s - loss: 361.6903 - mae: 361.6903 - val_loss: 415.3011 - val_mae: 415.3011\n",
      "Epoch 133/145\n",
      "63/63 - 1s - loss: 361.4497 - mae: 361.4497 - val_loss: 416.0192 - val_mae: 416.0192\n",
      "Epoch 134/145\n",
      "63/63 - 1s - loss: 361.0066 - mae: 361.0066 - val_loss: 415.9293 - val_mae: 415.9293\n",
      "Epoch 135/145\n",
      "63/63 - 1s - loss: 360.6714 - mae: 360.6714 - val_loss: 416.8265 - val_mae: 416.8265\n",
      "Epoch 136/145\n",
      "63/63 - 1s - loss: 361.7828 - mae: 361.7828 - val_loss: 415.7234 - val_mae: 415.7234\n",
      "Epoch 137/145\n",
      "63/63 - 1s - loss: 360.6629 - mae: 360.6629 - val_loss: 416.3119 - val_mae: 416.3119\n",
      "Epoch 138/145\n",
      "63/63 - 1s - loss: 360.6447 - mae: 360.6447 - val_loss: 415.4455 - val_mae: 415.4455\n",
      "Epoch 139/145\n",
      "63/63 - 0s - loss: 360.6236 - mae: 360.6236 - val_loss: 415.8140 - val_mae: 415.8140\n",
      "Epoch 140/145\n",
      "63/63 - 0s - loss: 360.2475 - mae: 360.2475 - val_loss: 415.9921 - val_mae: 415.9921\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 0s - loss: 359.5511 - mae: 359.5511 - val_loss: 415.9810 - val_mae: 415.9810\n",
      "Epoch 142/145\n",
      "63/63 - 0s - loss: 359.0896 - mae: 359.0896 - val_loss: 415.4929 - val_mae: 415.4929\n",
      "Epoch 143/145\n",
      "63/63 - 0s - loss: 358.8971 - mae: 358.8971 - val_loss: 415.2032 - val_mae: 415.2032\n",
      "Epoch 144/145\n",
      "63/63 - 0s - loss: 358.4226 - mae: 358.4226 - val_loss: 416.0621 - val_mae: 416.0621\n",
      "Epoch 145/145\n",
      "63/63 - 0s - loss: 358.3080 - mae: 358.3080 - val_loss: 415.3596 - val_mae: 415.3596\n",
      "\n",
      "val_mae is:415.3595781953812\n",
      "\n",
      "fold: 5\n",
      "X_train shape: (125000, 146)\n",
      "y_train shape: (125000,)\n",
      "X_val shape: (25000, 146)\n",
      "y_val shape: (25000,)\n",
      "Epoch 1/145\n",
      "63/63 - 1s - loss: 2817.5122 - mae: 2817.5122 - val_loss: 973.0909 - val_mae: 973.0909\n",
      "Epoch 2/145\n",
      "63/63 - 1s - loss: 819.9130 - mae: 819.9130 - val_loss: 789.8533 - val_mae: 789.8533\n",
      "Epoch 3/145\n",
      "63/63 - 1s - loss: 687.4046 - mae: 687.4046 - val_loss: 672.7626 - val_mae: 672.7626\n",
      "Epoch 4/145\n",
      "63/63 - 1s - loss: 614.0251 - mae: 614.0251 - val_loss: 587.9833 - val_mae: 587.9833\n",
      "Epoch 5/145\n",
      "63/63 - 1s - loss: 587.0410 - mae: 587.0410 - val_loss: 582.8359 - val_mae: 582.8359\n",
      "Epoch 6/145\n",
      "63/63 - 0s - loss: 562.7867 - mae: 562.7867 - val_loss: 563.5453 - val_mae: 563.5453\n",
      "Epoch 7/145\n",
      "63/63 - 0s - loss: 561.2720 - mae: 561.2720 - val_loss: 668.5500 - val_mae: 668.5500\n",
      "Epoch 8/145\n",
      "63/63 - 0s - loss: 573.5758 - mae: 573.5758 - val_loss: 585.1183 - val_mae: 585.1183\n",
      "Epoch 9/145\n",
      "63/63 - 0s - loss: 581.4792 - mae: 581.4792 - val_loss: 548.2649 - val_mae: 548.2649\n",
      "Epoch 10/145\n",
      "63/63 - 0s - loss: 522.1696 - mae: 522.1696 - val_loss: 500.0538 - val_mae: 500.0538\n",
      "Epoch 11/145\n",
      "63/63 - 0s - loss: 498.7315 - mae: 498.7315 - val_loss: 495.4466 - val_mae: 495.4466\n",
      "Epoch 12/145\n",
      "63/63 - 0s - loss: 493.5591 - mae: 493.5591 - val_loss: 502.3896 - val_mae: 502.3896\n",
      "Epoch 13/145\n",
      "63/63 - 0s - loss: 490.2367 - mae: 490.2367 - val_loss: 506.7032 - val_mae: 506.7032\n",
      "Epoch 14/145\n",
      "63/63 - 0s - loss: 505.1228 - mae: 505.1228 - val_loss: 480.3637 - val_mae: 480.3637\n",
      "Epoch 15/145\n",
      "63/63 - 0s - loss: 508.0709 - mae: 508.0709 - val_loss: 477.8696 - val_mae: 477.8696\n",
      "Epoch 16/145\n",
      "63/63 - 0s - loss: 492.6447 - mae: 492.6447 - val_loss: 562.3275 - val_mae: 562.3275\n",
      "Epoch 17/145\n",
      "63/63 - 0s - loss: 477.0684 - mae: 477.0684 - val_loss: 477.0109 - val_mae: 477.0109\n",
      "Epoch 18/145\n",
      "63/63 - 0s - loss: 488.4312 - mae: 488.4312 - val_loss: 523.1158 - val_mae: 523.1158\n",
      "Epoch 19/145\n",
      "63/63 - 1s - loss: 470.1936 - mae: 470.1936 - val_loss: 470.8538 - val_mae: 470.8538\n",
      "Epoch 20/145\n",
      "63/63 - 1s - loss: 481.6894 - mae: 481.6894 - val_loss: 519.4682 - val_mae: 519.4682\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 1s - loss: 450.7007 - mae: 450.7007 - val_loss: 459.7548 - val_mae: 459.7548\n",
      "Epoch 22/145\n",
      "63/63 - 1s - loss: 442.4665 - mae: 442.4665 - val_loss: 447.6223 - val_mae: 447.6223\n",
      "Epoch 23/145\n",
      "63/63 - 1s - loss: 442.2757 - mae: 442.2757 - val_loss: 445.7355 - val_mae: 445.7355\n",
      "Epoch 24/145\n",
      "63/63 - 1s - loss: 439.8698 - mae: 439.8698 - val_loss: 451.7101 - val_mae: 451.7101\n",
      "Epoch 25/145\n",
      "63/63 - 1s - loss: 451.1635 - mae: 451.1635 - val_loss: 462.6296 - val_mae: 462.6296\n",
      "Epoch 26/145\n",
      "63/63 - 1s - loss: 442.9669 - mae: 442.9669 - val_loss: 447.8654 - val_mae: 447.8654\n",
      "Epoch 27/145\n",
      "63/63 - 1s - loss: 437.6246 - mae: 437.6246 - val_loss: 447.1808 - val_mae: 447.1808\n",
      "Epoch 28/145\n",
      "63/63 - 0s - loss: 438.8005 - mae: 438.8005 - val_loss: 499.3692 - val_mae: 499.3692\n",
      "Epoch 29/145\n",
      "63/63 - 0s - loss: 452.4250 - mae: 452.4250 - val_loss: 447.2335 - val_mae: 447.2335\n",
      "Epoch 30/145\n",
      "63/63 - 0s - loss: 439.6749 - mae: 439.6749 - val_loss: 449.1524 - val_mae: 449.1524\n",
      "Epoch 31/145\n",
      "63/63 - 0s - loss: 434.7524 - mae: 434.7524 - val_loss: 442.8315 - val_mae: 442.8315\n",
      "Epoch 32/145\n",
      "63/63 - 0s - loss: 435.2583 - mae: 435.2583 - val_loss: 446.9875 - val_mae: 446.9875\n",
      "Epoch 33/145\n",
      "63/63 - 0s - loss: 434.1774 - mae: 434.1774 - val_loss: 446.4088 - val_mae: 446.4088\n",
      "Epoch 34/145\n",
      "63/63 - 0s - loss: 432.3881 - mae: 432.3881 - val_loss: 449.3373 - val_mae: 449.3373\n",
      "Epoch 35/145\n",
      "63/63 - 0s - loss: 427.2010 - mae: 427.2010 - val_loss: 443.5828 - val_mae: 443.5828\n",
      "Epoch 36/145\n",
      "63/63 - 0s - loss: 434.4680 - mae: 434.4680 - val_loss: 500.3514 - val_mae: 500.3514\n",
      "Epoch 37/145\n",
      "63/63 - 0s - loss: 458.7990 - mae: 458.7990 - val_loss: 470.8421 - val_mae: 470.8421\n",
      "Epoch 38/145\n",
      "63/63 - 0s - loss: 427.4734 - mae: 427.4734 - val_loss: 442.9388 - val_mae: 442.9388\n",
      "Epoch 39/145\n",
      "63/63 - 0s - loss: 452.4729 - mae: 452.4729 - val_loss: 448.8430 - val_mae: 448.8430\n",
      "Epoch 40/145\n",
      "63/63 - 0s - loss: 423.7040 - mae: 423.7040 - val_loss: 436.3889 - val_mae: 436.3889\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 0s - loss: 414.4583 - mae: 414.4583 - val_loss: 432.9520 - val_mae: 432.9520\n",
      "Epoch 42/145\n",
      "63/63 - 0s - loss: 418.8653 - mae: 418.8653 - val_loss: 449.6962 - val_mae: 449.6962\n",
      "Epoch 43/145\n",
      "63/63 - 0s - loss: 413.2044 - mae: 413.2044 - val_loss: 443.9055 - val_mae: 443.9055\n",
      "Epoch 44/145\n",
      "63/63 - 0s - loss: 413.3099 - mae: 413.3099 - val_loss: 437.9256 - val_mae: 437.9256\n",
      "Epoch 45/145\n",
      "63/63 - 0s - loss: 410.8638 - mae: 410.8638 - val_loss: 432.7704 - val_mae: 432.7704\n",
      "Epoch 46/145\n",
      "63/63 - 0s - loss: 413.5456 - mae: 413.5456 - val_loss: 429.4389 - val_mae: 429.4389\n",
      "Epoch 47/145\n",
      "63/63 - 0s - loss: 414.8387 - mae: 414.8387 - val_loss: 436.2057 - val_mae: 436.2057\n",
      "Epoch 48/145\n",
      "63/63 - 0s - loss: 423.7070 - mae: 423.7070 - val_loss: 448.4767 - val_mae: 448.4767\n",
      "Epoch 49/145\n",
      "63/63 - 0s - loss: 412.6372 - mae: 412.6372 - val_loss: 431.0326 - val_mae: 431.0326\n",
      "Epoch 50/145\n",
      "63/63 - 0s - loss: 414.7224 - mae: 414.7224 - val_loss: 427.8706 - val_mae: 427.8706\n",
      "Epoch 51/145\n",
      "63/63 - 0s - loss: 407.9401 - mae: 407.9401 - val_loss: 427.5017 - val_mae: 427.5017\n",
      "Epoch 52/145\n",
      "63/63 - 0s - loss: 407.1609 - mae: 407.1609 - val_loss: 424.2980 - val_mae: 424.2980\n",
      "Epoch 53/145\n",
      "63/63 - 0s - loss: 405.9281 - mae: 405.9281 - val_loss: 427.5742 - val_mae: 427.5742\n",
      "Epoch 54/145\n",
      "63/63 - 0s - loss: 408.0787 - mae: 408.0787 - val_loss: 425.3049 - val_mae: 425.3049\n",
      "Epoch 55/145\n",
      "63/63 - 0s - loss: 404.5591 - mae: 404.5591 - val_loss: 426.3772 - val_mae: 426.3772\n",
      "Epoch 56/145\n",
      "63/63 - 0s - loss: 405.0186 - mae: 405.0186 - val_loss: 428.4907 - val_mae: 428.4907\n",
      "Epoch 57/145\n",
      "63/63 - 0s - loss: 415.4198 - mae: 415.4198 - val_loss: 429.8085 - val_mae: 429.8085\n",
      "Epoch 58/145\n",
      "63/63 - 0s - loss: 402.7699 - mae: 402.7699 - val_loss: 430.5779 - val_mae: 430.5779\n",
      "Epoch 59/145\n",
      "63/63 - 0s - loss: 403.1684 - mae: 403.1684 - val_loss: 428.9367 - val_mae: 428.9367\n",
      "Epoch 60/145\n",
      "63/63 - 0s - loss: 405.0002 - mae: 405.0002 - val_loss: 441.0292 - val_mae: 441.0292\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 0s - loss: 397.7411 - mae: 397.7411 - val_loss: 421.3902 - val_mae: 421.3902\n",
      "Epoch 62/145\n",
      "63/63 - 0s - loss: 396.9250 - mae: 396.9250 - val_loss: 419.5137 - val_mae: 419.5137\n",
      "Epoch 63/145\n",
      "63/63 - 0s - loss: 395.3936 - mae: 395.3936 - val_loss: 422.5215 - val_mae: 422.5215\n",
      "Epoch 64/145\n",
      "63/63 - 0s - loss: 395.6968 - mae: 395.6968 - val_loss: 422.2478 - val_mae: 422.2478\n",
      "Epoch 65/145\n",
      "63/63 - 0s - loss: 396.4592 - mae: 396.4592 - val_loss: 422.5768 - val_mae: 422.5768\n",
      "Epoch 66/145\n",
      "63/63 - 0s - loss: 393.8717 - mae: 393.8717 - val_loss: 421.2298 - val_mae: 421.2298\n",
      "Epoch 67/145\n",
      "63/63 - 0s - loss: 395.1079 - mae: 395.1079 - val_loss: 422.4815 - val_mae: 422.4815\n",
      "Epoch 68/145\n",
      "63/63 - 0s - loss: 395.9864 - mae: 395.9864 - val_loss: 425.6308 - val_mae: 425.6308\n",
      "Epoch 69/145\n",
      "63/63 - 0s - loss: 394.4072 - mae: 394.4072 - val_loss: 419.5045 - val_mae: 419.5045\n",
      "Epoch 70/145\n",
      "63/63 - 0s - loss: 392.5802 - mae: 392.5802 - val_loss: 422.6713 - val_mae: 422.6713\n",
      "Epoch 71/145\n",
      "63/63 - 0s - loss: 393.2940 - mae: 393.2940 - val_loss: 418.2450 - val_mae: 418.2450\n",
      "Epoch 72/145\n",
      "63/63 - 0s - loss: 391.8665 - mae: 391.8665 - val_loss: 421.3323 - val_mae: 421.3323\n",
      "Epoch 73/145\n",
      "63/63 - 0s - loss: 391.8757 - mae: 391.8757 - val_loss: 421.3282 - val_mae: 421.3282\n",
      "Epoch 74/145\n",
      "63/63 - 0s - loss: 390.9814 - mae: 390.9814 - val_loss: 422.7769 - val_mae: 422.7769\n",
      "Epoch 75/145\n",
      "63/63 - 0s - loss: 395.0053 - mae: 395.0053 - val_loss: 425.0408 - val_mae: 425.0408\n",
      "Epoch 76/145\n",
      "63/63 - 0s - loss: 393.6651 - mae: 393.6651 - val_loss: 418.9107 - val_mae: 418.9107\n",
      "Epoch 77/145\n",
      "63/63 - 0s - loss: 392.4674 - mae: 392.4674 - val_loss: 423.5542 - val_mae: 423.5542\n",
      "Epoch 78/145\n",
      "63/63 - 0s - loss: 390.9081 - mae: 390.9081 - val_loss: 419.5114 - val_mae: 419.5114\n",
      "Epoch 79/145\n",
      "63/63 - 0s - loss: 388.3889 - mae: 388.3889 - val_loss: 418.8500 - val_mae: 418.8500\n",
      "Epoch 80/145\n",
      "63/63 - 0s - loss: 390.5780 - mae: 390.5780 - val_loss: 423.5927 - val_mae: 423.5927\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 0s - loss: 386.1925 - mae: 386.1925 - val_loss: 419.7253 - val_mae: 419.7253\n",
      "Epoch 82/145\n",
      "63/63 - 0s - loss: 385.0621 - mae: 385.0621 - val_loss: 418.7727 - val_mae: 418.7727\n",
      "Epoch 83/145\n",
      "63/63 - 0s - loss: 384.6294 - mae: 384.6294 - val_loss: 416.5440 - val_mae: 416.5440\n",
      "Epoch 84/145\n",
      "63/63 - 0s - loss: 386.0414 - mae: 386.0414 - val_loss: 420.1811 - val_mae: 420.1811\n",
      "Epoch 85/145\n",
      "63/63 - 0s - loss: 384.6437 - mae: 384.6437 - val_loss: 417.8393 - val_mae: 417.8393\n",
      "Epoch 86/145\n",
      "63/63 - 0s - loss: 385.1695 - mae: 385.1695 - val_loss: 416.5203 - val_mae: 416.5203\n",
      "Epoch 87/145\n",
      "63/63 - 0s - loss: 383.9165 - mae: 383.9165 - val_loss: 422.1677 - val_mae: 422.1677\n",
      "Epoch 88/145\n",
      "63/63 - 0s - loss: 383.9784 - mae: 383.9784 - val_loss: 417.1620 - val_mae: 417.1620\n",
      "Epoch 89/145\n",
      "63/63 - 0s - loss: 383.2749 - mae: 383.2749 - val_loss: 419.8446 - val_mae: 419.8446\n",
      "Epoch 90/145\n",
      "63/63 - 0s - loss: 384.4128 - mae: 384.4128 - val_loss: 417.2761 - val_mae: 417.2761\n",
      "Epoch 91/145\n",
      "63/63 - 0s - loss: 382.7493 - mae: 382.7493 - val_loss: 415.7420 - val_mae: 415.7420\n",
      "Epoch 92/145\n",
      "63/63 - 1s - loss: 382.9427 - mae: 382.9427 - val_loss: 415.8380 - val_mae: 415.8380\n",
      "Epoch 93/145\n",
      "63/63 - 1s - loss: 382.6299 - mae: 382.6299 - val_loss: 417.8746 - val_mae: 417.8746\n",
      "Epoch 94/145\n",
      "63/63 - 1s - loss: 381.7213 - mae: 381.7213 - val_loss: 415.4708 - val_mae: 415.4708\n",
      "Epoch 95/145\n",
      "63/63 - 1s - loss: 383.0941 - mae: 383.0941 - val_loss: 419.5840 - val_mae: 419.5840\n",
      "Epoch 96/145\n",
      "63/63 - 1s - loss: 382.6561 - mae: 382.6561 - val_loss: 416.8228 - val_mae: 416.8228\n",
      "Epoch 97/145\n",
      "63/63 - 1s - loss: 381.2798 - mae: 381.2798 - val_loss: 416.9402 - val_mae: 416.9402\n",
      "Epoch 98/145\n",
      "63/63 - 1s - loss: 381.2398 - mae: 381.2398 - val_loss: 417.5883 - val_mae: 417.5883\n",
      "Epoch 99/145\n",
      "63/63 - 1s - loss: 379.8500 - mae: 379.8500 - val_loss: 416.1734 - val_mae: 416.1734\n",
      "Epoch 100/145\n",
      "63/63 - 0s - loss: 381.6686 - mae: 381.6686 - val_loss: 416.8360 - val_mae: 416.8360\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 0s - loss: 378.6158 - mae: 378.6158 - val_loss: 415.0065 - val_mae: 415.0065\n",
      "Epoch 102/145\n",
      "63/63 - 0s - loss: 377.4849 - mae: 377.4849 - val_loss: 416.8181 - val_mae: 416.8181\n",
      "Epoch 103/145\n",
      "63/63 - 0s - loss: 377.3136 - mae: 377.3136 - val_loss: 415.3651 - val_mae: 415.3651\n",
      "Epoch 104/145\n",
      "63/63 - 0s - loss: 377.2405 - mae: 377.2405 - val_loss: 416.0004 - val_mae: 416.0004\n",
      "Epoch 105/145\n",
      "63/63 - 0s - loss: 377.5408 - mae: 377.5408 - val_loss: 417.9244 - val_mae: 417.9244\n",
      "Epoch 106/145\n",
      "63/63 - 0s - loss: 378.0370 - mae: 378.0370 - val_loss: 414.2468 - val_mae: 414.2468\n",
      "Epoch 107/145\n",
      "63/63 - 0s - loss: 377.0271 - mae: 377.0271 - val_loss: 420.9059 - val_mae: 420.9059\n",
      "Epoch 108/145\n",
      "63/63 - 0s - loss: 377.3123 - mae: 377.3123 - val_loss: 414.4298 - val_mae: 414.4298\n",
      "Epoch 109/145\n",
      "63/63 - 0s - loss: 376.9053 - mae: 376.9053 - val_loss: 413.8382 - val_mae: 413.8382\n",
      "Epoch 110/145\n",
      "63/63 - 0s - loss: 375.9867 - mae: 375.9867 - val_loss: 415.3955 - val_mae: 415.3955\n",
      "Epoch 111/145\n",
      "63/63 - 0s - loss: 376.1244 - mae: 376.1244 - val_loss: 413.8558 - val_mae: 413.8558\n",
      "Epoch 112/145\n",
      "63/63 - 0s - loss: 375.4128 - mae: 375.4128 - val_loss: 414.1510 - val_mae: 414.1510\n",
      "Epoch 113/145\n",
      "63/63 - 1s - loss: 375.5891 - mae: 375.5891 - val_loss: 415.8474 - val_mae: 415.8474\n",
      "Epoch 114/145\n",
      "63/63 - 1s - loss: 375.3170 - mae: 375.3170 - val_loss: 413.9445 - val_mae: 413.9445\n",
      "Epoch 115/145\n",
      "63/63 - 1s - loss: 375.9817 - mae: 375.9817 - val_loss: 416.2036 - val_mae: 416.2036\n",
      "Epoch 116/145\n",
      "63/63 - 1s - loss: 375.4052 - mae: 375.4052 - val_loss: 413.9957 - val_mae: 413.9957\n",
      "Epoch 117/145\n",
      "63/63 - 1s - loss: 374.7686 - mae: 374.7686 - val_loss: 415.5644 - val_mae: 415.5644\n",
      "Epoch 118/145\n",
      "63/63 - 1s - loss: 374.6636 - mae: 374.6636 - val_loss: 415.3115 - val_mae: 415.3115\n",
      "Epoch 119/145\n",
      "63/63 - 1s - loss: 374.5886 - mae: 374.5886 - val_loss: 414.4520 - val_mae: 414.4520\n",
      "Epoch 120/145\n",
      "63/63 - 1s - loss: 375.1303 - mae: 375.1303 - val_loss: 415.0549 - val_mae: 415.0549\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 1s - loss: 372.9184 - mae: 372.9184 - val_loss: 413.4830 - val_mae: 413.4830\n",
      "Epoch 122/145\n",
      "63/63 - 0s - loss: 372.2329 - mae: 372.2329 - val_loss: 413.8775 - val_mae: 413.8775\n",
      "Epoch 123/145\n",
      "63/63 - 0s - loss: 372.2556 - mae: 372.2556 - val_loss: 413.7852 - val_mae: 413.7852\n",
      "Epoch 124/145\n",
      "63/63 - 0s - loss: 372.3953 - mae: 372.3953 - val_loss: 415.7679 - val_mae: 415.7679\n",
      "Epoch 125/145\n",
      "63/63 - 0s - loss: 372.8344 - mae: 372.8344 - val_loss: 412.9843 - val_mae: 412.9843\n",
      "Epoch 126/145\n",
      "63/63 - 0s - loss: 372.4027 - mae: 372.4027 - val_loss: 414.4237 - val_mae: 414.4237\n",
      "Epoch 127/145\n",
      "63/63 - 0s - loss: 372.4532 - mae: 372.4532 - val_loss: 413.4077 - val_mae: 413.4077\n",
      "Epoch 128/145\n",
      "63/63 - 0s - loss: 371.9356 - mae: 371.9356 - val_loss: 413.7273 - val_mae: 413.7273\n",
      "Epoch 129/145\n",
      "63/63 - 0s - loss: 371.7073 - mae: 371.7073 - val_loss: 413.4678 - val_mae: 413.4678\n",
      "Epoch 130/145\n",
      "63/63 - 0s - loss: 371.7233 - mae: 371.7233 - val_loss: 413.0547 - val_mae: 413.0547\n",
      "Epoch 131/145\n",
      "63/63 - 0s - loss: 371.4438 - mae: 371.4438 - val_loss: 414.4058 - val_mae: 414.4058\n",
      "Epoch 132/145\n",
      "63/63 - 0s - loss: 371.9414 - mae: 371.9414 - val_loss: 413.9847 - val_mae: 413.9847\n",
      "Epoch 133/145\n",
      "63/63 - 0s - loss: 371.5255 - mae: 371.5255 - val_loss: 415.5900 - val_mae: 415.5900\n",
      "Epoch 134/145\n",
      "63/63 - 0s - loss: 371.1391 - mae: 371.1391 - val_loss: 414.9171 - val_mae: 414.9171\n",
      "Epoch 135/145\n",
      "63/63 - 1s - loss: 371.2485 - mae: 371.2485 - val_loss: 414.0410 - val_mae: 414.0410\n",
      "Epoch 136/145\n",
      "63/63 - 1s - loss: 371.1766 - mae: 371.1766 - val_loss: 413.6967 - val_mae: 413.6967\n",
      "Epoch 137/145\n",
      "63/63 - 1s - loss: 370.7643 - mae: 370.7643 - val_loss: 413.7127 - val_mae: 413.7127\n",
      "Epoch 138/145\n",
      "63/63 - 1s - loss: 370.7585 - mae: 370.7585 - val_loss: 413.3122 - val_mae: 413.3122\n",
      "Epoch 139/145\n",
      "63/63 - 1s - loss: 370.6930 - mae: 370.6930 - val_loss: 413.5778 - val_mae: 413.5778\n",
      "Epoch 140/145\n",
      "63/63 - 1s - loss: 370.7554 - mae: 370.7554 - val_loss: 413.9421 - val_mae: 413.9421\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 1s - loss: 369.6289 - mae: 369.6289 - val_loss: 413.0811 - val_mae: 413.0811\n",
      "Epoch 142/145\n",
      "63/63 - 1s - loss: 369.4765 - mae: 369.4765 - val_loss: 413.3946 - val_mae: 413.3946\n",
      "Epoch 143/145\n",
      "63/63 - 1s - loss: 369.2293 - mae: 369.2293 - val_loss: 413.6093 - val_mae: 413.6093\n",
      "Epoch 144/145\n",
      "63/63 - 0s - loss: 369.2851 - mae: 369.2851 - val_loss: 413.2712 - val_mae: 413.2712\n",
      "Epoch 145/145\n",
      "63/63 - 0s - loss: 369.3413 - mae: 369.3413 - val_loss: 413.0209 - val_mae: 413.0209\n",
      "\n",
      "val_mae is:413.0209427711868\n"
     ]
    },
    {
     "data": {
      "text/plain": "416.0627699047375"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_splits = 6\n",
    "kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "import keras \n",
    "\n",
    "b_size = 2000\n",
    "max_epochs = 145\n",
    "oof_pred = np.zeros((len(X_pca), ))\n",
    "\n",
    "sub = pd.read_csv('./data/used_car_testB_20200421.csv',sep = ' ')[['SaleID']].copy()\n",
    "sub['price'] = 0\n",
    "reduce_lr = LearningRateScheduler(scheduler)\n",
    "\n",
    "avg_mae = 0\n",
    "for fold, (trn_idx, val_idx) in enumerate(kf.split(X_pca, y)):\n",
    "    print('fold:', fold)\n",
    "    X_train, y_train = X_pca[trn_idx], y[trn_idx]\n",
    "    X_val, y_val = X_pca[val_idx], y[val_idx]\n",
    "    \n",
    "    model = NN_model(X_train.shape[1])\n",
    "    simple_adam = Adam(lr = 0.015)\n",
    "    model.compile(loss='mae', optimizer=simple_adam,metrics=['mae'])\n",
    "    es = EarlyStopping(monitor='val_score', patience=10, verbose=2, mode='min', restore_best_weights=True,)\n",
    "    es.set_model(model)\n",
    "    metric = Metric(model, [es], [(X_train, y_train), (X_val, y_val)])\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    print(\"X_val shape:\", X_val.shape)\n",
    "    print(\"y_val shape:\", y_val.shape)\n",
    "    model.fit(X_train, y_train, batch_size=b_size, epochs=max_epochs, \n",
    "          validation_data=(X_val, y_val),  # Use a tuple here\n",
    "          callbacks=[reduce_lr], shuffle=True, verbose=2)\n",
    "    y_pred3 = model.predict(X_val)\n",
    "    y_pred = np.zeros((len(y_pred3), ))\n",
    "    sub['price'] += model.predict(test).reshape(-1,)/n_splits\n",
    "    for i in range(len(y_pred3)):\n",
    "        y_pred[i] = y_pred3[i]\n",
    "        \n",
    "    oof_pred[val_idx] = y_pred\n",
    "    val_mae = mean_absolute_error(y[val_idx], y_pred)\n",
    "    avg_mae += val_mae/n_splits\n",
    "    print()\n",
    "    print('val_mae is:{}'.format(val_mae))\n",
    "    print()\n",
    "mean_absolute_error(y, oof_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T18:58:56.687736200Z",
     "start_time": "2024-01-14T18:51:50.479678500Z"
    }
   },
   "id": "2ff52be0a8bc6f4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d6e978e8c477656e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
