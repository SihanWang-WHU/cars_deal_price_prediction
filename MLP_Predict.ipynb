{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:43:09.950049Z",
     "start_time": "2024-01-16T23:43:09.920058100Z"
    }
   },
   "outputs": [],
   "source": [
    "# 绘图案例 an example of matplotlib\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import jn\n",
    "from IPython.display import display, clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#将kilometer当做类别变量处理试试,异常值用groupby处理,'匿名特征可以进一步处理一下'\n",
    "## 基础工具\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import jn\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "## 模型预测的\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "\n",
    "## 数据降维处理的\n",
    "from sklearn.decomposition import PCA,FastICA,FactorAnalysis,SparsePCA\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "## 参数搜索和评价的\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score,StratifiedKFold,train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import scipy.signal as signal"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:43:12.595489200Z",
     "start_time": "2024-01-16T23:43:09.959050500Z"
    }
   },
   "id": "960a4858d4e53369"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predefined Functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71d3ff038c610adf"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#处理异常值\n",
    "def smooth_cols(group,out_value,kind):\n",
    "    cols = ['power']\n",
    "    if kind == 'g':\n",
    "        for col in cols:\n",
    "            yes_no = (group[col]<out_value).astype('int')\n",
    "            new = yes_no * group[col]\n",
    "            group[col] = new.replace(0,group[col].quantile(q=0.995))\n",
    "        return group\n",
    "    if kind == 'l':\n",
    "        for col in cols:\n",
    "            yes_no = (group[col]>out_value).astype('int')\n",
    "            new = yes_no * group[col]\n",
    "            group[col] = new.replace(0,group[col].quantile(q=0.07))\n",
    "        return group   \n",
    "    \n",
    "def date_proc(x):\n",
    "    m = int(x[4:6])\n",
    "    if m == 0:\n",
    "        m = 1\n",
    "    return x[:4] + '-' + str(m) + '-' + x[6:]\n",
    "\n",
    "#定义日期提取函数\n",
    "def date_tran(df,fea_col):\n",
    "    for f in tqdm(fea_col):\n",
    "        df[f] = pd.to_datetime(df[f].astype('str').apply(date_proc))\n",
    "        df[f + '_year'] = df[f].dt.year\n",
    "        df[f + '_month'] = df[f].dt.month\n",
    "        df[f + '_day'] = df[f].dt.day\n",
    "        df[f + '_dayofweek'] = df[f].dt.dayofweek\n",
    "    return (df)\n",
    "\n",
    "#分桶操作\n",
    "def cut_group(df,cols,num_bins=50):\n",
    "    for col in cols:\n",
    "        all_range = int(df[col].max()-df[col].min())\n",
    "        bin = [i*all_range/num_bins for i in range(all_range)]\n",
    "        df[col+'_bin'] = pd.cut(df[col], bin, labels=False)\n",
    "    return df\n",
    "\n",
    "### count编码\n",
    "def count_coding(df,fea_col):\n",
    "    for f in fea_col:\n",
    "        df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "    return(df)\n",
    "\n",
    "#定义交叉特征统计\n",
    "def cross_cat_num(df,num_col,cat_col):\n",
    "    for f1 in tqdm(cat_col):\n",
    "        g = df.groupby(f1, as_index=False)\n",
    "        for f2 in tqdm(num_col):\n",
    "            feat = g[f2].agg({\n",
    "                '{}_{}_max'.format(f1, f2): 'max', '{}_{}_min'.format(f1, f2): 'min',\n",
    "                '{}_{}_median'.format(f1, f2): 'median',\n",
    "            })\n",
    "            df = df.merge(feat, on=f1, how='left')\n",
    "    return(df)\n",
    "\n",
    "### 类别特征的二阶交叉\n",
    "from scipy.stats import entropy\n",
    "def cross_qua_cat_num(df):\n",
    "    for f_pair in tqdm([\n",
    "        ['model', 'brand'], ['model', 'regionCode'], ['brand', 'regionCode']\n",
    "    ]):\n",
    "        ### 共现次数\n",
    "        df['_'.join(f_pair) + '_count'] = df.groupby(f_pair)['SaleID'].transform('count')\n",
    "        ### n unique、熵\n",
    "        df = df.merge(df.groupby(f_pair[0], as_index=False)[f_pair[1]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[0], f_pair[1]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[0], f_pair[1]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[0], how='left')\n",
    "        df = df.merge(df.groupby(f_pair[1], as_index=False)[f_pair[0]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[1], f_pair[0]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[1], f_pair[0]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[1], how='left')\n",
    "        ### 比例偏好\n",
    "        df['{}_in_{}_prop'.format(f_pair[0], f_pair[1])] = df['_'.join(f_pair) + '_count'] / df[f_pair[1] + '_count']\n",
    "        df['{}_in_{}_prop'.format(f_pair[1], f_pair[0])] = df['_'.join(f_pair) + '_count'] / df[f_pair[0] + '_count']\n",
    "    return (df)\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:43:12.663493100Z",
     "start_time": "2024-01-16T23:43:12.596489300Z"
    }
   },
   "id": "476f1db95fded050"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b6888b17d8d5c42"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 37200128.00 MB\n",
      "Memory usage after optimization is: 10200260.00 MB\n",
      "Decreased by 72.6%\n",
      "Memory usage of dataframe is 12000128.00 MB\n",
      "Memory usage after optimization is: 3200260.00 MB\n",
      "Decreased by 73.3%\n",
      "Train data shape: (150000, 31)\n",
      "TestA data shape: (50000, 30)\n",
      "concat_data shape: (200000, 31)\n"
     ]
    }
   ],
   "source": [
    "## 通过Pandas对于数据进行读取 (pandas是一个很友好的数据读取函数库)\n",
    "Train_data = reduce_mem_usage(pd.read_csv('./data/used_car_train_20200313.csv', sep=' '))\n",
    "TestA_data = reduce_mem_usage(pd.read_csv('./data/used_car_testB_20200421.csv', sep=' '))\n",
    "\n",
    "#Train_data = Train_data[Train_data['price']>100]\n",
    "#Train_data['price'] = np.log1p(Train_data['price'])\n",
    "## 输出数据的大小信息\n",
    "print('Train data shape:',Train_data.shape)\n",
    "print('TestA data shape:',TestA_data.shape)\n",
    "\n",
    "\n",
    "#合并数据集\n",
    "concat_data = pd.concat([Train_data,TestA_data])\n",
    "concat_data['notRepairedDamage'] = concat_data['notRepairedDamage'].replace('-',0).astype('float16')\n",
    "concat_data = concat_data.fillna(concat_data.mode().iloc[0,:])\n",
    "print('concat_data shape:',concat_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:43:15.551100500Z",
     "start_time": "2024-01-16T23:43:12.671489200Z"
    }
   },
   "id": "4e26bd2a52d5f947"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#截断异常值\n",
    "concat_data['power'][concat_data['power']>600] = 600\n",
    "concat_data['power'][concat_data['power']<1] = 1\n",
    "\n",
    "concat_data['v_13'][concat_data['v_13']>6] = 6\n",
    "concat_data['v_14'][concat_data['v_14']>4] = 4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:43:15.638103300Z",
     "start_time": "2024-01-16T23:43:15.547101600Z"
    }
   },
   "id": "60d13082b4b937f6"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "       SaleID    name   regDate  model  brand  bodyType  fuelType  gearbox  \\\n0           0     736  20040402   30.0      6       1.0       0.0      0.0   \n1           1    2262  20030301   40.0      1       2.0       0.0      0.0   \n2           2   14874  20040403  115.0     15       1.0       0.0      0.0   \n3           3   71865  19960908  109.0     10       0.0       0.0      1.0   \n4           4  111080  20120103  110.0      5       1.0       0.0      0.0   \n...       ...     ...       ...    ...    ...       ...       ...      ...   \n49995  249995  111443  20041005    4.0      4       0.0       0.0      1.0   \n49996  249996  152834  20130409   65.0      1       0.0       0.0      0.0   \n49997  249997  132531  20041211    4.0      4       0.0       0.0      1.0   \n49998  249998  143405  20020702   40.0      1       4.0       0.0      1.0   \n49999  249999   78202  20090708   32.0      8       1.0       0.0      0.0   \n\n       power  kilometer  ...  regionCode*v_4  regionCode*v_5  regionCode*v_6  \\\n0         60       12.5  ...     1196.158203      246.560791      106.681274   \n1          1       15.0  ...    -6207.906250     1156.520996      528.428833   \n2        163       12.5  ...    -2797.779297      705.610352      322.491333   \n3        193       15.0  ...     -967.175781      119.095703       47.866089   \n4         68        5.0  ...    15670.996094     1590.946777      510.584900   \n...      ...        ...  ...             ...             ...             ...   \n49995    150       15.0  ...    -4330.574219     1467.070312        1.625037   \n49996    179        4.0  ...     1865.742188     1333.037109        5.172329   \n49997    147       12.5  ...    -2607.209473      997.855225        1.206096   \n49998    176       15.0  ...     -112.528320       17.215820        0.001407   \n49999      1        3.0  ...     7442.982422      962.349609      432.194458   \n\n       regionCode*v_7  regionCode*v_8  regionCode*v_9  regionCode*v_10  \\\n0          135.474365       23.861237      101.956909     -3013.378906   \n1          592.650391      116.118439       89.870209    -21403.632812   \n2          463.442139      174.433044       75.955872    -13602.523438   \n3           52.925537       14.489624        0.000000     -1956.390625   \n4          640.892639      549.762390      847.851990    -13231.771484   \n...               ...             ...             ...              ...   \n49995      789.229492      425.178711      218.532349     11530.085938   \n49996      813.713379      565.839844      354.287109      7090.839844   \n49997      538.304443      273.089905      163.066406      8220.029297   \n49998        4.117798        4.117798        0.549164       123.787109   \n49999      399.202515      259.113647      458.080444    -15340.746094   \n\n       regionCode*v_11  regionCode*v_12  regionCode*v_13  \n0          2933.703125     -2531.238281       831.999023  \n1          9149.839844     -4498.173828     -7521.117188  \n2          5061.212891      4392.595703     -2336.049805  \n3           558.181641      -217.847656     -1057.875000  \n4          6353.566895      6496.649902     19772.708984  \n...                ...              ...              ...  \n49995    -14083.875000      9552.257812     -5917.183594  \n49996    -17179.101562     22286.953125       733.425293  \n49997     -9176.191406      5203.300781     -4072.954102  \n49998      -179.306641        34.699707      -104.784180  \n49999      8454.058594       453.766113      9160.593750  \n\n[200000 rows x 353 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SaleID</th>\n      <th>name</th>\n      <th>regDate</th>\n      <th>model</th>\n      <th>brand</th>\n      <th>bodyType</th>\n      <th>fuelType</th>\n      <th>gearbox</th>\n      <th>power</th>\n      <th>kilometer</th>\n      <th>...</th>\n      <th>regionCode*v_4</th>\n      <th>regionCode*v_5</th>\n      <th>regionCode*v_6</th>\n      <th>regionCode*v_7</th>\n      <th>regionCode*v_8</th>\n      <th>regionCode*v_9</th>\n      <th>regionCode*v_10</th>\n      <th>regionCode*v_11</th>\n      <th>regionCode*v_12</th>\n      <th>regionCode*v_13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>736</td>\n      <td>20040402</td>\n      <td>30.0</td>\n      <td>6</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>60</td>\n      <td>12.5</td>\n      <td>...</td>\n      <td>1196.158203</td>\n      <td>246.560791</td>\n      <td>106.681274</td>\n      <td>135.474365</td>\n      <td>23.861237</td>\n      <td>101.956909</td>\n      <td>-3013.378906</td>\n      <td>2933.703125</td>\n      <td>-2531.238281</td>\n      <td>831.999023</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2262</td>\n      <td>20030301</td>\n      <td>40.0</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>15.0</td>\n      <td>...</td>\n      <td>-6207.906250</td>\n      <td>1156.520996</td>\n      <td>528.428833</td>\n      <td>592.650391</td>\n      <td>116.118439</td>\n      <td>89.870209</td>\n      <td>-21403.632812</td>\n      <td>9149.839844</td>\n      <td>-4498.173828</td>\n      <td>-7521.117188</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>14874</td>\n      <td>20040403</td>\n      <td>115.0</td>\n      <td>15</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>163</td>\n      <td>12.5</td>\n      <td>...</td>\n      <td>-2797.779297</td>\n      <td>705.610352</td>\n      <td>322.491333</td>\n      <td>463.442139</td>\n      <td>174.433044</td>\n      <td>75.955872</td>\n      <td>-13602.523438</td>\n      <td>5061.212891</td>\n      <td>4392.595703</td>\n      <td>-2336.049805</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>71865</td>\n      <td>19960908</td>\n      <td>109.0</td>\n      <td>10</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>193</td>\n      <td>15.0</td>\n      <td>...</td>\n      <td>-967.175781</td>\n      <td>119.095703</td>\n      <td>47.866089</td>\n      <td>52.925537</td>\n      <td>14.489624</td>\n      <td>0.000000</td>\n      <td>-1956.390625</td>\n      <td>558.181641</td>\n      <td>-217.847656</td>\n      <td>-1057.875000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>111080</td>\n      <td>20120103</td>\n      <td>110.0</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>68</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>15670.996094</td>\n      <td>1590.946777</td>\n      <td>510.584900</td>\n      <td>640.892639</td>\n      <td>549.762390</td>\n      <td>847.851990</td>\n      <td>-13231.771484</td>\n      <td>6353.566895</td>\n      <td>6496.649902</td>\n      <td>19772.708984</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>249995</td>\n      <td>111443</td>\n      <td>20041005</td>\n      <td>4.0</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>150</td>\n      <td>15.0</td>\n      <td>...</td>\n      <td>-4330.574219</td>\n      <td>1467.070312</td>\n      <td>1.625037</td>\n      <td>789.229492</td>\n      <td>425.178711</td>\n      <td>218.532349</td>\n      <td>11530.085938</td>\n      <td>-14083.875000</td>\n      <td>9552.257812</td>\n      <td>-5917.183594</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>249996</td>\n      <td>152834</td>\n      <td>20130409</td>\n      <td>65.0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>179</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>1865.742188</td>\n      <td>1333.037109</td>\n      <td>5.172329</td>\n      <td>813.713379</td>\n      <td>565.839844</td>\n      <td>354.287109</td>\n      <td>7090.839844</td>\n      <td>-17179.101562</td>\n      <td>22286.953125</td>\n      <td>733.425293</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>249997</td>\n      <td>132531</td>\n      <td>20041211</td>\n      <td>4.0</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>147</td>\n      <td>12.5</td>\n      <td>...</td>\n      <td>-2607.209473</td>\n      <td>997.855225</td>\n      <td>1.206096</td>\n      <td>538.304443</td>\n      <td>273.089905</td>\n      <td>163.066406</td>\n      <td>8220.029297</td>\n      <td>-9176.191406</td>\n      <td>5203.300781</td>\n      <td>-4072.954102</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>249998</td>\n      <td>143405</td>\n      <td>20020702</td>\n      <td>40.0</td>\n      <td>1</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>176</td>\n      <td>15.0</td>\n      <td>...</td>\n      <td>-112.528320</td>\n      <td>17.215820</td>\n      <td>0.001407</td>\n      <td>4.117798</td>\n      <td>4.117798</td>\n      <td>0.549164</td>\n      <td>123.787109</td>\n      <td>-179.306641</td>\n      <td>34.699707</td>\n      <td>-104.784180</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>249999</td>\n      <td>78202</td>\n      <td>20090708</td>\n      <td>32.0</td>\n      <td>8</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>...</td>\n      <td>7442.982422</td>\n      <td>962.349609</td>\n      <td>432.194458</td>\n      <td>399.202515</td>\n      <td>259.113647</td>\n      <td>458.080444</td>\n      <td>-15340.746094</td>\n      <td>8454.058594</td>\n      <td>453.766113</td>\n      <td>9160.593750</td>\n    </tr>\n  </tbody>\n</table>\n<p>200000 rows × 353 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in ['v_' +str(i) for i in range(14)]:\n",
    "    for j in ['v_' +str(i) for i in range(14)]:\n",
    "        concat_data[str(i)+'+'+str(j)] = concat_data[str(i)]+concat_data[str(j)]\n",
    "for i in ['model','brand', 'bodyType', 'fuelType','gearbox', 'power', 'kilometer', 'notRepairedDamage', 'regionCode']:\n",
    "    for j in ['v_' +str(i) for i in range(14)]:\n",
    "        concat_data[str(i)+'*'+str(j)] = concat_data[i]*concat_data[j]    \n",
    "concat_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:43:20.196576400Z",
     "start_time": "2024-01-16T23:43:15.644101800Z"
    }
   },
   "id": "b538a870da7ff463"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "#提取日期信息\n",
    "date_cols = ['regDate', 'creatDate']\n",
    "concat_data = date_tran(concat_data,date_cols)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:43:21.796327900Z",
     "start_time": "2024-01-16T23:43:20.183609200Z"
    }
   },
   "id": "716ee76142c4f824"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "data = concat_data.copy()\n",
    "\n",
    "#count编码\n",
    "count_list = ['regDate', 'creatDate', 'model', 'brand', 'regionCode','bodyType','fuelType','name','regDate_year', 'regDate_month', 'regDate_day', 'regDate_dayofweek' , 'creatDate_month','creatDate_day', 'creatDate_dayofweek','kilometer']\n",
    "       \n",
    "data = count_coding(data,count_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:43:23.026330100Z",
     "start_time": "2024-01-16T23:43:21.780331300Z"
    }
   },
   "id": "bbc86cabfa5d939d"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 特征构造\n",
    "# 使用时间：data['creatDate'] - data['regDate']，反应汽车使用时间，一般来说价格与使用时间成反比\n",
    "# 不过要注意，数据里有时间出错的格式，所以我们需要 errors='coerce'\n",
    "data['used_time1'] = (pd.to_datetime(data['creatDate'], format='%Y%m%d', errors='coerce') - \n",
    "                            pd.to_datetime(data['regDate'], format='%Y%m%d', errors='coerce')).dt.days\n",
    "data['used_time2'] = (pd.datetime.now() - pd.to_datetime(data['regDate'], format='%Y%m%d', errors='coerce')).dt.days                        \n",
    "data['used_time3'] = (pd.datetime.now() - pd.to_datetime(data['creatDate'], format='%Y%m%d', errors='coerce') ).dt.days\n",
    "\n",
    "#分桶操作\n",
    "cut_cols = ['power']+['used_time1','used_time2','used_time3']\n",
    "data = cut_group(data,cut_cols,50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:43:23.260030600Z",
     "start_time": "2024-01-16T23:43:22.968329900Z"
    }
   },
   "id": "1a5b315466fe5137"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      " 17%|█▋        | 1/6 [00:00<00:04,  1.08it/s]\u001B[A\n",
      " 33%|███▎      | 2/6 [00:01<00:03,  1.02it/s]\u001B[A\n",
      " 50%|█████     | 3/6 [00:02<00:02,  1.19it/s]\u001B[A\n",
      " 67%|██████▋   | 4/6 [00:03<00:01,  1.28it/s]\u001B[A\n",
      " 83%|████████▎ | 5/6 [00:04<00:00,  1.30it/s]\u001B[A\n",
      "100%|██████████| 6/6 [00:04<00:00,  1.22it/s]\u001B[A\n",
      " 33%|███▎      | 1/3 [00:04<00:09,  4.92s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  1.94it/s]\u001B[A\n",
      " 33%|███▎      | 2/6 [00:01<00:02,  1.61it/s]\u001B[A\n",
      " 50%|█████     | 3/6 [00:01<00:01,  1.55it/s]\u001B[A\n",
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.56it/s]\u001B[A\n",
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.55it/s]\u001B[A\n",
      "100%|██████████| 6/6 [00:03<00:00,  1.58it/s]\u001B[A\n",
      " 67%|██████▋   | 2/3 [00:08<00:04,  4.27s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001B[A\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  1.79it/s]\u001B[A\n",
      " 33%|███▎      | 2/6 [00:01<00:02,  1.63it/s]\u001B[A\n",
      " 50%|█████     | 3/6 [00:01<00:01,  1.56it/s]\u001B[A\n",
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.53it/s]\u001B[A\n",
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.50it/s]\u001B[A\n",
      "100%|██████████| 6/6 [00:03<00:00,  1.51it/s]\u001B[A\n",
      "100%|██████████| 3/3 [00:12<00:00,  4.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "   SaleID    name    regDate  model  brand  bodyType  fuelType  gearbox  \\\n0       0     736 2004-04-02   30.0      6       1.0       0.0      0.0   \n1       1    2262 2003-03-01   40.0      1       2.0       0.0      0.0   \n2       2   14874 2004-04-03  115.0     15       1.0       0.0      0.0   \n3       3   71865 1996-09-08  109.0     10       0.0       0.0      1.0   \n4       4  111080 2012-01-03  110.0      5       1.0       0.0      0.0   \n\n   power  kilometer  ...  regDate_year_v_4_median  regDate_year_v_8_max  \\\n0     60       12.5  ...                -0.005676              0.128052   \n1      1       15.0  ...                 0.046509              0.117371   \n2    163       12.5  ...                -0.005676              0.128052   \n3    193       15.0  ...                -0.537598              0.114380   \n4     68        5.0  ...                 0.357178              0.157471   \n\n   regDate_year_v_8_min  regDate_year_v_8_median regDate_year_v_12_max  \\\n0                   0.0                 0.060364              9.820312   \n1                   0.0                 0.054596              8.468750   \n2                   0.0                 0.060364              9.820312   \n3                   0.0                 0.025009              6.750000   \n4                   0.0                 0.097290             13.382812   \n\n   regDate_year_v_12_min  regDate_year_v_12_median  regDate_year_power_max  \\\n0              -5.843750                 -0.032684                     600   \n1              -7.988281                 -0.627930                     600   \n2              -5.843750                 -0.032684                     600   \n3              -9.640625                 -2.695312                     600   \n4              -6.367188                  2.882812                     600   \n\n   regDate_year_power_min  regDate_year_power_median  \n0                       1                      116.0  \n1                       1                      110.0  \n2                       1                      116.0  \n3                       1                       90.0  \n4                       1                      136.0  \n\n[5 rows x 438 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SaleID</th>\n      <th>name</th>\n      <th>regDate</th>\n      <th>model</th>\n      <th>brand</th>\n      <th>bodyType</th>\n      <th>fuelType</th>\n      <th>gearbox</th>\n      <th>power</th>\n      <th>kilometer</th>\n      <th>...</th>\n      <th>regDate_year_v_4_median</th>\n      <th>regDate_year_v_8_max</th>\n      <th>regDate_year_v_8_min</th>\n      <th>regDate_year_v_8_median</th>\n      <th>regDate_year_v_12_max</th>\n      <th>regDate_year_v_12_min</th>\n      <th>regDate_year_v_12_median</th>\n      <th>regDate_year_power_max</th>\n      <th>regDate_year_power_min</th>\n      <th>regDate_year_power_median</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>736</td>\n      <td>2004-04-02</td>\n      <td>30.0</td>\n      <td>6</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>60</td>\n      <td>12.5</td>\n      <td>...</td>\n      <td>-0.005676</td>\n      <td>0.128052</td>\n      <td>0.0</td>\n      <td>0.060364</td>\n      <td>9.820312</td>\n      <td>-5.843750</td>\n      <td>-0.032684</td>\n      <td>600</td>\n      <td>1</td>\n      <td>116.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2262</td>\n      <td>2003-03-01</td>\n      <td>40.0</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>15.0</td>\n      <td>...</td>\n      <td>0.046509</td>\n      <td>0.117371</td>\n      <td>0.0</td>\n      <td>0.054596</td>\n      <td>8.468750</td>\n      <td>-7.988281</td>\n      <td>-0.627930</td>\n      <td>600</td>\n      <td>1</td>\n      <td>110.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>14874</td>\n      <td>2004-04-03</td>\n      <td>115.0</td>\n      <td>15</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>163</td>\n      <td>12.5</td>\n      <td>...</td>\n      <td>-0.005676</td>\n      <td>0.128052</td>\n      <td>0.0</td>\n      <td>0.060364</td>\n      <td>9.820312</td>\n      <td>-5.843750</td>\n      <td>-0.032684</td>\n      <td>600</td>\n      <td>1</td>\n      <td>116.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>71865</td>\n      <td>1996-09-08</td>\n      <td>109.0</td>\n      <td>10</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>193</td>\n      <td>15.0</td>\n      <td>...</td>\n      <td>-0.537598</td>\n      <td>0.114380</td>\n      <td>0.0</td>\n      <td>0.025009</td>\n      <td>6.750000</td>\n      <td>-9.640625</td>\n      <td>-2.695312</td>\n      <td>600</td>\n      <td>1</td>\n      <td>90.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>111080</td>\n      <td>2012-01-03</td>\n      <td>110.0</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>68</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>0.357178</td>\n      <td>0.157471</td>\n      <td>0.0</td>\n      <td>0.097290</td>\n      <td>13.382812</td>\n      <td>-6.367188</td>\n      <td>2.882812</td>\n      <td>600</td>\n      <td>1</td>\n      <td>136.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 438 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 用数值特征对类别特征做统计刻画，随便挑了几个跟price相关性最高的匿名特征\n",
    "cross_cat = ['model', 'brand','regDate_year']\n",
    "cross_num = ['v_0','v_3', 'v_4', 'v_8', 'v_12','power']\n",
    "data = cross_cat_num(data,cross_num,cross_cat)#一阶交叉\n",
    "data.head()\n",
    "#data = cross_qua_cat_num(data)#二阶交叉"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:43:36.100023400Z",
     "start_time": "2024-01-16T23:43:23.266029500Z"
    }
   },
   "id": "1e7018f8e7b1c1c6"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['name',\n 'regDate',\n 'model',\n 'brand',\n 'bodyType',\n 'fuelType',\n 'gearbox',\n 'power',\n 'kilometer',\n 'notRepairedDamage',\n 'regionCode',\n 'creatDate',\n 'v_0',\n 'v_1',\n 'v_2',\n 'v_3',\n 'v_4',\n 'v_5',\n 'v_6',\n 'v_7',\n 'v_8',\n 'v_9',\n 'v_10',\n 'v_11',\n 'v_12',\n 'v_13',\n 'v_14',\n 'v_0+v_0',\n 'v_0+v_1',\n 'v_0+v_2',\n 'v_0+v_3',\n 'v_0+v_4',\n 'v_0+v_5',\n 'v_0+v_6',\n 'v_0+v_7',\n 'v_0+v_8',\n 'v_0+v_9',\n 'v_0+v_10',\n 'v_0+v_11',\n 'v_0+v_12',\n 'v_0+v_13',\n 'v_1+v_0',\n 'v_1+v_1',\n 'v_1+v_2',\n 'v_1+v_3',\n 'v_1+v_4',\n 'v_1+v_5',\n 'v_1+v_6',\n 'v_1+v_7',\n 'v_1+v_8',\n 'v_1+v_9',\n 'v_1+v_10',\n 'v_1+v_11',\n 'v_1+v_12',\n 'v_1+v_13',\n 'v_2+v_0',\n 'v_2+v_1',\n 'v_2+v_2',\n 'v_2+v_3',\n 'v_2+v_4',\n 'v_2+v_5',\n 'v_2+v_6',\n 'v_2+v_7',\n 'v_2+v_8',\n 'v_2+v_9',\n 'v_2+v_10',\n 'v_2+v_11',\n 'v_2+v_12',\n 'v_2+v_13',\n 'v_3+v_0',\n 'v_3+v_1',\n 'v_3+v_2',\n 'v_3+v_3',\n 'v_3+v_4',\n 'v_3+v_5',\n 'v_3+v_6',\n 'v_3+v_7',\n 'v_3+v_8',\n 'v_3+v_9',\n 'v_3+v_10',\n 'v_3+v_11',\n 'v_3+v_12',\n 'v_3+v_13',\n 'v_4+v_0',\n 'v_4+v_1',\n 'v_4+v_2',\n 'v_4+v_3',\n 'v_4+v_4',\n 'v_4+v_5',\n 'v_4+v_6',\n 'v_4+v_7',\n 'v_4+v_8',\n 'v_4+v_9',\n 'v_4+v_10',\n 'v_4+v_11',\n 'v_4+v_12',\n 'v_4+v_13',\n 'v_5+v_0',\n 'v_5+v_1',\n 'v_5+v_2',\n 'v_5+v_3',\n 'v_5+v_4',\n 'v_5+v_5',\n 'v_5+v_6',\n 'v_5+v_7',\n 'v_5+v_8',\n 'v_5+v_9',\n 'v_5+v_10',\n 'v_5+v_11',\n 'v_5+v_12',\n 'v_5+v_13',\n 'v_6+v_0',\n 'v_6+v_1',\n 'v_6+v_2',\n 'v_6+v_3',\n 'v_6+v_4',\n 'v_6+v_5',\n 'v_6+v_6',\n 'v_6+v_7',\n 'v_6+v_8',\n 'v_6+v_9',\n 'v_6+v_10',\n 'v_6+v_11',\n 'v_6+v_12',\n 'v_6+v_13',\n 'v_7+v_0',\n 'v_7+v_1',\n 'v_7+v_2',\n 'v_7+v_3',\n 'v_7+v_4',\n 'v_7+v_5',\n 'v_7+v_6',\n 'v_7+v_7',\n 'v_7+v_8',\n 'v_7+v_9',\n 'v_7+v_10',\n 'v_7+v_11',\n 'v_7+v_12',\n 'v_7+v_13',\n 'v_8+v_0',\n 'v_8+v_1',\n 'v_8+v_2',\n 'v_8+v_3',\n 'v_8+v_4',\n 'v_8+v_5',\n 'v_8+v_6',\n 'v_8+v_7',\n 'v_8+v_8',\n 'v_8+v_9',\n 'v_8+v_10',\n 'v_8+v_11',\n 'v_8+v_12',\n 'v_8+v_13',\n 'v_9+v_0',\n 'v_9+v_1',\n 'v_9+v_2',\n 'v_9+v_3',\n 'v_9+v_4',\n 'v_9+v_5',\n 'v_9+v_6',\n 'v_9+v_7',\n 'v_9+v_8',\n 'v_9+v_9',\n 'v_9+v_10',\n 'v_9+v_11',\n 'v_9+v_12',\n 'v_9+v_13',\n 'v_10+v_0',\n 'v_10+v_1',\n 'v_10+v_2',\n 'v_10+v_3',\n 'v_10+v_4',\n 'v_10+v_5',\n 'v_10+v_6',\n 'v_10+v_7',\n 'v_10+v_8',\n 'v_10+v_9',\n 'v_10+v_10',\n 'v_10+v_11',\n 'v_10+v_12',\n 'v_10+v_13',\n 'v_11+v_0',\n 'v_11+v_1',\n 'v_11+v_2',\n 'v_11+v_3',\n 'v_11+v_4',\n 'v_11+v_5',\n 'v_11+v_6',\n 'v_11+v_7',\n 'v_11+v_8',\n 'v_11+v_9',\n 'v_11+v_10',\n 'v_11+v_11',\n 'v_11+v_12',\n 'v_11+v_13',\n 'v_12+v_0',\n 'v_12+v_1',\n 'v_12+v_2',\n 'v_12+v_3',\n 'v_12+v_4',\n 'v_12+v_5',\n 'v_12+v_6',\n 'v_12+v_7',\n 'v_12+v_8',\n 'v_12+v_9',\n 'v_12+v_10',\n 'v_12+v_11',\n 'v_12+v_12',\n 'v_12+v_13',\n 'v_13+v_0',\n 'v_13+v_1',\n 'v_13+v_2',\n 'v_13+v_3',\n 'v_13+v_4',\n 'v_13+v_5',\n 'v_13+v_6',\n 'v_13+v_7',\n 'v_13+v_8',\n 'v_13+v_9',\n 'v_13+v_10',\n 'v_13+v_11',\n 'v_13+v_12',\n 'v_13+v_13',\n 'model*v_0',\n 'model*v_1',\n 'model*v_2',\n 'model*v_3',\n 'model*v_4',\n 'model*v_5',\n 'model*v_6',\n 'model*v_7',\n 'model*v_8',\n 'model*v_9',\n 'model*v_10',\n 'model*v_11',\n 'model*v_12',\n 'model*v_13',\n 'brand*v_0',\n 'brand*v_1',\n 'brand*v_2',\n 'brand*v_3',\n 'brand*v_4',\n 'brand*v_5',\n 'brand*v_6',\n 'brand*v_7',\n 'brand*v_8',\n 'brand*v_9',\n 'brand*v_10',\n 'brand*v_11',\n 'brand*v_12',\n 'brand*v_13',\n 'bodyType*v_0',\n 'bodyType*v_1',\n 'bodyType*v_2',\n 'bodyType*v_3',\n 'bodyType*v_4',\n 'bodyType*v_5',\n 'bodyType*v_6',\n 'bodyType*v_7',\n 'bodyType*v_8',\n 'bodyType*v_9',\n 'bodyType*v_10',\n 'bodyType*v_11',\n 'bodyType*v_12',\n 'bodyType*v_13',\n 'fuelType*v_0',\n 'fuelType*v_1',\n 'fuelType*v_2',\n 'fuelType*v_3',\n 'fuelType*v_4',\n 'fuelType*v_5',\n 'fuelType*v_6',\n 'fuelType*v_7',\n 'fuelType*v_8',\n 'fuelType*v_9',\n 'fuelType*v_10',\n 'fuelType*v_11',\n 'fuelType*v_12',\n 'fuelType*v_13',\n 'gearbox*v_0',\n 'gearbox*v_1',\n 'gearbox*v_2',\n 'gearbox*v_3',\n 'gearbox*v_4',\n 'gearbox*v_5',\n 'gearbox*v_6',\n 'gearbox*v_7',\n 'gearbox*v_8',\n 'gearbox*v_9',\n 'gearbox*v_10',\n 'gearbox*v_11',\n 'gearbox*v_12',\n 'gearbox*v_13',\n 'power*v_0',\n 'power*v_1',\n 'power*v_2',\n 'power*v_3',\n 'power*v_4',\n 'power*v_5',\n 'power*v_6',\n 'power*v_7',\n 'power*v_8',\n 'power*v_9',\n 'power*v_10',\n 'power*v_11',\n 'power*v_12',\n 'power*v_13',\n 'kilometer*v_0',\n 'kilometer*v_1',\n 'kilometer*v_2',\n 'kilometer*v_3',\n 'kilometer*v_4',\n 'kilometer*v_5',\n 'kilometer*v_6',\n 'kilometer*v_7',\n 'kilometer*v_8',\n 'kilometer*v_9',\n 'kilometer*v_10',\n 'kilometer*v_11',\n 'kilometer*v_12',\n 'kilometer*v_13',\n 'notRepairedDamage*v_0',\n 'notRepairedDamage*v_1',\n 'notRepairedDamage*v_2',\n 'notRepairedDamage*v_3',\n 'notRepairedDamage*v_4',\n 'notRepairedDamage*v_5',\n 'notRepairedDamage*v_6',\n 'notRepairedDamage*v_7',\n 'notRepairedDamage*v_8',\n 'notRepairedDamage*v_9',\n 'notRepairedDamage*v_10',\n 'notRepairedDamage*v_11',\n 'notRepairedDamage*v_12',\n 'notRepairedDamage*v_13',\n 'regionCode*v_0',\n 'regionCode*v_1',\n 'regionCode*v_2',\n 'regionCode*v_3',\n 'regionCode*v_4',\n 'regionCode*v_5',\n 'regionCode*v_6',\n 'regionCode*v_7',\n 'regionCode*v_8',\n 'regionCode*v_9',\n 'regionCode*v_10',\n 'regionCode*v_11',\n 'regionCode*v_12',\n 'regionCode*v_13',\n 'regDate_year',\n 'regDate_month',\n 'regDate_day',\n 'regDate_dayofweek',\n 'creatDate_year',\n 'creatDate_month',\n 'creatDate_day',\n 'creatDate_dayofweek',\n 'regDate_count',\n 'creatDate_count',\n 'model_count',\n 'brand_count',\n 'regionCode_count',\n 'bodyType_count',\n 'fuelType_count',\n 'name_count',\n 'regDate_year_count',\n 'regDate_month_count',\n 'regDate_day_count',\n 'regDate_dayofweek_count',\n 'creatDate_month_count',\n 'creatDate_day_count',\n 'creatDate_dayofweek_count',\n 'kilometer_count',\n 'used_time1',\n 'used_time2',\n 'used_time3',\n 'power_bin',\n 'used_time1_bin',\n 'used_time2_bin',\n 'used_time3_bin',\n 'model_v_0_max',\n 'model_v_0_min',\n 'model_v_0_median',\n 'model_v_3_max',\n 'model_v_3_min',\n 'model_v_3_median',\n 'model_v_4_max',\n 'model_v_4_min',\n 'model_v_4_median',\n 'model_v_8_max',\n 'model_v_8_min',\n 'model_v_8_median',\n 'model_v_12_max',\n 'model_v_12_min',\n 'model_v_12_median',\n 'model_power_max',\n 'model_power_min',\n 'model_power_median',\n 'brand_v_0_max',\n 'brand_v_0_min',\n 'brand_v_0_median',\n 'brand_v_3_max',\n 'brand_v_3_min',\n 'brand_v_3_median',\n 'brand_v_4_max',\n 'brand_v_4_min',\n 'brand_v_4_median',\n 'brand_v_8_max',\n 'brand_v_8_min',\n 'brand_v_8_median',\n 'brand_v_12_max',\n 'brand_v_12_min',\n 'brand_v_12_median',\n 'brand_power_max',\n 'brand_power_min',\n 'brand_power_median',\n 'regDate_year_v_0_max',\n 'regDate_year_v_0_min',\n 'regDate_year_v_0_median',\n 'regDate_year_v_3_max',\n 'regDate_year_v_3_min',\n 'regDate_year_v_3_median',\n 'regDate_year_v_4_max',\n 'regDate_year_v_4_min',\n 'regDate_year_v_4_median',\n 'regDate_year_v_8_max',\n 'regDate_year_v_8_min',\n 'regDate_year_v_8_median',\n 'regDate_year_v_12_max',\n 'regDate_year_v_12_min',\n 'regDate_year_v_12_median',\n 'regDate_year_power_max',\n 'regDate_year_power_min',\n 'regDate_year_power_median']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 选择特征列\n",
    "numerical_cols = data.columns\n",
    "\n",
    "cat_fea = ['SaleID','offerType','seller']\n",
    "feature_cols = [col for col in numerical_cols if col not in cat_fea]\n",
    "feature_cols = [col for col in feature_cols if col not in ['price']]\n",
    "\n",
    "## 提前特征列，标签列构造训练样本和测试样本\n",
    "X_data = data.iloc[:len(Train_data),:][feature_cols]\n",
    "Y_data = Train_data['price']\n",
    "X_test  = data.iloc[len(Train_data):,:][feature_cols]\n",
    "\n",
    "feature_cols"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:43:36.390313Z",
     "start_time": "2024-01-16T23:43:36.091020Z"
    }
   },
   "id": "8b9dc722ccd3c43"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from meanencoder import MeanEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:43:36.507841800Z",
     "start_time": "2024-01-16T23:43:36.388311900Z"
    }
   },
   "id": "e586f6128e46d1b4"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class_list = ['model','brand','name','regionCode']+date_cols\n",
    "MeanEnocodeFeature = class_list#声明需要平均数编码的特征\n",
    "ME = MeanEncoder(MeanEnocodeFeature,target_type='regression') #声明平均数编码的类\n",
    "X_data = ME.fit_transform(X_data,Y_data)#对训练数据集的X和y进行拟合\n",
    "#x_train_fav = ME.fit_transform(x_train,y_train_fav)#对训练数据集的X和y进行拟合\n",
    "X_test = ME.transform(X_test)#对测试集进行编码"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:44:11.734397900Z",
     "start_time": "2024-01-16T23:43:36.439311200Z"
    }
   },
   "id": "4ac8932222ce8d94"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "X_data['price'] = Train_data['price']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:44:11.752400200Z",
     "start_time": "2024-01-16T23:44:11.736400300Z"
    }
   },
   "id": "5d7fd3ad324321a7"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:37<00:00,  6.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "(150000, 459)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "### target encoding目标编码，回归场景相对来说做目标编码的选择更多，不仅可以做均值编码，还可以做标准差编码、中位数编码等\n",
    "enc_cols = []\n",
    "stats_default_dict = {\n",
    "    'max': X_data['price'].max(),\n",
    "    'min': X_data['price'].min(),\n",
    "    'median': X_data['price'].median(),\n",
    "    'mean': X_data['price'].mean(),\n",
    "    'sum': X_data['price'].sum(),\n",
    "    'std': X_data['price'].std(),\n",
    "    'skew': X_data['price'].skew(),\n",
    "    'kurt': X_data['price'].kurt(),\n",
    "    'mad': X_data['price'].mad()\n",
    "}\n",
    "### 暂且选择这三种编码\n",
    "enc_stats = ['max','min','mean']\n",
    "skf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for f in tqdm(['regionCode','brand','regDate_year','creatDate_year','kilometer','model']):\n",
    "    enc_dict = {}\n",
    "    for stat in enc_stats:\n",
    "        enc_dict['{}_target_{}'.format(f, stat)] = stat\n",
    "        X_data['{}_target_{}'.format(f, stat)] = 0\n",
    "        X_test['{}_target_{}'.format(f, stat)] = 0\n",
    "        enc_cols.append('{}_target_{}'.format(f, stat))\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(X_data, Y_data)):\n",
    "        trn_x, val_x = X_data.iloc[trn_idx].reset_index(drop=True), X_data.iloc[val_idx].reset_index(drop=True)\n",
    "        enc_df = trn_x.groupby(f, as_index=False)['price'].agg(enc_dict)\n",
    "        val_x = val_x[[f]].merge(enc_df, on=f, how='left')\n",
    "        test_x = X_test[[f]].merge(enc_df, on=f, how='left')\n",
    "        for stat in enc_stats:\n",
    "            val_x['{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "            test_x['{}_target_{}'.format(f, stat)] = test_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "            X_data.loc[val_idx, '{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].values \n",
    "            X_test['{}_target_{}'.format(f, stat)] += test_x['{}_target_{}'.format(f, stat)].values / skf.n_splits\n",
    "\n",
    "X_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:44:49.418560800Z",
     "start_time": "2024-01-16T23:44:11.761397800Z"
    }
   },
   "id": "d32ff18bd1a69442"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(150000, 454)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_list = ['regDate', 'creatDate','brand_power_min', 'regDate_year_power_min']\n",
    "x_train = X_data.drop(drop_list+['price'],axis=1)\n",
    "x_test = X_test.drop(drop_list,axis=1)\n",
    "x_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:44:49.622645200Z",
     "start_time": "2024-01-16T23:44:49.368087700Z"
    }
   },
   "id": "e15c481563a7b0ae"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:44:50.140636100Z",
     "start_time": "2024-01-16T23:44:49.602657Z"
    }
   },
   "id": "b9ad7c25893a68c4"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#特征归一化\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(pd.concat([x_train,x_test]).values)\n",
    "all_data = min_max_scaler.transform(pd.concat([x_train,x_test]).values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:44:52.458652600Z",
     "start_time": "2024-01-16T23:44:50.150632500Z"
    }
   },
   "id": "f49f25a990708417"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=146)\n",
    "all_pca = pca.fit_transform(all_data)\n",
    "X_pca = all_pca[:len(x_train)]\n",
    "test = all_pca[len(x_train):]\n",
    "y = Train_data['price'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:45:08.321391200Z",
     "start_time": "2024-01-16T23:44:52.461620700Z"
    }
   },
   "id": "8c0231af56293e43"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from model import NN_model\n",
    "from evaluation import Metric\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import LearningRateScheduler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:45:17.577128100Z",
     "start_time": "2024-01-16T23:45:08.326823900Z"
    }
   },
   "id": "9334634e54ecff44"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def scheduler(epoch):\n",
    "    # 每隔100个epoch，学习率减小为原来的1/10\n",
    "    if epoch % 20 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.6)\n",
    "        print(\"lr changed to {}\".format(lr * 0.6))\n",
    "    return K.get_value(model.optimizer.lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:45:17.682170Z",
     "start_time": "2024-01-16T23:45:17.590129800Z"
    }
   },
   "id": "9fe99c4084510300"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.optimizers import Adam"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T23:45:18.037133100Z",
     "start_time": "2024-01-16T23:45:17.615130300Z"
    }
   },
   "id": "5307ef69b8f7d9aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "X_train shape: (125000, 146)\n",
      "y_train shape: (125000,)\n",
      "X_val shape: (25000, 146)\n",
      "y_val shape: (25000,)\n",
      "Epoch 1/145\n",
      "63/63 - 3s - loss: 2659.1897 - mae: 2659.1897 - val_loss: 942.0569 - val_mae: 942.0569\n",
      "Epoch 2/145\n",
      "63/63 - 0s - loss: 802.2687 - mae: 802.2687 - val_loss: 693.8436 - val_mae: 693.8436\n",
      "Epoch 3/145\n",
      "63/63 - 1s - loss: 690.1297 - mae: 690.1297 - val_loss: 760.4976 - val_mae: 760.4976\n",
      "Epoch 4/145\n",
      "63/63 - 1s - loss: 649.7761 - mae: 649.7761 - val_loss: 582.4513 - val_mae: 582.4513\n",
      "Epoch 5/145\n",
      "63/63 - 0s - loss: 579.1445 - mae: 579.1445 - val_loss: 607.6285 - val_mae: 607.6285\n",
      "Epoch 6/145\n",
      "63/63 - 0s - loss: 557.7840 - mae: 557.7840 - val_loss: 557.1953 - val_mae: 557.1953\n",
      "Epoch 7/145\n",
      "63/63 - 0s - loss: 577.1560 - mae: 577.1560 - val_loss: 591.2554 - val_mae: 591.2554\n",
      "Epoch 8/145\n",
      "63/63 - 0s - loss: 530.0547 - mae: 530.0547 - val_loss: 513.4880 - val_mae: 513.4880\n",
      "Epoch 9/145\n",
      "63/63 - 0s - loss: 519.0400 - mae: 519.0400 - val_loss: 679.2574 - val_mae: 679.2574\n",
      "Epoch 10/145\n",
      "63/63 - 0s - loss: 564.5353 - mae: 564.5353 - val_loss: 619.3884 - val_mae: 619.3884\n",
      "Epoch 11/145\n",
      "63/63 - 0s - loss: 516.6348 - mae: 516.6348 - val_loss: 498.8586 - val_mae: 498.8586\n",
      "Epoch 12/145\n",
      "63/63 - 0s - loss: 501.3720 - mae: 501.3720 - val_loss: 495.1949 - val_mae: 495.1949\n",
      "Epoch 13/145\n",
      "63/63 - 0s - loss: 487.2951 - mae: 487.2951 - val_loss: 478.1796 - val_mae: 478.1796\n",
      "Epoch 14/145\n",
      "63/63 - 0s - loss: 497.7845 - mae: 497.7845 - val_loss: 491.8081 - val_mae: 491.8081\n",
      "Epoch 15/145\n",
      "63/63 - 0s - loss: 478.1411 - mae: 478.1411 - val_loss: 533.5056 - val_mae: 533.5056\n",
      "Epoch 16/145\n",
      "63/63 - 0s - loss: 480.8681 - mae: 480.8681 - val_loss: 538.1798 - val_mae: 538.1798\n",
      "Epoch 17/145\n",
      "63/63 - 0s - loss: 498.5797 - mae: 498.5797 - val_loss: 523.1959 - val_mae: 523.1959\n",
      "Epoch 18/145\n",
      "63/63 - 0s - loss: 492.2098 - mae: 492.2098 - val_loss: 490.3495 - val_mae: 490.3495\n",
      "Epoch 19/145\n",
      "63/63 - 0s - loss: 480.7495 - mae: 480.7495 - val_loss: 488.7329 - val_mae: 488.7329\n",
      "Epoch 20/145\n",
      "63/63 - 0s - loss: 522.9705 - mae: 522.9705 - val_loss: 475.8549 - val_mae: 475.8549\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 0s - loss: 441.9581 - mae: 441.9581 - val_loss: 462.0298 - val_mae: 462.0298\n",
      "Epoch 22/145\n",
      "63/63 - 0s - loss: 441.0164 - mae: 441.0164 - val_loss: 467.4912 - val_mae: 467.4912\n",
      "Epoch 23/145\n",
      "63/63 - 0s - loss: 435.1718 - mae: 435.1718 - val_loss: 449.2696 - val_mae: 449.2696\n",
      "Epoch 24/145\n",
      "63/63 - 0s - loss: 435.2960 - mae: 435.2960 - val_loss: 471.7802 - val_mae: 471.7802\n",
      "Epoch 25/145\n",
      "63/63 - 0s - loss: 463.9592 - mae: 463.9592 - val_loss: 457.7773 - val_mae: 457.7773\n",
      "Epoch 26/145\n",
      "63/63 - 0s - loss: 434.4807 - mae: 434.4807 - val_loss: 460.6064 - val_mae: 460.6064\n",
      "Epoch 27/145\n",
      "63/63 - 0s - loss: 441.8153 - mae: 441.8153 - val_loss: 449.4599 - val_mae: 449.4599\n",
      "Epoch 28/145\n",
      "63/63 - 1s - loss: 469.4067 - mae: 469.4067 - val_loss: 468.7270 - val_mae: 468.7270\n",
      "Epoch 29/145\n",
      "63/63 - 0s - loss: 432.0421 - mae: 432.0421 - val_loss: 451.8723 - val_mae: 451.8723\n",
      "Epoch 30/145\n",
      "63/63 - 0s - loss: 446.4284 - mae: 446.4284 - val_loss: 466.9776 - val_mae: 466.9776\n",
      "Epoch 31/145\n",
      "63/63 - 0s - loss: 471.8776 - mae: 471.8776 - val_loss: 473.3364 - val_mae: 473.3364\n",
      "Epoch 32/145\n",
      "63/63 - 0s - loss: 427.9698 - mae: 427.9698 - val_loss: 463.0770 - val_mae: 463.0770\n",
      "Epoch 33/145\n",
      "63/63 - 0s - loss: 426.2586 - mae: 426.2586 - val_loss: 452.8384 - val_mae: 452.8384\n",
      "Epoch 34/145\n",
      "63/63 - 0s - loss: 432.6416 - mae: 432.6416 - val_loss: 465.9221 - val_mae: 465.9221\n",
      "Epoch 35/145\n",
      "63/63 - 0s - loss: 430.1502 - mae: 430.1502 - val_loss: 479.1703 - val_mae: 479.1703\n",
      "Epoch 36/145\n",
      "63/63 - 0s - loss: 429.2394 - mae: 429.2394 - val_loss: 467.5671 - val_mae: 467.5671\n",
      "Epoch 37/145\n",
      "63/63 - 0s - loss: 444.4141 - mae: 444.4141 - val_loss: 441.5717 - val_mae: 441.5717\n",
      "Epoch 38/145\n",
      "63/63 - 0s - loss: 419.7642 - mae: 419.7642 - val_loss: 461.8262 - val_mae: 461.8262\n",
      "Epoch 39/145\n",
      "63/63 - 0s - loss: 423.1895 - mae: 423.1895 - val_loss: 442.1304 - val_mae: 442.1304\n",
      "Epoch 40/145\n",
      "63/63 - 0s - loss: 422.6662 - mae: 422.6662 - val_loss: 442.1376 - val_mae: 442.1376\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 0s - loss: 409.9550 - mae: 409.9550 - val_loss: 434.1121 - val_mae: 434.1121\n",
      "Epoch 42/145\n",
      "63/63 - 1s - loss: 405.5215 - mae: 405.5215 - val_loss: 433.5327 - val_mae: 433.5327\n",
      "Epoch 43/145\n",
      "63/63 - 1s - loss: 407.5274 - mae: 407.5274 - val_loss: 435.4460 - val_mae: 435.4460\n",
      "Epoch 44/145\n",
      "63/63 - 1s - loss: 404.5703 - mae: 404.5703 - val_loss: 432.7740 - val_mae: 432.7740\n",
      "Epoch 45/145\n",
      "63/63 - 0s - loss: 403.1758 - mae: 403.1758 - val_loss: 432.4443 - val_mae: 432.4443\n",
      "Epoch 46/145\n",
      "63/63 - 0s - loss: 405.3599 - mae: 405.3599 - val_loss: 441.0552 - val_mae: 441.0552\n",
      "Epoch 47/145\n",
      "63/63 - 0s - loss: 400.4558 - mae: 400.4558 - val_loss: 431.0859 - val_mae: 431.0859\n",
      "Epoch 48/145\n",
      "63/63 - 0s - loss: 401.0205 - mae: 401.0205 - val_loss: 433.4587 - val_mae: 433.4587\n",
      "Epoch 49/145\n",
      "63/63 - 0s - loss: 403.2835 - mae: 403.2835 - val_loss: 433.9560 - val_mae: 433.9560\n",
      "Epoch 50/145\n",
      "63/63 - 0s - loss: 402.9818 - mae: 402.9818 - val_loss: 440.9821 - val_mae: 440.9821\n",
      "Epoch 51/145\n",
      "63/63 - 0s - loss: 412.5438 - mae: 412.5438 - val_loss: 456.5521 - val_mae: 456.5521\n",
      "Epoch 52/145\n",
      "63/63 - 0s - loss: 404.6595 - mae: 404.6595 - val_loss: 432.5534 - val_mae: 432.5534\n",
      "Epoch 53/145\n",
      "63/63 - 0s - loss: 400.0604 - mae: 400.0604 - val_loss: 441.3866 - val_mae: 441.3866\n",
      "Epoch 54/145\n",
      "63/63 - 0s - loss: 399.5844 - mae: 399.5844 - val_loss: 429.2742 - val_mae: 429.2742\n",
      "Epoch 55/145\n",
      "63/63 - 0s - loss: 399.4666 - mae: 399.4666 - val_loss: 432.1665 - val_mae: 432.1665\n",
      "Epoch 56/145\n",
      "63/63 - 0s - loss: 396.0271 - mae: 396.0271 - val_loss: 439.2799 - val_mae: 439.2799\n",
      "Epoch 57/145\n",
      "63/63 - 0s - loss: 399.1983 - mae: 399.1983 - val_loss: 443.6284 - val_mae: 443.6284\n",
      "Epoch 58/145\n",
      "63/63 - 0s - loss: 397.2676 - mae: 397.2676 - val_loss: 429.6819 - val_mae: 429.6819\n",
      "Epoch 59/145\n",
      "63/63 - 0s - loss: 396.2899 - mae: 396.2899 - val_loss: 433.4155 - val_mae: 433.4155\n",
      "Epoch 60/145\n",
      "63/63 - 0s - loss: 396.3258 - mae: 396.3258 - val_loss: 433.7684 - val_mae: 433.7684\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 0s - loss: 388.1072 - mae: 388.1072 - val_loss: 424.7762 - val_mae: 424.7762\n",
      "Epoch 62/145\n",
      "63/63 - 0s - loss: 386.3842 - mae: 386.3842 - val_loss: 426.3331 - val_mae: 426.3331\n",
      "Epoch 63/145\n",
      "63/63 - 0s - loss: 384.7291 - mae: 384.7291 - val_loss: 424.2662 - val_mae: 424.2662\n",
      "Epoch 64/145\n",
      "63/63 - 0s - loss: 385.8115 - mae: 385.8115 - val_loss: 427.9715 - val_mae: 427.9715\n",
      "Epoch 65/145\n",
      "63/63 - 0s - loss: 387.0022 - mae: 387.0022 - val_loss: 424.8974 - val_mae: 424.8974\n",
      "Epoch 66/145\n",
      "63/63 - 0s - loss: 384.9955 - mae: 384.9955 - val_loss: 426.9785 - val_mae: 426.9785\n",
      "Epoch 67/145\n",
      "63/63 - 1s - loss: 384.7816 - mae: 384.7816 - val_loss: 426.4799 - val_mae: 426.4799\n",
      "Epoch 68/145\n",
      "63/63 - 0s - loss: 383.2209 - mae: 383.2209 - val_loss: 424.3418 - val_mae: 424.3418\n",
      "Epoch 69/145\n",
      "63/63 - 0s - loss: 383.8241 - mae: 383.8241 - val_loss: 425.9608 - val_mae: 425.9608\n",
      "Epoch 70/145\n",
      "63/63 - 1s - loss: 385.5752 - mae: 385.5752 - val_loss: 424.4326 - val_mae: 424.4326\n",
      "Epoch 71/145\n",
      "63/63 - 0s - loss: 383.0784 - mae: 383.0784 - val_loss: 426.1613 - val_mae: 426.1613\n",
      "Epoch 72/145\n",
      "63/63 - 0s - loss: 383.1477 - mae: 383.1477 - val_loss: 427.4732 - val_mae: 427.4732\n",
      "Epoch 73/145\n",
      "63/63 - 0s - loss: 384.5332 - mae: 384.5332 - val_loss: 428.9174 - val_mae: 428.9174\n",
      "Epoch 74/145\n",
      "63/63 - 1s - loss: 381.9701 - mae: 381.9701 - val_loss: 429.9641 - val_mae: 429.9641\n",
      "Epoch 75/145\n",
      "63/63 - 0s - loss: 382.4387 - mae: 382.4387 - val_loss: 423.1702 - val_mae: 423.1702\n",
      "Epoch 76/145\n",
      "63/63 - 0s - loss: 378.9988 - mae: 378.9988 - val_loss: 424.8850 - val_mae: 424.8850\n",
      "Epoch 77/145\n",
      "63/63 - 0s - loss: 380.4077 - mae: 380.4077 - val_loss: 425.3723 - val_mae: 425.3723\n",
      "Epoch 78/145\n",
      "63/63 - 0s - loss: 383.2990 - mae: 383.2990 - val_loss: 423.3116 - val_mae: 423.3116\n",
      "Epoch 79/145\n",
      "63/63 - 0s - loss: 380.6279 - mae: 380.6279 - val_loss: 423.5553 - val_mae: 423.5553\n",
      "Epoch 80/145\n",
      "63/63 - 0s - loss: 379.4266 - mae: 379.4266 - val_loss: 426.3364 - val_mae: 426.3364\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 0s - loss: 375.3687 - mae: 375.3687 - val_loss: 424.0731 - val_mae: 424.0731\n",
      "Epoch 82/145\n",
      "63/63 - 0s - loss: 375.1841 - mae: 375.1841 - val_loss: 423.5182 - val_mae: 423.5182\n",
      "Epoch 83/145\n",
      "63/63 - 0s - loss: 375.6773 - mae: 375.6773 - val_loss: 423.3604 - val_mae: 423.3604\n",
      "Epoch 84/145\n",
      "63/63 - 0s - loss: 375.2741 - mae: 375.2741 - val_loss: 423.8428 - val_mae: 423.8428\n",
      "Epoch 85/145\n",
      "63/63 - 1s - loss: 373.3217 - mae: 373.3217 - val_loss: 422.9461 - val_mae: 422.9461\n",
      "Epoch 86/145\n",
      "63/63 - 1s - loss: 373.4687 - mae: 373.4687 - val_loss: 422.2223 - val_mae: 422.2223\n",
      "Epoch 87/145\n",
      "63/63 - 1s - loss: 372.5184 - mae: 372.5184 - val_loss: 422.2244 - val_mae: 422.2244\n",
      "Epoch 88/145\n",
      "63/63 - 0s - loss: 373.3916 - mae: 373.3916 - val_loss: 424.0946 - val_mae: 424.0946\n",
      "Epoch 89/145\n",
      "63/63 - 0s - loss: 371.9553 - mae: 371.9553 - val_loss: 423.3202 - val_mae: 423.3202\n",
      "Epoch 90/145\n",
      "63/63 - 0s - loss: 371.7194 - mae: 371.7194 - val_loss: 421.6765 - val_mae: 421.6765\n",
      "Epoch 91/145\n",
      "63/63 - 0s - loss: 372.0318 - mae: 372.0318 - val_loss: 420.8816 - val_mae: 420.8816\n",
      "Epoch 92/145\n",
      "63/63 - 0s - loss: 370.6498 - mae: 370.6498 - val_loss: 420.7041 - val_mae: 420.7041\n",
      "Epoch 93/145\n",
      "63/63 - 0s - loss: 370.5647 - mae: 370.5647 - val_loss: 422.0036 - val_mae: 422.0036\n",
      "Epoch 94/145\n",
      "63/63 - 0s - loss: 369.6768 - mae: 369.6768 - val_loss: 422.1811 - val_mae: 422.1811\n",
      "Epoch 95/145\n",
      "63/63 - 0s - loss: 370.3658 - mae: 370.3658 - val_loss: 422.1832 - val_mae: 422.1832\n",
      "Epoch 96/145\n",
      "63/63 - 0s - loss: 368.9012 - mae: 368.9012 - val_loss: 422.8804 - val_mae: 422.8804\n",
      "Epoch 97/145\n",
      "63/63 - 1s - loss: 369.3326 - mae: 369.3326 - val_loss: 422.8557 - val_mae: 422.8557\n",
      "Epoch 98/145\n",
      "63/63 - 1s - loss: 369.5988 - mae: 369.5988 - val_loss: 422.2820 - val_mae: 422.2820\n",
      "Epoch 99/145\n",
      "63/63 - 1s - loss: 368.7940 - mae: 368.7940 - val_loss: 423.8507 - val_mae: 423.8507\n",
      "Epoch 100/145\n",
      "63/63 - 0s - loss: 369.3915 - mae: 369.3915 - val_loss: 425.7668 - val_mae: 425.7668\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 0s - loss: 366.0615 - mae: 366.0615 - val_loss: 421.4330 - val_mae: 421.4330\n",
      "Epoch 102/145\n",
      "63/63 - 0s - loss: 364.7704 - mae: 364.7704 - val_loss: 419.7387 - val_mae: 419.7387\n",
      "Epoch 103/145\n",
      "63/63 - 0s - loss: 364.3083 - mae: 364.3083 - val_loss: 420.1241 - val_mae: 420.1241\n",
      "Epoch 104/145\n",
      "63/63 - 0s - loss: 364.0209 - mae: 364.0209 - val_loss: 419.6465 - val_mae: 419.6465\n",
      "Epoch 105/145\n",
      "63/63 - 0s - loss: 363.9225 - mae: 363.9225 - val_loss: 420.6548 - val_mae: 420.6548\n",
      "Epoch 106/145\n",
      "63/63 - 0s - loss: 364.4380 - mae: 364.4380 - val_loss: 420.7474 - val_mae: 420.7474\n",
      "Epoch 107/145\n",
      "63/63 - 0s - loss: 363.7224 - mae: 363.7224 - val_loss: 419.4495 - val_mae: 419.4495\n",
      "Epoch 108/145\n",
      "63/63 - 0s - loss: 364.1410 - mae: 364.1410 - val_loss: 421.7078 - val_mae: 421.7078\n",
      "Epoch 109/145\n",
      "63/63 - 0s - loss: 364.0786 - mae: 364.0786 - val_loss: 420.9077 - val_mae: 420.9077\n",
      "Epoch 110/145\n",
      "63/63 - 0s - loss: 363.7103 - mae: 363.7103 - val_loss: 421.1701 - val_mae: 421.1701\n",
      "Epoch 111/145\n",
      "63/63 - 0s - loss: 362.3549 - mae: 362.3549 - val_loss: 420.8569 - val_mae: 420.8569\n",
      "Epoch 112/145\n",
      "63/63 - 0s - loss: 363.1665 - mae: 363.1665 - val_loss: 420.9326 - val_mae: 420.9326\n",
      "Epoch 113/145\n",
      "63/63 - 0s - loss: 362.2527 - mae: 362.2527 - val_loss: 420.5268 - val_mae: 420.5268\n",
      "Epoch 114/145\n",
      "63/63 - 0s - loss: 362.5966 - mae: 362.5966 - val_loss: 419.7815 - val_mae: 419.7815\n",
      "Epoch 115/145\n",
      "63/63 - 0s - loss: 361.9745 - mae: 361.9745 - val_loss: 421.7957 - val_mae: 421.7957\n",
      "Epoch 116/145\n",
      "63/63 - 0s - loss: 362.0055 - mae: 362.0055 - val_loss: 420.4900 - val_mae: 420.4900\n",
      "Epoch 117/145\n",
      "63/63 - 1s - loss: 361.0572 - mae: 361.0572 - val_loss: 422.4412 - val_mae: 422.4412\n",
      "Epoch 118/145\n",
      "63/63 - 0s - loss: 361.1393 - mae: 361.1393 - val_loss: 422.1734 - val_mae: 422.1734\n",
      "Epoch 119/145\n",
      "63/63 - 1s - loss: 360.6015 - mae: 360.6015 - val_loss: 423.6194 - val_mae: 423.6194\n",
      "Epoch 120/145\n",
      "63/63 - 1s - loss: 361.0377 - mae: 361.0377 - val_loss: 421.7010 - val_mae: 421.7010\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 1s - loss: 358.9190 - mae: 358.9190 - val_loss: 420.0636 - val_mae: 420.0636\n",
      "Epoch 122/145\n",
      "63/63 - 0s - loss: 358.3965 - mae: 358.3965 - val_loss: 419.6224 - val_mae: 419.6224\n",
      "Epoch 123/145\n",
      "63/63 - 0s - loss: 358.2506 - mae: 358.2506 - val_loss: 420.2080 - val_mae: 420.2080\n",
      "Epoch 124/145\n",
      "63/63 - 0s - loss: 358.6979 - mae: 358.6979 - val_loss: 419.8066 - val_mae: 419.8066\n",
      "Epoch 125/145\n",
      "63/63 - 0s - loss: 358.0976 - mae: 358.0976 - val_loss: 420.1978 - val_mae: 420.1978\n",
      "Epoch 126/145\n",
      "63/63 - 1s - loss: 357.7007 - mae: 357.7007 - val_loss: 420.2847 - val_mae: 420.2847\n",
      "Epoch 127/145\n",
      "63/63 - 0s - loss: 357.9555 - mae: 357.9555 - val_loss: 419.7178 - val_mae: 419.7178\n",
      "Epoch 128/145\n",
      "63/63 - 1s - loss: 357.7490 - mae: 357.7490 - val_loss: 419.6374 - val_mae: 419.6374\n",
      "Epoch 129/145\n",
      "63/63 - 0s - loss: 357.8810 - mae: 357.8810 - val_loss: 421.1440 - val_mae: 421.1440\n",
      "Epoch 130/145\n",
      "63/63 - 0s - loss: 357.2990 - mae: 357.2990 - val_loss: 421.1245 - val_mae: 421.1245\n",
      "Epoch 131/145\n",
      "63/63 - 0s - loss: 357.6569 - mae: 357.6569 - val_loss: 420.3764 - val_mae: 420.3764\n",
      "Epoch 132/145\n",
      "63/63 - 0s - loss: 356.7381 - mae: 356.7381 - val_loss: 419.6228 - val_mae: 419.6228\n",
      "Epoch 133/145\n",
      "63/63 - 0s - loss: 357.0678 - mae: 357.0678 - val_loss: 421.5705 - val_mae: 421.5705\n",
      "Epoch 134/145\n",
      "63/63 - 0s - loss: 357.3365 - mae: 357.3365 - val_loss: 419.7352 - val_mae: 419.7352\n",
      "Epoch 135/145\n",
      "63/63 - 0s - loss: 357.1322 - mae: 357.1322 - val_loss: 420.5158 - val_mae: 420.5158\n",
      "Epoch 136/145\n",
      "63/63 - 0s - loss: 356.3812 - mae: 356.3812 - val_loss: 420.9849 - val_mae: 420.9849\n",
      "Epoch 137/145\n",
      "63/63 - 0s - loss: 356.4712 - mae: 356.4712 - val_loss: 419.9409 - val_mae: 419.9409\n",
      "Epoch 138/145\n",
      "63/63 - 0s - loss: 356.2388 - mae: 356.2388 - val_loss: 420.5178 - val_mae: 420.5178\n",
      "Epoch 139/145\n",
      "63/63 - 0s - loss: 356.1241 - mae: 356.1241 - val_loss: 420.4001 - val_mae: 420.4001\n",
      "Epoch 140/145\n",
      "63/63 - 0s - loss: 355.7951 - mae: 355.7951 - val_loss: 419.9756 - val_mae: 419.9756\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 0s - loss: 354.7281 - mae: 354.7281 - val_loss: 420.8898 - val_mae: 420.8898\n",
      "Epoch 142/145\n",
      "63/63 - 0s - loss: 354.6068 - mae: 354.6068 - val_loss: 419.9393 - val_mae: 419.9393\n",
      "Epoch 143/145\n",
      "63/63 - 0s - loss: 354.1288 - mae: 354.1288 - val_loss: 419.7468 - val_mae: 419.7468\n",
      "Epoch 144/145\n",
      "63/63 - 0s - loss: 354.4079 - mae: 354.4079 - val_loss: 420.0470 - val_mae: 420.0470\n",
      "Epoch 145/145\n",
      "63/63 - 0s - loss: 354.0063 - mae: 354.0063 - val_loss: 421.7607 - val_mae: 421.7607\n",
      "\n",
      "val_mae is:421.7607248244905\n",
      "\n",
      "fold: 1\n",
      "X_train shape: (125000, 146)\n",
      "y_train shape: (125000,)\n",
      "X_val shape: (25000, 146)\n",
      "y_val shape: (25000,)\n",
      "Epoch 1/145\n",
      "63/63 - 1s - loss: 2530.9834 - mae: 2530.9834 - val_loss: 903.3593 - val_mae: 903.3593\n",
      "Epoch 2/145\n",
      "63/63 - 1s - loss: 776.5204 - mae: 776.5204 - val_loss: 758.3347 - val_mae: 758.3347\n",
      "Epoch 3/145\n",
      "63/63 - 1s - loss: 697.4680 - mae: 697.4680 - val_loss: 612.9960 - val_mae: 612.9960\n",
      "Epoch 4/145\n",
      "63/63 - 1s - loss: 622.1455 - mae: 622.1455 - val_loss: 604.6565 - val_mae: 604.6565\n",
      "Epoch 5/145\n",
      "63/63 - 0s - loss: 598.6787 - mae: 598.6787 - val_loss: 561.8325 - val_mae: 561.8325\n",
      "Epoch 6/145\n",
      "63/63 - 0s - loss: 621.9009 - mae: 621.9009 - val_loss: 584.8602 - val_mae: 584.8602\n",
      "Epoch 7/145\n",
      "63/63 - 0s - loss: 544.6053 - mae: 544.6053 - val_loss: 571.2589 - val_mae: 571.2589\n",
      "Epoch 8/145\n",
      "63/63 - 0s - loss: 605.7774 - mae: 605.7774 - val_loss: 620.4084 - val_mae: 620.4084\n",
      "Epoch 9/145\n",
      "63/63 - 1s - loss: 590.6803 - mae: 590.6803 - val_loss: 539.7665 - val_mae: 539.7665\n",
      "Epoch 10/145\n",
      "63/63 - 1s - loss: 537.2467 - mae: 537.2467 - val_loss: 495.9846 - val_mae: 495.9846\n",
      "Epoch 11/145\n",
      "63/63 - 0s - loss: 502.3617 - mae: 502.3617 - val_loss: 494.0268 - val_mae: 494.0268\n",
      "Epoch 12/145\n",
      "63/63 - 0s - loss: 505.5865 - mae: 505.5865 - val_loss: 528.7049 - val_mae: 528.7049\n",
      "Epoch 13/145\n",
      "63/63 - 0s - loss: 494.4857 - mae: 494.4857 - val_loss: 488.9787 - val_mae: 488.9787\n",
      "Epoch 14/145\n",
      "63/63 - 0s - loss: 533.8041 - mae: 533.8041 - val_loss: 496.7134 - val_mae: 496.7134\n",
      "Epoch 15/145\n",
      "63/63 - 0s - loss: 484.3690 - mae: 484.3690 - val_loss: 471.0030 - val_mae: 471.0030\n",
      "Epoch 16/145\n",
      "63/63 - 0s - loss: 479.3401 - mae: 479.3401 - val_loss: 477.5144 - val_mae: 477.5144\n",
      "Epoch 17/145\n",
      "63/63 - 0s - loss: 480.5164 - mae: 480.5164 - val_loss: 472.2747 - val_mae: 472.2747\n",
      "Epoch 18/145\n",
      "63/63 - 0s - loss: 528.7824 - mae: 528.7824 - val_loss: 489.2705 - val_mae: 489.2705\n",
      "Epoch 19/145\n",
      "63/63 - 0s - loss: 482.1009 - mae: 482.1009 - val_loss: 491.8954 - val_mae: 491.8955\n",
      "Epoch 20/145\n",
      "63/63 - 0s - loss: 515.5218 - mae: 515.5218 - val_loss: 470.7824 - val_mae: 470.7824\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 0s - loss: 447.7181 - mae: 447.7181 - val_loss: 459.2869 - val_mae: 459.2869\n",
      "Epoch 22/145\n",
      "63/63 - 0s - loss: 444.1628 - mae: 444.1628 - val_loss: 453.9291 - val_mae: 453.9291\n",
      "Epoch 23/145\n",
      "63/63 - 0s - loss: 445.3747 - mae: 445.3747 - val_loss: 475.5170 - val_mae: 475.5170\n",
      "Epoch 24/145\n",
      "63/63 - 0s - loss: 445.8677 - mae: 445.8677 - val_loss: 453.1812 - val_mae: 453.1812\n",
      "Epoch 25/145\n",
      "63/63 - 0s - loss: 446.4157 - mae: 446.4157 - val_loss: 532.4023 - val_mae: 532.4023\n",
      "Epoch 26/145\n",
      "63/63 - 0s - loss: 480.7697 - mae: 480.7697 - val_loss: 469.6761 - val_mae: 469.6761\n",
      "Epoch 27/145\n",
      "63/63 - 0s - loss: 467.2286 - mae: 467.2286 - val_loss: 457.9594 - val_mae: 457.9594\n",
      "Epoch 28/145\n",
      "63/63 - 1s - loss: 443.0637 - mae: 443.0637 - val_loss: 448.7168 - val_mae: 448.7168\n",
      "Epoch 29/145\n",
      "63/63 - 1s - loss: 435.3391 - mae: 435.3391 - val_loss: 489.3189 - val_mae: 489.3189\n",
      "Epoch 30/145\n",
      "63/63 - 1s - loss: 443.1266 - mae: 443.1266 - val_loss: 463.8668 - val_mae: 463.8668\n",
      "Epoch 31/145\n",
      "63/63 - 0s - loss: 434.4808 - mae: 434.4808 - val_loss: 447.9013 - val_mae: 447.9013\n",
      "Epoch 32/145\n",
      "63/63 - 0s - loss: 432.3078 - mae: 432.3078 - val_loss: 454.7010 - val_mae: 454.7010\n",
      "Epoch 33/145\n",
      "63/63 - 0s - loss: 453.7386 - mae: 453.7386 - val_loss: 478.6525 - val_mae: 478.6525\n",
      "Epoch 34/145\n",
      "63/63 - 0s - loss: 447.3964 - mae: 447.3964 - val_loss: 482.5537 - val_mae: 482.5537\n",
      "Epoch 35/145\n",
      "63/63 - 0s - loss: 447.9080 - mae: 447.9080 - val_loss: 447.7088 - val_mae: 447.7088\n",
      "Epoch 36/145\n",
      "63/63 - 0s - loss: 430.2564 - mae: 430.2564 - val_loss: 463.4916 - val_mae: 463.4916\n",
      "Epoch 37/145\n",
      "63/63 - 0s - loss: 430.5285 - mae: 430.5285 - val_loss: 468.0360 - val_mae: 468.0360\n",
      "Epoch 38/145\n",
      "63/63 - 0s - loss: 431.0637 - mae: 431.0637 - val_loss: 455.6858 - val_mae: 455.6858\n",
      "Epoch 39/145\n",
      "63/63 - 0s - loss: 428.7351 - mae: 428.7351 - val_loss: 458.3249 - val_mae: 458.3249\n",
      "Epoch 40/145\n",
      "63/63 - 0s - loss: 431.7155 - mae: 431.7155 - val_loss: 440.9700 - val_mae: 440.9700\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 0s - loss: 414.9554 - mae: 414.9554 - val_loss: 440.9891 - val_mae: 440.9891\n",
      "Epoch 42/145\n",
      "63/63 - 0s - loss: 414.6955 - mae: 414.6955 - val_loss: 436.0281 - val_mae: 436.0281\n",
      "Epoch 43/145\n",
      "63/63 - 0s - loss: 411.0544 - mae: 411.0544 - val_loss: 435.0219 - val_mae: 435.0219\n",
      "Epoch 44/145\n",
      "63/63 - 0s - loss: 412.5526 - mae: 412.5526 - val_loss: 452.0578 - val_mae: 452.0578\n",
      "Epoch 45/145\n",
      "63/63 - 0s - loss: 418.5846 - mae: 418.5846 - val_loss: 438.6929 - val_mae: 438.6929\n",
      "Epoch 46/145\n",
      "63/63 - 0s - loss: 411.2344 - mae: 411.2344 - val_loss: 440.2191 - val_mae: 440.2191\n",
      "Epoch 47/145\n",
      "63/63 - 0s - loss: 412.5822 - mae: 412.5822 - val_loss: 459.3285 - val_mae: 459.3285\n",
      "Epoch 48/145\n",
      "63/63 - 0s - loss: 434.9456 - mae: 434.9456 - val_loss: 452.5447 - val_mae: 452.5447\n",
      "Epoch 49/145\n",
      "63/63 - 0s - loss: 413.1917 - mae: 413.1917 - val_loss: 435.6187 - val_mae: 435.6187\n",
      "Epoch 50/145\n",
      "63/63 - 0s - loss: 407.4875 - mae: 407.4875 - val_loss: 446.1437 - val_mae: 446.1437\n",
      "Epoch 51/145\n",
      "63/63 - 0s - loss: 406.6748 - mae: 406.6748 - val_loss: 443.8379 - val_mae: 443.8379\n",
      "Epoch 52/145\n",
      "63/63 - 0s - loss: 405.7872 - mae: 405.7872 - val_loss: 434.8888 - val_mae: 434.8888\n",
      "Epoch 53/145\n",
      "63/63 - 0s - loss: 405.5676 - mae: 405.5676 - val_loss: 435.8262 - val_mae: 435.8262\n",
      "Epoch 54/145\n",
      "63/63 - 0s - loss: 404.7690 - mae: 404.7690 - val_loss: 448.5369 - val_mae: 448.5369\n",
      "Epoch 55/145\n",
      "63/63 - 0s - loss: 410.0044 - mae: 410.0044 - val_loss: 443.3590 - val_mae: 443.3590\n",
      "Epoch 56/145\n",
      "63/63 - 0s - loss: 404.7604 - mae: 404.7604 - val_loss: 439.7226 - val_mae: 439.7226\n",
      "Epoch 57/145\n",
      "63/63 - 0s - loss: 402.1884 - mae: 402.1884 - val_loss: 433.5016 - val_mae: 433.5016\n",
      "Epoch 58/145\n",
      "63/63 - 0s - loss: 404.3133 - mae: 404.3133 - val_loss: 431.3912 - val_mae: 431.3912\n",
      "Epoch 59/145\n",
      "63/63 - 1s - loss: 403.6874 - mae: 403.6874 - val_loss: 444.1104 - val_mae: 444.1104\n",
      "Epoch 60/145\n",
      "63/63 - 0s - loss: 407.6662 - mae: 407.6662 - val_loss: 471.8378 - val_mae: 471.8378\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 0s - loss: 397.5267 - mae: 397.5267 - val_loss: 427.2241 - val_mae: 427.2241\n",
      "Epoch 62/145\n",
      "63/63 - 0s - loss: 393.2639 - mae: 393.2639 - val_loss: 426.1389 - val_mae: 426.1389\n",
      "Epoch 63/145\n",
      "63/63 - 0s - loss: 393.3992 - mae: 393.3992 - val_loss: 427.2769 - val_mae: 427.2769\n",
      "Epoch 64/145\n",
      "63/63 - 0s - loss: 391.6818 - mae: 391.6818 - val_loss: 430.7137 - val_mae: 430.7137\n",
      "Epoch 65/145\n",
      "63/63 - 0s - loss: 392.8868 - mae: 392.8868 - val_loss: 425.6103 - val_mae: 425.6103\n",
      "Epoch 66/145\n",
      "63/63 - 0s - loss: 392.1701 - mae: 392.1701 - val_loss: 431.6930 - val_mae: 431.6930\n",
      "Epoch 67/145\n",
      "63/63 - 0s - loss: 394.4195 - mae: 394.4195 - val_loss: 429.3620 - val_mae: 429.3620\n",
      "Epoch 68/145\n",
      "63/63 - 1s - loss: 395.9036 - mae: 395.9036 - val_loss: 429.0199 - val_mae: 429.0199\n",
      "Epoch 69/145\n",
      "63/63 - 0s - loss: 391.3318 - mae: 391.3318 - val_loss: 432.2227 - val_mae: 432.2227\n",
      "Epoch 70/145\n",
      "63/63 - 0s - loss: 390.0235 - mae: 390.0235 - val_loss: 427.0204 - val_mae: 427.0204\n",
      "Epoch 71/145\n",
      "63/63 - 0s - loss: 391.8919 - mae: 391.8919 - val_loss: 428.3155 - val_mae: 428.3155\n",
      "Epoch 72/145\n",
      "63/63 - 0s - loss: 387.6902 - mae: 387.6902 - val_loss: 431.5295 - val_mae: 431.5295\n",
      "Epoch 73/145\n",
      "63/63 - 0s - loss: 390.0627 - mae: 390.0627 - val_loss: 428.0393 - val_mae: 428.0393\n",
      "Epoch 74/145\n",
      "63/63 - 0s - loss: 388.4476 - mae: 388.4476 - val_loss: 430.9947 - val_mae: 430.9947\n",
      "Epoch 75/145\n",
      "63/63 - 0s - loss: 397.2595 - mae: 397.2595 - val_loss: 440.0986 - val_mae: 440.0986\n",
      "Epoch 76/145\n",
      "63/63 - 0s - loss: 389.8421 - mae: 389.8421 - val_loss: 429.0105 - val_mae: 429.0105\n",
      "Epoch 77/145\n",
      "63/63 - 0s - loss: 388.3564 - mae: 388.3564 - val_loss: 429.9809 - val_mae: 429.9809\n",
      "Epoch 78/145\n",
      "63/63 - 1s - loss: 386.6860 - mae: 386.6860 - val_loss: 426.9159 - val_mae: 426.9159\n",
      "Epoch 79/145\n",
      "63/63 - 0s - loss: 385.0441 - mae: 385.0441 - val_loss: 426.3862 - val_mae: 426.3862\n",
      "Epoch 80/145\n",
      "63/63 - 1s - loss: 384.9296 - mae: 384.9296 - val_loss: 425.6624 - val_mae: 425.6624\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 0s - loss: 380.4161 - mae: 380.4161 - val_loss: 421.9240 - val_mae: 421.9240\n",
      "Epoch 82/145\n",
      "63/63 - 0s - loss: 379.7239 - mae: 379.7239 - val_loss: 422.1008 - val_mae: 422.1008\n",
      "Epoch 83/145\n",
      "63/63 - 0s - loss: 381.4329 - mae: 381.4329 - val_loss: 422.4390 - val_mae: 422.4390\n",
      "Epoch 84/145\n",
      "63/63 - 0s - loss: 378.6559 - mae: 378.6559 - val_loss: 423.4104 - val_mae: 423.4104\n",
      "Epoch 85/145\n",
      "63/63 - 0s - loss: 378.5559 - mae: 378.5559 - val_loss: 422.9039 - val_mae: 422.9039\n",
      "Epoch 86/145\n",
      "63/63 - 0s - loss: 378.5854 - mae: 378.5854 - val_loss: 420.8448 - val_mae: 420.8448\n",
      "Epoch 87/145\n",
      "63/63 - 0s - loss: 377.5202 - mae: 377.5202 - val_loss: 421.0541 - val_mae: 421.0541\n",
      "Epoch 88/145\n",
      "63/63 - 0s - loss: 377.8752 - mae: 377.8752 - val_loss: 422.3998 - val_mae: 422.3998\n",
      "Epoch 89/145\n",
      "63/63 - 0s - loss: 377.9762 - mae: 377.9762 - val_loss: 423.5929 - val_mae: 423.5929\n",
      "Epoch 90/145\n",
      "63/63 - 0s - loss: 376.6592 - mae: 376.6592 - val_loss: 421.2956 - val_mae: 421.2956\n",
      "Epoch 91/145\n",
      "63/63 - 1s - loss: 377.8741 - mae: 377.8741 - val_loss: 421.9284 - val_mae: 421.9284\n",
      "Epoch 92/145\n",
      "63/63 - 0s - loss: 376.3509 - mae: 376.3509 - val_loss: 420.1221 - val_mae: 420.1221\n",
      "Epoch 93/145\n",
      "63/63 - 1s - loss: 376.4019 - mae: 376.4019 - val_loss: 421.1740 - val_mae: 421.1740\n",
      "Epoch 94/145\n",
      "63/63 - 1s - loss: 376.1488 - mae: 376.1488 - val_loss: 419.8897 - val_mae: 419.8897\n",
      "Epoch 95/145\n",
      "63/63 - 1s - loss: 375.0143 - mae: 375.0143 - val_loss: 423.4165 - val_mae: 423.4165\n",
      "Epoch 96/145\n",
      "63/63 - 0s - loss: 376.1860 - mae: 376.1860 - val_loss: 421.4332 - val_mae: 421.4332\n",
      "Epoch 97/145\n",
      "63/63 - 0s - loss: 375.2726 - mae: 375.2726 - val_loss: 418.8181 - val_mae: 418.8181\n",
      "Epoch 98/145\n",
      "63/63 - 0s - loss: 374.0470 - mae: 374.0470 - val_loss: 421.2864 - val_mae: 421.2864\n",
      "Epoch 99/145\n",
      "63/63 - 0s - loss: 375.1087 - mae: 375.1087 - val_loss: 420.5310 - val_mae: 420.5310\n",
      "Epoch 100/145\n",
      "63/63 - 0s - loss: 376.1697 - mae: 376.1697 - val_loss: 424.7849 - val_mae: 424.7849\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 0s - loss: 372.2998 - mae: 372.2998 - val_loss: 419.3198 - val_mae: 419.3198\n",
      "Epoch 102/145\n",
      "63/63 - 1s - loss: 371.2066 - mae: 371.2066 - val_loss: 419.2918 - val_mae: 419.2918\n",
      "Epoch 103/145\n",
      "63/63 - 0s - loss: 371.3770 - mae: 371.3770 - val_loss: 422.1715 - val_mae: 422.1715\n",
      "Epoch 104/145\n",
      "63/63 - 0s - loss: 370.9552 - mae: 370.9552 - val_loss: 419.6117 - val_mae: 419.6117\n",
      "Epoch 105/145\n",
      "63/63 - 0s - loss: 370.8510 - mae: 370.8510 - val_loss: 419.5582 - val_mae: 419.5582\n",
      "Epoch 106/145\n",
      "63/63 - 1s - loss: 369.5962 - mae: 369.5962 - val_loss: 418.4789 - val_mae: 418.4789\n",
      "Epoch 107/145\n",
      "63/63 - 1s - loss: 369.7211 - mae: 369.7211 - val_loss: 418.1731 - val_mae: 418.1731\n",
      "Epoch 108/145\n",
      "63/63 - 1s - loss: 369.4393 - mae: 369.4393 - val_loss: 421.8817 - val_mae: 421.8817\n",
      "Epoch 109/145\n",
      "63/63 - 0s - loss: 369.3861 - mae: 369.3861 - val_loss: 419.2677 - val_mae: 419.2677\n",
      "Epoch 110/145\n",
      "63/63 - 0s - loss: 368.3105 - mae: 368.3105 - val_loss: 418.1732 - val_mae: 418.1732\n",
      "Epoch 111/145\n",
      "63/63 - 0s - loss: 369.6034 - mae: 369.6034 - val_loss: 420.5647 - val_mae: 420.5647\n",
      "Epoch 112/145\n",
      "63/63 - 0s - loss: 368.2737 - mae: 368.2737 - val_loss: 418.5504 - val_mae: 418.5504\n",
      "Epoch 113/145\n",
      "63/63 - 0s - loss: 368.1869 - mae: 368.1869 - val_loss: 418.3406 - val_mae: 418.3406\n",
      "Epoch 114/145\n",
      "63/63 - 0s - loss: 368.2286 - mae: 368.2286 - val_loss: 419.1360 - val_mae: 419.1360\n",
      "Epoch 115/145\n",
      "63/63 - 0s - loss: 368.3538 - mae: 368.3538 - val_loss: 421.8658 - val_mae: 421.8658\n",
      "Epoch 116/145\n",
      "63/63 - 0s - loss: 367.2947 - mae: 367.2947 - val_loss: 419.5444 - val_mae: 419.5444\n",
      "Epoch 117/145\n",
      "63/63 - 0s - loss: 367.0861 - mae: 367.0861 - val_loss: 419.8736 - val_mae: 419.8736\n",
      "Epoch 118/145\n",
      "63/63 - 0s - loss: 367.0905 - mae: 367.0905 - val_loss: 419.2848 - val_mae: 419.2848\n",
      "Epoch 119/145\n",
      "63/63 - 0s - loss: 366.7505 - mae: 366.7505 - val_loss: 420.9994 - val_mae: 420.9994\n",
      "Epoch 120/145\n",
      "63/63 - 0s - loss: 367.1214 - mae: 367.1214 - val_loss: 419.2952 - val_mae: 419.2952\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 1s - loss: 364.9665 - mae: 364.9665 - val_loss: 417.2979 - val_mae: 417.2979\n",
      "Epoch 122/145\n",
      "63/63 - 0s - loss: 364.3879 - mae: 364.3879 - val_loss: 417.9355 - val_mae: 417.9355\n",
      "Epoch 123/145\n",
      "63/63 - 1s - loss: 364.4897 - mae: 364.4897 - val_loss: 419.0646 - val_mae: 419.0646\n",
      "Epoch 124/145\n",
      "63/63 - 0s - loss: 364.3287 - mae: 364.3287 - val_loss: 417.0383 - val_mae: 417.0383\n",
      "Epoch 125/145\n",
      "63/63 - 0s - loss: 364.7832 - mae: 364.7832 - val_loss: 418.1644 - val_mae: 418.1644\n",
      "Epoch 126/145\n",
      "63/63 - 1s - loss: 363.7924 - mae: 363.7924 - val_loss: 418.9771 - val_mae: 418.9771\n",
      "Epoch 127/145\n",
      "63/63 - 0s - loss: 364.1092 - mae: 364.1092 - val_loss: 417.9632 - val_mae: 417.9632\n",
      "Epoch 128/145\n",
      "63/63 - 0s - loss: 363.4162 - mae: 363.4162 - val_loss: 417.5016 - val_mae: 417.5016\n",
      "Epoch 129/145\n",
      "63/63 - 0s - loss: 363.7142 - mae: 363.7142 - val_loss: 418.3159 - val_mae: 418.3159\n",
      "Epoch 130/145\n",
      "63/63 - 0s - loss: 363.4936 - mae: 363.4936 - val_loss: 417.7403 - val_mae: 417.7403\n",
      "Epoch 131/145\n",
      "63/63 - 0s - loss: 362.7830 - mae: 362.7830 - val_loss: 418.7817 - val_mae: 418.7817\n",
      "Epoch 132/145\n",
      "63/63 - 1s - loss: 362.5331 - mae: 362.5331 - val_loss: 418.1519 - val_mae: 418.1519\n",
      "Epoch 133/145\n",
      "63/63 - 0s - loss: 362.9194 - mae: 362.9194 - val_loss: 418.0300 - val_mae: 418.0300\n",
      "Epoch 134/145\n",
      "63/63 - 0s - loss: 362.5685 - mae: 362.5685 - val_loss: 417.9628 - val_mae: 417.9628\n",
      "Epoch 135/145\n",
      "63/63 - 0s - loss: 362.6238 - mae: 362.6238 - val_loss: 418.3218 - val_mae: 418.3218\n",
      "Epoch 136/145\n",
      "63/63 - 0s - loss: 362.6427 - mae: 362.6427 - val_loss: 418.2118 - val_mae: 418.2118\n",
      "Epoch 137/145\n",
      "63/63 - 0s - loss: 362.1905 - mae: 362.1905 - val_loss: 418.1622 - val_mae: 418.1622\n",
      "Epoch 138/145\n",
      "63/63 - 0s - loss: 361.9091 - mae: 361.9091 - val_loss: 418.2684 - val_mae: 418.2684\n",
      "Epoch 139/145\n",
      "63/63 - 0s - loss: 361.8766 - mae: 361.8766 - val_loss: 419.9512 - val_mae: 419.9512\n",
      "Epoch 140/145\n",
      "63/63 - 1s - loss: 362.7884 - mae: 362.7884 - val_loss: 419.0649 - val_mae: 419.0649\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 1s - loss: 360.9482 - mae: 360.9482 - val_loss: 417.3257 - val_mae: 417.3257\n",
      "Epoch 142/145\n",
      "63/63 - 0s - loss: 360.4881 - mae: 360.4881 - val_loss: 417.3739 - val_mae: 417.3739\n",
      "Epoch 143/145\n",
      "63/63 - 1s - loss: 360.3195 - mae: 360.3195 - val_loss: 417.4113 - val_mae: 417.4113\n",
      "Epoch 144/145\n",
      "63/63 - 1s - loss: 360.4012 - mae: 360.4012 - val_loss: 417.4507 - val_mae: 417.4507\n",
      "Epoch 145/145\n",
      "63/63 - 1s - loss: 360.1396 - mae: 360.1396 - val_loss: 417.6864 - val_mae: 417.6864\n",
      "\n",
      "val_mae is:417.68632835056303\n",
      "\n",
      "fold: 2\n",
      "X_train shape: (125000, 146)\n",
      "y_train shape: (125000,)\n",
      "X_val shape: (25000, 146)\n",
      "y_val shape: (25000,)\n",
      "Epoch 1/145\n",
      "63/63 - 1s - loss: 2676.4846 - mae: 2676.4846 - val_loss: 933.7377 - val_mae: 933.7377\n",
      "Epoch 2/145\n",
      "63/63 - 0s - loss: 793.9495 - mae: 793.9495 - val_loss: 712.4302 - val_mae: 712.4302\n",
      "Epoch 3/145\n",
      "63/63 - 0s - loss: 672.8418 - mae: 672.8418 - val_loss: 600.0071 - val_mae: 600.0071\n",
      "Epoch 4/145\n",
      "63/63 - 0s - loss: 700.0167 - mae: 700.0167 - val_loss: 695.5670 - val_mae: 695.5670\n",
      "Epoch 5/145\n",
      "63/63 - 0s - loss: 618.4891 - mae: 618.4891 - val_loss: 534.2131 - val_mae: 534.2131\n",
      "Epoch 6/145\n",
      "63/63 - 0s - loss: 559.0303 - mae: 559.0303 - val_loss: 524.0373 - val_mae: 524.0373\n",
      "Epoch 7/145\n",
      "63/63 - 0s - loss: 582.2073 - mae: 582.2073 - val_loss: 517.4819 - val_mae: 517.4819\n",
      "Epoch 8/145\n",
      "63/63 - 0s - loss: 516.7654 - mae: 516.7654 - val_loss: 493.1472 - val_mae: 493.1472\n",
      "Epoch 9/145\n",
      "63/63 - 1s - loss: 516.6331 - mae: 516.6331 - val_loss: 517.4839 - val_mae: 517.4839\n",
      "Epoch 10/145\n",
      "63/63 - 1s - loss: 558.8071 - mae: 558.8071 - val_loss: 545.5575 - val_mae: 545.5575\n",
      "Epoch 11/145\n",
      "63/63 - 1s - loss: 563.0508 - mae: 563.0508 - val_loss: 530.1390 - val_mae: 530.1390\n",
      "Epoch 12/145\n",
      "63/63 - 0s - loss: 533.6630 - mae: 533.6630 - val_loss: 527.1852 - val_mae: 527.1852\n",
      "Epoch 13/145\n",
      "63/63 - 0s - loss: 524.2913 - mae: 524.2913 - val_loss: 472.1310 - val_mae: 472.1310\n",
      "Epoch 14/145\n",
      "63/63 - 1s - loss: 486.9312 - mae: 486.9312 - val_loss: 483.1170 - val_mae: 483.1170\n",
      "Epoch 15/145\n",
      "63/63 - 1s - loss: 474.9165 - mae: 474.9165 - val_loss: 472.7026 - val_mae: 472.7026\n",
      "Epoch 16/145\n",
      "63/63 - 0s - loss: 497.2449 - mae: 497.2449 - val_loss: 551.6448 - val_mae: 551.6448\n",
      "Epoch 17/145\n",
      "63/63 - 0s - loss: 534.3772 - mae: 534.3772 - val_loss: 502.6799 - val_mae: 502.6799\n",
      "Epoch 18/145\n",
      "63/63 - 0s - loss: 529.5269 - mae: 529.5269 - val_loss: 503.1229 - val_mae: 503.1229\n",
      "Epoch 19/145\n",
      "63/63 - 0s - loss: 472.1886 - mae: 472.1886 - val_loss: 459.0539 - val_mae: 459.0539\n",
      "Epoch 20/145\n",
      "63/63 - 0s - loss: 468.8379 - mae: 468.8379 - val_loss: 460.3961 - val_mae: 460.3961\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 1s - loss: 445.3602 - mae: 445.3602 - val_loss: 440.3909 - val_mae: 440.3909\n",
      "Epoch 22/145\n",
      "63/63 - 1s - loss: 443.4320 - mae: 443.4320 - val_loss: 437.7498 - val_mae: 437.7498\n",
      "Epoch 23/145\n",
      "63/63 - 0s - loss: 444.5145 - mae: 444.5145 - val_loss: 440.8266 - val_mae: 440.8266\n",
      "Epoch 24/145\n",
      "63/63 - 0s - loss: 441.2583 - mae: 441.2583 - val_loss: 443.0975 - val_mae: 443.0975\n",
      "Epoch 25/145\n",
      "63/63 - 0s - loss: 441.7611 - mae: 441.7611 - val_loss: 439.9971 - val_mae: 439.9971\n",
      "Epoch 26/145\n",
      "63/63 - 0s - loss: 441.2945 - mae: 441.2945 - val_loss: 451.3994 - val_mae: 451.3994\n",
      "Epoch 27/145\n",
      "63/63 - 0s - loss: 437.5323 - mae: 437.5323 - val_loss: 445.6341 - val_mae: 445.6341\n",
      "Epoch 28/145\n",
      "63/63 - 0s - loss: 440.0472 - mae: 440.0472 - val_loss: 437.6601 - val_mae: 437.6601\n",
      "Epoch 29/145\n",
      "63/63 - 0s - loss: 463.5792 - mae: 463.5792 - val_loss: 567.1045 - val_mae: 567.1045\n",
      "Epoch 30/145\n",
      "63/63 - 0s - loss: 462.3744 - mae: 462.3744 - val_loss: 432.1537 - val_mae: 432.1537\n",
      "Epoch 31/145\n",
      "63/63 - 0s - loss: 438.1516 - mae: 438.1516 - val_loss: 441.8894 - val_mae: 441.8894\n",
      "Epoch 32/145\n",
      "63/63 - 0s - loss: 460.2001 - mae: 460.2001 - val_loss: 442.8056 - val_mae: 442.8056\n",
      "Epoch 33/145\n",
      "63/63 - 0s - loss: 434.7754 - mae: 434.7754 - val_loss: 467.1758 - val_mae: 467.1758\n",
      "Epoch 34/145\n",
      "63/63 - 0s - loss: 433.0918 - mae: 433.0918 - val_loss: 435.1135 - val_mae: 435.1135\n",
      "Epoch 35/145\n",
      "63/63 - 0s - loss: 442.1693 - mae: 442.1693 - val_loss: 463.2071 - val_mae: 463.2071\n",
      "Epoch 36/145\n",
      "63/63 - 0s - loss: 459.0646 - mae: 459.0646 - val_loss: 441.0761 - val_mae: 441.0761\n",
      "Epoch 37/145\n",
      "63/63 - 0s - loss: 429.8079 - mae: 429.8079 - val_loss: 445.1725 - val_mae: 445.1725\n",
      "Epoch 38/145\n",
      "63/63 - 0s - loss: 433.2692 - mae: 433.2692 - val_loss: 477.4773 - val_mae: 477.4773\n",
      "Epoch 39/145\n",
      "63/63 - 0s - loss: 446.0303 - mae: 446.0303 - val_loss: 446.7386 - val_mae: 446.7386\n",
      "Epoch 40/145\n",
      "63/63 - 0s - loss: 426.4358 - mae: 426.4358 - val_loss: 454.2772 - val_mae: 454.2772\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 0s - loss: 420.7963 - mae: 420.7963 - val_loss: 427.3680 - val_mae: 427.3680\n",
      "Epoch 42/145\n",
      "63/63 - 0s - loss: 414.2212 - mae: 414.2212 - val_loss: 423.4077 - val_mae: 423.4077\n",
      "Epoch 43/145\n",
      "63/63 - 0s - loss: 413.0356 - mae: 413.0356 - val_loss: 426.5247 - val_mae: 426.5247\n",
      "Epoch 44/145\n",
      "63/63 - 0s - loss: 413.6651 - mae: 413.6651 - val_loss: 421.6078 - val_mae: 421.6078\n",
      "Epoch 45/145\n",
      "63/63 - 0s - loss: 412.2630 - mae: 412.2630 - val_loss: 421.3914 - val_mae: 421.3914\n",
      "Epoch 46/145\n",
      "63/63 - 0s - loss: 410.7711 - mae: 410.7711 - val_loss: 421.0938 - val_mae: 421.0938\n",
      "Epoch 47/145\n",
      "63/63 - 0s - loss: 408.4830 - mae: 408.4830 - val_loss: 420.9197 - val_mae: 420.9197\n",
      "Epoch 48/145\n",
      "63/63 - 0s - loss: 410.6967 - mae: 410.6967 - val_loss: 426.6712 - val_mae: 426.6712\n",
      "Epoch 49/145\n",
      "63/63 - 0s - loss: 409.8478 - mae: 409.8478 - val_loss: 420.1865 - val_mae: 420.1865\n",
      "Epoch 50/145\n",
      "63/63 - 0s - loss: 407.3595 - mae: 407.3595 - val_loss: 422.1684 - val_mae: 422.1684\n",
      "Epoch 51/145\n",
      "63/63 - 0s - loss: 411.0640 - mae: 411.0640 - val_loss: 419.7084 - val_mae: 419.7084\n",
      "Epoch 52/145\n",
      "63/63 - 0s - loss: 408.2754 - mae: 408.2754 - val_loss: 420.8739 - val_mae: 420.8739\n",
      "Epoch 53/145\n",
      "63/63 - 0s - loss: 408.3195 - mae: 408.3195 - val_loss: 423.3792 - val_mae: 423.3792\n",
      "Epoch 54/145\n",
      "63/63 - 0s - loss: 407.2291 - mae: 407.2291 - val_loss: 421.0694 - val_mae: 421.0694\n",
      "Epoch 55/145\n",
      "63/63 - 0s - loss: 408.8717 - mae: 408.8717 - val_loss: 421.7818 - val_mae: 421.7818\n",
      "Epoch 56/145\n",
      "63/63 - 1s - loss: 409.1340 - mae: 409.1340 - val_loss: 422.6091 - val_mae: 422.6091\n",
      "Epoch 57/145\n",
      "63/63 - 1s - loss: 408.1741 - mae: 408.1741 - val_loss: 418.1239 - val_mae: 418.1239\n",
      "Epoch 58/145\n",
      "63/63 - 1s - loss: 405.0087 - mae: 405.0087 - val_loss: 421.1183 - val_mae: 421.1183\n",
      "Epoch 59/145\n",
      "63/63 - 0s - loss: 405.0209 - mae: 405.0209 - val_loss: 416.2260 - val_mae: 416.2260\n",
      "Epoch 60/145\n",
      "63/63 - 0s - loss: 403.6119 - mae: 403.6119 - val_loss: 419.0547 - val_mae: 419.0547\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 0s - loss: 396.9104 - mae: 396.9104 - val_loss: 416.9853 - val_mae: 416.9853\n",
      "Epoch 62/145\n",
      "63/63 - 0s - loss: 395.5103 - mae: 395.5103 - val_loss: 412.8674 - val_mae: 412.8674\n",
      "Epoch 63/145\n",
      "63/63 - 0s - loss: 394.9322 - mae: 394.9322 - val_loss: 420.5140 - val_mae: 420.5140\n",
      "Epoch 64/145\n",
      "63/63 - 0s - loss: 394.4325 - mae: 394.4325 - val_loss: 415.3509 - val_mae: 415.3509\n",
      "Epoch 65/145\n",
      "63/63 - 0s - loss: 392.5876 - mae: 392.5876 - val_loss: 414.2411 - val_mae: 414.2411\n",
      "Epoch 66/145\n",
      "63/63 - 0s - loss: 392.7891 - mae: 392.7891 - val_loss: 414.9227 - val_mae: 414.9227\n",
      "Epoch 67/145\n",
      "63/63 - 0s - loss: 393.8042 - mae: 393.8042 - val_loss: 427.7954 - val_mae: 427.7954\n",
      "Epoch 68/145\n",
      "63/63 - 1s - loss: 406.5615 - mae: 406.5615 - val_loss: 421.6201 - val_mae: 421.6201\n",
      "Epoch 69/145\n",
      "63/63 - 0s - loss: 391.3230 - mae: 391.3230 - val_loss: 411.3196 - val_mae: 411.3196\n",
      "Epoch 70/145\n",
      "63/63 - 0s - loss: 390.2749 - mae: 390.2749 - val_loss: 417.3677 - val_mae: 417.3677\n",
      "Epoch 71/145\n",
      "63/63 - 0s - loss: 392.1828 - mae: 392.1828 - val_loss: 411.6829 - val_mae: 411.6829\n",
      "Epoch 72/145\n",
      "63/63 - 0s - loss: 395.2466 - mae: 395.2466 - val_loss: 412.6879 - val_mae: 412.6879\n",
      "Epoch 73/145\n",
      "63/63 - 0s - loss: 387.5976 - mae: 387.5976 - val_loss: 412.4319 - val_mae: 412.4319\n",
      "Epoch 74/145\n",
      "63/63 - 0s - loss: 389.7184 - mae: 389.7184 - val_loss: 413.3668 - val_mae: 413.3668\n",
      "Epoch 75/145\n",
      "63/63 - 0s - loss: 387.4213 - mae: 387.4213 - val_loss: 415.3264 - val_mae: 415.3264\n",
      "Epoch 76/145\n",
      "63/63 - 0s - loss: 387.6006 - mae: 387.6006 - val_loss: 413.5765 - val_mae: 413.5765\n",
      "Epoch 77/145\n",
      "63/63 - 0s - loss: 387.0052 - mae: 387.0052 - val_loss: 411.6980 - val_mae: 411.6980\n",
      "Epoch 78/145\n",
      "63/63 - 0s - loss: 386.5806 - mae: 386.5806 - val_loss: 413.9365 - val_mae: 413.9365\n",
      "Epoch 79/145\n",
      "63/63 - 0s - loss: 388.1331 - mae: 388.1331 - val_loss: 414.2687 - val_mae: 414.2687\n",
      "Epoch 80/145\n",
      "63/63 - 0s - loss: 388.0613 - mae: 388.0613 - val_loss: 411.7947 - val_mae: 411.7947\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 0s - loss: 381.7196 - mae: 381.7196 - val_loss: 411.0975 - val_mae: 411.0975\n",
      "Epoch 82/145\n",
      "63/63 - 0s - loss: 382.3031 - mae: 382.3031 - val_loss: 410.7574 - val_mae: 410.7574\n",
      "Epoch 83/145\n",
      "63/63 - 0s - loss: 380.2271 - mae: 380.2271 - val_loss: 408.4548 - val_mae: 408.4548\n",
      "Epoch 84/145\n",
      "63/63 - 0s - loss: 380.9182 - mae: 380.9182 - val_loss: 409.8030 - val_mae: 409.8030\n",
      "Epoch 85/145\n",
      "63/63 - 1s - loss: 379.7005 - mae: 379.7005 - val_loss: 409.1736 - val_mae: 409.1736\n",
      "Epoch 86/145\n",
      "63/63 - 0s - loss: 379.4427 - mae: 379.4427 - val_loss: 407.5504 - val_mae: 407.5504\n",
      "Epoch 87/145\n",
      "63/63 - 0s - loss: 381.2529 - mae: 381.2529 - val_loss: 407.4659 - val_mae: 407.4659\n",
      "Epoch 88/145\n",
      "63/63 - 0s - loss: 378.4735 - mae: 378.4735 - val_loss: 407.3553 - val_mae: 407.3553\n",
      "Epoch 89/145\n",
      "63/63 - 0s - loss: 379.3828 - mae: 379.3828 - val_loss: 410.3073 - val_mae: 410.3073\n",
      "Epoch 90/145\n",
      "63/63 - 0s - loss: 379.9087 - mae: 379.9087 - val_loss: 410.3300 - val_mae: 410.3300\n",
      "Epoch 91/145\n",
      "63/63 - 0s - loss: 380.1962 - mae: 380.1962 - val_loss: 408.5369 - val_mae: 408.5369\n",
      "Epoch 92/145\n",
      "63/63 - 0s - loss: 377.6400 - mae: 377.6400 - val_loss: 406.4996 - val_mae: 406.4996\n",
      "Epoch 93/145\n",
      "63/63 - 0s - loss: 377.5631 - mae: 377.5631 - val_loss: 407.6172 - val_mae: 407.6172\n",
      "Epoch 94/145\n",
      "63/63 - 0s - loss: 377.7409 - mae: 377.7409 - val_loss: 410.1437 - val_mae: 410.1437\n",
      "Epoch 95/145\n",
      "63/63 - 0s - loss: 378.9142 - mae: 378.9142 - val_loss: 406.8307 - val_mae: 406.8307\n",
      "Epoch 96/145\n",
      "63/63 - 0s - loss: 376.7663 - mae: 376.7663 - val_loss: 410.9213 - val_mae: 410.9213\n",
      "Epoch 97/145\n",
      "63/63 - 0s - loss: 377.0404 - mae: 377.0404 - val_loss: 413.2690 - val_mae: 413.2690\n",
      "Epoch 98/145\n",
      "63/63 - 0s - loss: 378.3579 - mae: 378.3579 - val_loss: 410.4911 - val_mae: 410.4911\n",
      "Epoch 99/145\n",
      "63/63 - 0s - loss: 379.6350 - mae: 379.6350 - val_loss: 414.9784 - val_mae: 414.9784\n",
      "Epoch 100/145\n",
      "63/63 - 0s - loss: 376.5854 - mae: 376.5854 - val_loss: 406.5151 - val_mae: 406.5151\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 0s - loss: 373.0353 - mae: 373.0353 - val_loss: 406.3795 - val_mae: 406.3795\n",
      "Epoch 102/145\n",
      "63/63 - 0s - loss: 372.8262 - mae: 372.8262 - val_loss: 406.7648 - val_mae: 406.7648\n",
      "Epoch 103/145\n",
      "63/63 - 0s - loss: 371.7108 - mae: 371.7108 - val_loss: 407.3415 - val_mae: 407.3415\n",
      "Epoch 104/145\n",
      "63/63 - 0s - loss: 372.1447 - mae: 372.1447 - val_loss: 407.8908 - val_mae: 407.8908\n",
      "Epoch 105/145\n",
      "63/63 - 0s - loss: 372.1444 - mae: 372.1444 - val_loss: 406.6357 - val_mae: 406.6357\n",
      "Epoch 106/145\n",
      "63/63 - 0s - loss: 371.6423 - mae: 371.6423 - val_loss: 407.2750 - val_mae: 407.2750\n",
      "Epoch 107/145\n",
      "63/63 - 0s - loss: 371.3897 - mae: 371.3897 - val_loss: 406.2054 - val_mae: 406.2054\n",
      "Epoch 108/145\n",
      "63/63 - 0s - loss: 371.6970 - mae: 371.6970 - val_loss: 406.5264 - val_mae: 406.5264\n",
      "Epoch 109/145\n",
      "63/63 - 1s - loss: 371.3393 - mae: 371.3393 - val_loss: 406.1111 - val_mae: 406.1111\n",
      "Epoch 110/145\n",
      "63/63 - 1s - loss: 371.2793 - mae: 371.2793 - val_loss: 407.0910 - val_mae: 407.0910\n",
      "Epoch 111/145\n",
      "63/63 - 0s - loss: 371.3880 - mae: 371.3880 - val_loss: 406.3087 - val_mae: 406.3087\n",
      "Epoch 112/145\n",
      "63/63 - 0s - loss: 370.0676 - mae: 370.0676 - val_loss: 407.1550 - val_mae: 407.1550\n",
      "Epoch 113/145\n",
      "63/63 - 0s - loss: 370.8977 - mae: 370.8977 - val_loss: 407.4086 - val_mae: 407.4086\n",
      "Epoch 114/145\n",
      "63/63 - 0s - loss: 370.4211 - mae: 370.4211 - val_loss: 408.6984 - val_mae: 408.6984\n",
      "Epoch 115/145\n",
      "63/63 - 0s - loss: 370.0681 - mae: 370.0681 - val_loss: 406.4900 - val_mae: 406.4900\n",
      "Epoch 116/145\n",
      "63/63 - 0s - loss: 369.7770 - mae: 369.7770 - val_loss: 409.0582 - val_mae: 409.0582\n",
      "Epoch 117/145\n",
      "63/63 - 0s - loss: 369.9845 - mae: 369.9845 - val_loss: 408.1988 - val_mae: 408.1988\n",
      "Epoch 118/145\n",
      "63/63 - 0s - loss: 369.3650 - mae: 369.3650 - val_loss: 408.1154 - val_mae: 408.1154\n",
      "Epoch 119/145\n",
      "63/63 - 0s - loss: 369.0578 - mae: 369.0578 - val_loss: 405.8892 - val_mae: 405.8892\n",
      "Epoch 120/145\n",
      "63/63 - 0s - loss: 368.3492 - mae: 368.3492 - val_loss: 410.2676 - val_mae: 410.2676\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 1s - loss: 368.0293 - mae: 368.0293 - val_loss: 408.3536 - val_mae: 408.3536\n",
      "Epoch 122/145\n",
      "63/63 - 1s - loss: 366.7311 - mae: 366.7311 - val_loss: 406.2873 - val_mae: 406.2873\n",
      "Epoch 123/145\n",
      "63/63 - 0s - loss: 366.7268 - mae: 366.7268 - val_loss: 406.1666 - val_mae: 406.1666\n",
      "Epoch 124/145\n",
      "63/63 - 0s - loss: 366.8586 - mae: 366.8586 - val_loss: 407.2808 - val_mae: 407.2808\n",
      "Epoch 125/145\n",
      "63/63 - 0s - loss: 366.5315 - mae: 366.5315 - val_loss: 406.5215 - val_mae: 406.5215\n",
      "Epoch 126/145\n",
      "63/63 - 0s - loss: 366.6747 - mae: 366.6747 - val_loss: 407.6454 - val_mae: 407.6454\n",
      "Epoch 127/145\n",
      "63/63 - 0s - loss: 366.0396 - mae: 366.0396 - val_loss: 405.6934 - val_mae: 405.6934\n",
      "Epoch 128/145\n",
      "63/63 - 0s - loss: 365.7332 - mae: 365.7332 - val_loss: 406.6751 - val_mae: 406.6751\n",
      "Epoch 129/145\n",
      "63/63 - 0s - loss: 366.0093 - mae: 366.0093 - val_loss: 406.3912 - val_mae: 406.3912\n",
      "Epoch 130/145\n",
      "63/63 - 0s - loss: 366.1370 - mae: 366.1370 - val_loss: 406.5689 - val_mae: 406.5689\n",
      "Epoch 131/145\n",
      "63/63 - 0s - loss: 366.2929 - mae: 366.2929 - val_loss: 406.2051 - val_mae: 406.2051\n",
      "Epoch 132/145\n",
      "63/63 - 0s - loss: 365.6464 - mae: 365.6464 - val_loss: 405.5215 - val_mae: 405.5215\n",
      "Epoch 133/145\n",
      "63/63 - 1s - loss: 365.6108 - mae: 365.6108 - val_loss: 406.4520 - val_mae: 406.4520\n",
      "Epoch 134/145\n",
      "63/63 - 1s - loss: 365.4596 - mae: 365.4596 - val_loss: 406.3845 - val_mae: 406.3845\n",
      "Epoch 135/145\n",
      "63/63 - 0s - loss: 365.2216 - mae: 365.2216 - val_loss: 406.3239 - val_mae: 406.3239\n",
      "Epoch 136/145\n",
      "63/63 - 0s - loss: 364.7640 - mae: 364.7640 - val_loss: 406.4483 - val_mae: 406.4483\n",
      "Epoch 137/145\n",
      "63/63 - 0s - loss: 364.9136 - mae: 364.9136 - val_loss: 406.2195 - val_mae: 406.2195\n",
      "Epoch 138/145\n",
      "63/63 - 0s - loss: 364.7646 - mae: 364.7646 - val_loss: 407.3932 - val_mae: 407.3932\n",
      "Epoch 139/145\n",
      "63/63 - 0s - loss: 364.6220 - mae: 364.6220 - val_loss: 407.0062 - val_mae: 407.0062\n",
      "Epoch 140/145\n",
      "63/63 - 0s - loss: 364.9099 - mae: 364.9099 - val_loss: 406.4651 - val_mae: 406.4651\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 0s - loss: 363.6173 - mae: 363.6173 - val_loss: 406.5137 - val_mae: 406.5137\n",
      "Epoch 142/145\n",
      "63/63 - 0s - loss: 363.0158 - mae: 363.0158 - val_loss: 405.7125 - val_mae: 405.7125\n",
      "Epoch 143/145\n",
      "63/63 - 0s - loss: 363.1747 - mae: 363.1747 - val_loss: 405.8143 - val_mae: 405.8143\n",
      "Epoch 144/145\n",
      "63/63 - 0s - loss: 363.0557 - mae: 363.0557 - val_loss: 406.2422 - val_mae: 406.2422\n",
      "Epoch 145/145\n",
      "63/63 - 1s - loss: 363.1740 - mae: 363.1740 - val_loss: 405.8939 - val_mae: 405.8939\n",
      "\n",
      "val_mae is:405.89389040885925\n",
      "\n",
      "fold: 3\n",
      "X_train shape: (125000, 146)\n",
      "y_train shape: (125000,)\n",
      "X_val shape: (25000, 146)\n",
      "y_val shape: (25000,)\n",
      "Epoch 1/145\n",
      "63/63 - 1s - loss: 2531.4556 - mae: 2531.4556 - val_loss: 930.2685 - val_mae: 930.2685\n",
      "Epoch 2/145\n",
      "63/63 - 1s - loss: 767.4578 - mae: 767.4578 - val_loss: 700.9124 - val_mae: 700.9124\n",
      "Epoch 3/145\n",
      "63/63 - 1s - loss: 672.9771 - mae: 672.9771 - val_loss: 660.6374 - val_mae: 660.6374\n",
      "Epoch 4/145\n",
      "63/63 - 0s - loss: 619.1060 - mae: 619.1060 - val_loss: 587.8533 - val_mae: 587.8533\n",
      "Epoch 5/145\n",
      "63/63 - 0s - loss: 595.0123 - mae: 595.0123 - val_loss: 558.6432 - val_mae: 558.6432\n",
      "Epoch 6/145\n",
      "63/63 - 0s - loss: 578.9645 - mae: 578.9645 - val_loss: 549.4389 - val_mae: 549.4389\n",
      "Epoch 7/145\n",
      "63/63 - 0s - loss: 551.0563 - mae: 551.0563 - val_loss: 702.0438 - val_mae: 702.0438\n",
      "Epoch 8/145\n",
      "63/63 - 0s - loss: 595.6547 - mae: 595.6547 - val_loss: 557.8300 - val_mae: 557.8300\n",
      "Epoch 9/145\n",
      "63/63 - 0s - loss: 519.2133 - mae: 519.2133 - val_loss: 574.1730 - val_mae: 574.1730\n",
      "Epoch 10/145\n",
      "63/63 - 0s - loss: 507.8874 - mae: 507.8874 - val_loss: 503.7262 - val_mae: 503.7262\n",
      "Epoch 11/145\n",
      "63/63 - 1s - loss: 507.0130 - mae: 507.0130 - val_loss: 497.9277 - val_mae: 497.9277\n",
      "Epoch 12/145\n",
      "63/63 - 0s - loss: 558.6035 - mae: 558.6035 - val_loss: 658.4685 - val_mae: 658.4685\n",
      "Epoch 13/145\n",
      "63/63 - 0s - loss: 542.1315 - mae: 542.1315 - val_loss: 527.1816 - val_mae: 527.1816\n",
      "Epoch 14/145\n",
      "63/63 - 0s - loss: 489.9526 - mae: 489.9526 - val_loss: 560.9968 - val_mae: 560.9968\n",
      "Epoch 15/145\n",
      "63/63 - 0s - loss: 520.8566 - mae: 520.8566 - val_loss: 509.8431 - val_mae: 509.8431\n",
      "Epoch 16/145\n",
      "63/63 - 0s - loss: 528.1789 - mae: 528.1789 - val_loss: 495.8215 - val_mae: 495.8215\n",
      "Epoch 17/145\n",
      "63/63 - 0s - loss: 493.8049 - mae: 493.8049 - val_loss: 616.3459 - val_mae: 616.3459\n",
      "Epoch 18/145\n",
      "63/63 - 1s - loss: 502.0640 - mae: 502.0640 - val_loss: 477.8008 - val_mae: 477.8008\n",
      "Epoch 19/145\n",
      "63/63 - 0s - loss: 478.8669 - mae: 478.8669 - val_loss: 490.8731 - val_mae: 490.8731\n",
      "Epoch 20/145\n",
      "63/63 - 0s - loss: 472.5769 - mae: 472.5769 - val_loss: 591.7828 - val_mae: 591.7828\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 0s - loss: 467.1716 - mae: 467.1716 - val_loss: 460.8257 - val_mae: 460.8257\n",
      "Epoch 22/145\n",
      "63/63 - 1s - loss: 442.9609 - mae: 442.9609 - val_loss: 466.7946 - val_mae: 466.7946\n",
      "Epoch 23/145\n",
      "63/63 - 1s - loss: 443.3180 - mae: 443.3180 - val_loss: 466.0175 - val_mae: 466.0175\n",
      "Epoch 24/145\n",
      "63/63 - 1s - loss: 439.9781 - mae: 439.9781 - val_loss: 453.8472 - val_mae: 453.8472\n",
      "Epoch 25/145\n",
      "63/63 - 0s - loss: 442.5133 - mae: 442.5133 - val_loss: 450.2813 - val_mae: 450.2813\n",
      "Epoch 26/145\n",
      "63/63 - 0s - loss: 441.3549 - mae: 441.3549 - val_loss: 450.5300 - val_mae: 450.5300\n",
      "Epoch 27/145\n",
      "63/63 - 0s - loss: 454.1087 - mae: 454.1087 - val_loss: 449.9215 - val_mae: 449.9215\n",
      "Epoch 28/145\n",
      "63/63 - 0s - loss: 437.7550 - mae: 437.7550 - val_loss: 472.2703 - val_mae: 472.2703\n",
      "Epoch 29/145\n",
      "63/63 - 0s - loss: 448.7404 - mae: 448.7404 - val_loss: 460.8526 - val_mae: 460.8526\n",
      "Epoch 30/145\n",
      "63/63 - 0s - loss: 443.0910 - mae: 443.0910 - val_loss: 583.9575 - val_mae: 583.9575\n",
      "Epoch 31/145\n",
      "63/63 - 1s - loss: 464.7249 - mae: 464.7249 - val_loss: 462.0251 - val_mae: 462.0250\n",
      "Epoch 32/145\n",
      "63/63 - 1s - loss: 433.4164 - mae: 433.4164 - val_loss: 453.2604 - val_mae: 453.2604\n",
      "Epoch 33/145\n",
      "63/63 - 0s - loss: 432.9868 - mae: 432.9868 - val_loss: 449.1036 - val_mae: 449.1036\n",
      "Epoch 34/145\n",
      "63/63 - 0s - loss: 436.8175 - mae: 436.8175 - val_loss: 444.7365 - val_mae: 444.7365\n",
      "Epoch 35/145\n",
      "63/63 - 0s - loss: 439.3098 - mae: 439.3098 - val_loss: 442.8160 - val_mae: 442.8160\n",
      "Epoch 36/145\n",
      "63/63 - 0s - loss: 431.1063 - mae: 431.1063 - val_loss: 446.5490 - val_mae: 446.5490\n",
      "Epoch 37/145\n",
      "63/63 - 0s - loss: 429.0798 - mae: 429.0798 - val_loss: 449.7359 - val_mae: 449.7359\n",
      "Epoch 38/145\n",
      "63/63 - 0s - loss: 434.2180 - mae: 434.2180 - val_loss: 454.4214 - val_mae: 454.4214\n",
      "Epoch 39/145\n",
      "63/63 - 0s - loss: 429.6971 - mae: 429.6971 - val_loss: 450.1654 - val_mae: 450.1654\n",
      "Epoch 40/145\n",
      "63/63 - 1s - loss: 452.4828 - mae: 452.4828 - val_loss: 451.5874 - val_mae: 451.5874\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 1s - loss: 415.7477 - mae: 415.7477 - val_loss: 433.8570 - val_mae: 433.8570\n",
      "Epoch 42/145\n",
      "63/63 - 1s - loss: 411.9137 - mae: 411.9137 - val_loss: 433.3851 - val_mae: 433.3851\n",
      "Epoch 43/145\n",
      "63/63 - 0s - loss: 414.1641 - mae: 414.1641 - val_loss: 431.3654 - val_mae: 431.3654\n",
      "Epoch 44/145\n",
      "63/63 - 0s - loss: 411.0345 - mae: 411.0345 - val_loss: 432.2023 - val_mae: 432.2023\n",
      "Epoch 45/145\n",
      "63/63 - 1s - loss: 428.3344 - mae: 428.3344 - val_loss: 433.3338 - val_mae: 433.3338\n",
      "Epoch 46/145\n",
      "63/63 - 1s - loss: 410.4824 - mae: 410.4824 - val_loss: 433.1086 - val_mae: 433.1086\n",
      "Epoch 47/145\n",
      "63/63 - 1s - loss: 410.0946 - mae: 410.0946 - val_loss: 441.3596 - val_mae: 441.3596\n",
      "Epoch 48/145\n",
      "63/63 - 0s - loss: 409.5690 - mae: 409.5690 - val_loss: 431.6938 - val_mae: 431.6938\n",
      "Epoch 49/145\n",
      "63/63 - 0s - loss: 405.8788 - mae: 405.8788 - val_loss: 435.0382 - val_mae: 435.0382\n",
      "Epoch 50/145\n",
      "63/63 - 0s - loss: 408.5662 - mae: 408.5662 - val_loss: 429.5671 - val_mae: 429.5671\n",
      "Epoch 51/145\n",
      "63/63 - 0s - loss: 407.5659 - mae: 407.5659 - val_loss: 431.4021 - val_mae: 431.4021\n",
      "Epoch 52/145\n",
      "63/63 - 0s - loss: 406.1727 - mae: 406.1727 - val_loss: 432.2433 - val_mae: 432.2433\n",
      "Epoch 53/145\n",
      "63/63 - 0s - loss: 406.4867 - mae: 406.4867 - val_loss: 430.9221 - val_mae: 430.9221\n",
      "Epoch 54/145\n",
      "63/63 - 1s - loss: 404.9356 - mae: 404.9356 - val_loss: 444.0750 - val_mae: 444.0750\n",
      "Epoch 55/145\n",
      "63/63 - 1s - loss: 429.3154 - mae: 429.3154 - val_loss: 454.5088 - val_mae: 454.5088\n",
      "Epoch 56/145\n",
      "63/63 - 1s - loss: 418.3993 - mae: 418.3993 - val_loss: 431.8087 - val_mae: 431.8087\n",
      "Epoch 57/145\n",
      "63/63 - 0s - loss: 405.7287 - mae: 405.7287 - val_loss: 445.6691 - val_mae: 445.6691\n",
      "Epoch 58/145\n",
      "63/63 - 0s - loss: 415.8333 - mae: 415.8333 - val_loss: 428.7664 - val_mae: 428.7664\n",
      "Epoch 59/145\n",
      "63/63 - 0s - loss: 400.1320 - mae: 400.1320 - val_loss: 427.9480 - val_mae: 427.9480\n",
      "Epoch 60/145\n",
      "63/63 - 0s - loss: 398.8112 - mae: 398.8112 - val_loss: 435.9998 - val_mae: 435.9998\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 0s - loss: 391.9148 - mae: 391.9148 - val_loss: 424.3836 - val_mae: 424.3836\n",
      "Epoch 62/145\n",
      "63/63 - 0s - loss: 391.6746 - mae: 391.6746 - val_loss: 423.5610 - val_mae: 423.5610\n",
      "Epoch 63/145\n",
      "63/63 - 1s - loss: 391.5039 - mae: 391.5039 - val_loss: 424.3719 - val_mae: 424.3719\n",
      "Epoch 64/145\n",
      "63/63 - 1s - loss: 390.9269 - mae: 390.9269 - val_loss: 422.9058 - val_mae: 422.9058\n",
      "Epoch 65/145\n",
      "63/63 - 1s - loss: 390.8096 - mae: 390.8096 - val_loss: 427.7657 - val_mae: 427.7657\n",
      "Epoch 66/145\n",
      "63/63 - 1s - loss: 389.6724 - mae: 389.6724 - val_loss: 426.9925 - val_mae: 426.9925\n",
      "Epoch 67/145\n",
      "63/63 - 1s - loss: 391.2086 - mae: 391.2086 - val_loss: 425.8787 - val_mae: 425.8787\n",
      "Epoch 68/145\n",
      "63/63 - 0s - loss: 389.5964 - mae: 389.5964 - val_loss: 422.3373 - val_mae: 422.3373\n",
      "Epoch 69/145\n",
      "63/63 - 0s - loss: 389.5916 - mae: 389.5916 - val_loss: 429.5933 - val_mae: 429.5933\n",
      "Epoch 70/145\n",
      "63/63 - 0s - loss: 389.9158 - mae: 389.9158 - val_loss: 423.2082 - val_mae: 423.2082\n",
      "Epoch 71/145\n",
      "63/63 - 0s - loss: 390.6593 - mae: 390.6593 - val_loss: 433.8286 - val_mae: 433.8286\n",
      "Epoch 72/145\n",
      "63/63 - 0s - loss: 390.9200 - mae: 390.9200 - val_loss: 427.1416 - val_mae: 427.1416\n",
      "Epoch 73/145\n",
      "63/63 - 0s - loss: 386.8215 - mae: 386.8215 - val_loss: 424.8828 - val_mae: 424.8828\n",
      "Epoch 74/145\n",
      "63/63 - 0s - loss: 385.8065 - mae: 385.8065 - val_loss: 422.6734 - val_mae: 422.6734\n",
      "Epoch 75/145\n",
      "63/63 - 1s - loss: 386.4827 - mae: 386.4827 - val_loss: 428.0828 - val_mae: 428.0828\n",
      "Epoch 76/145\n",
      "63/63 - 0s - loss: 397.7438 - mae: 397.7438 - val_loss: 436.5475 - val_mae: 436.5475\n",
      "Epoch 77/145\n",
      "63/63 - 0s - loss: 395.2108 - mae: 395.2108 - val_loss: 421.7113 - val_mae: 421.7113\n",
      "Epoch 78/145\n",
      "63/63 - 1s - loss: 385.1072 - mae: 385.1072 - val_loss: 424.6891 - val_mae: 424.6891\n",
      "Epoch 79/145\n",
      "63/63 - 0s - loss: 385.9118 - mae: 385.9118 - val_loss: 421.2455 - val_mae: 421.2455\n",
      "Epoch 80/145\n",
      "63/63 - 0s - loss: 384.9999 - mae: 384.9999 - val_loss: 434.4127 - val_mae: 434.4127\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 1s - loss: 380.5698 - mae: 380.5698 - val_loss: 419.1345 - val_mae: 419.1345\n",
      "Epoch 82/145\n",
      "63/63 - 1s - loss: 378.8273 - mae: 378.8273 - val_loss: 419.1088 - val_mae: 419.1088\n",
      "Epoch 83/145\n",
      "63/63 - 1s - loss: 377.7858 - mae: 377.7858 - val_loss: 419.1161 - val_mae: 419.1161\n",
      "Epoch 84/145\n",
      "63/63 - 0s - loss: 378.1538 - mae: 378.1538 - val_loss: 419.8622 - val_mae: 419.8622\n",
      "Epoch 85/145\n",
      "63/63 - 0s - loss: 377.1928 - mae: 377.1928 - val_loss: 421.8084 - val_mae: 421.8084\n",
      "Epoch 86/145\n",
      "63/63 - 0s - loss: 378.2179 - mae: 378.2179 - val_loss: 420.2303 - val_mae: 420.2303\n",
      "Epoch 87/145\n",
      "63/63 - 0s - loss: 378.7781 - mae: 378.7781 - val_loss: 420.1978 - val_mae: 420.1978\n",
      "Epoch 88/145\n",
      "63/63 - 0s - loss: 377.5644 - mae: 377.5644 - val_loss: 419.5208 - val_mae: 419.5208\n",
      "Epoch 89/145\n",
      "63/63 - 1s - loss: 376.2174 - mae: 376.2174 - val_loss: 418.4566 - val_mae: 418.4566\n",
      "Epoch 90/145\n",
      "63/63 - 0s - loss: 374.7832 - mae: 374.7832 - val_loss: 417.6623 - val_mae: 417.6623\n",
      "Epoch 91/145\n",
      "63/63 - 0s - loss: 375.7047 - mae: 375.7047 - val_loss: 420.3654 - val_mae: 420.3654\n",
      "Epoch 92/145\n",
      "63/63 - 0s - loss: 376.2600 - mae: 376.2600 - val_loss: 418.7827 - val_mae: 418.7827\n",
      "Epoch 93/145\n",
      "63/63 - 1s - loss: 374.8362 - mae: 374.8362 - val_loss: 419.0211 - val_mae: 419.0211\n",
      "Epoch 94/145\n",
      "63/63 - 1s - loss: 373.7893 - mae: 373.7893 - val_loss: 419.7071 - val_mae: 419.7071\n",
      "Epoch 95/145\n",
      "63/63 - 1s - loss: 373.2896 - mae: 373.2896 - val_loss: 417.0744 - val_mae: 417.0744\n",
      "Epoch 96/145\n",
      "63/63 - 0s - loss: 374.7623 - mae: 374.7623 - val_loss: 417.4461 - val_mae: 417.4461\n",
      "Epoch 97/145\n",
      "63/63 - 1s - loss: 373.1497 - mae: 373.1497 - val_loss: 421.3302 - val_mae: 421.3302\n",
      "Epoch 98/145\n",
      "63/63 - 1s - loss: 373.3582 - mae: 373.3582 - val_loss: 419.0763 - val_mae: 419.0763\n",
      "Epoch 99/145\n",
      "63/63 - 1s - loss: 371.5839 - mae: 371.5839 - val_loss: 417.8972 - val_mae: 417.8972\n",
      "Epoch 100/145\n",
      "63/63 - 0s - loss: 372.3351 - mae: 372.3351 - val_loss: 417.9584 - val_mae: 417.9584\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 0s - loss: 369.6523 - mae: 369.6523 - val_loss: 417.9486 - val_mae: 417.9486\n",
      "Epoch 102/145\n",
      "63/63 - 1s - loss: 368.6105 - mae: 368.6105 - val_loss: 415.4476 - val_mae: 415.4476\n",
      "Epoch 103/145\n",
      "63/63 - 1s - loss: 368.4966 - mae: 368.4966 - val_loss: 416.2020 - val_mae: 416.2020\n",
      "Epoch 104/145\n",
      "63/63 - 1s - loss: 368.4101 - mae: 368.4101 - val_loss: 416.8088 - val_mae: 416.8088\n",
      "Epoch 105/145\n",
      "63/63 - 0s - loss: 367.8516 - mae: 367.8516 - val_loss: 416.6697 - val_mae: 416.6697\n",
      "Epoch 106/145\n",
      "63/63 - 0s - loss: 367.3619 - mae: 367.3619 - val_loss: 417.1547 - val_mae: 417.1547\n",
      "Epoch 107/145\n",
      "63/63 - 0s - loss: 367.4279 - mae: 367.4279 - val_loss: 415.8335 - val_mae: 415.8335\n",
      "Epoch 108/145\n",
      "63/63 - 0s - loss: 367.3422 - mae: 367.3422 - val_loss: 416.4440 - val_mae: 416.4440\n",
      "Epoch 109/145\n",
      "63/63 - 0s - loss: 367.0855 - mae: 367.0855 - val_loss: 417.7733 - val_mae: 417.7733\n",
      "Epoch 110/145\n",
      "63/63 - 0s - loss: 366.7516 - mae: 366.7516 - val_loss: 416.4512 - val_mae: 416.4512\n",
      "Epoch 111/145\n",
      "63/63 - 1s - loss: 366.6888 - mae: 366.6888 - val_loss: 417.0127 - val_mae: 417.0127\n",
      "Epoch 112/145\n",
      "63/63 - 1s - loss: 365.6715 - mae: 365.6715 - val_loss: 418.7609 - val_mae: 418.7609\n",
      "Epoch 113/145\n",
      "63/63 - 1s - loss: 366.3432 - mae: 366.3432 - val_loss: 416.8372 - val_mae: 416.8372\n",
      "Epoch 114/145\n",
      "63/63 - 0s - loss: 367.1429 - mae: 367.1429 - val_loss: 416.4889 - val_mae: 416.4889\n",
      "Epoch 115/145\n",
      "63/63 - 0s - loss: 366.4861 - mae: 366.4861 - val_loss: 415.2776 - val_mae: 415.2776\n",
      "Epoch 116/145\n",
      "63/63 - 0s - loss: 365.0921 - mae: 365.0921 - val_loss: 416.1172 - val_mae: 416.1172\n",
      "Epoch 117/145\n",
      "63/63 - 0s - loss: 364.8115 - mae: 364.8115 - val_loss: 415.6900 - val_mae: 415.6900\n",
      "Epoch 118/145\n",
      "63/63 - 0s - loss: 364.2800 - mae: 364.2800 - val_loss: 416.0311 - val_mae: 416.0311\n",
      "Epoch 119/145\n",
      "63/63 - 1s - loss: 365.1250 - mae: 365.1250 - val_loss: 416.4963 - val_mae: 416.4963\n",
      "Epoch 120/145\n",
      "63/63 - 0s - loss: 364.3683 - mae: 364.3683 - val_loss: 415.0992 - val_mae: 415.0992\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 0s - loss: 362.7438 - mae: 362.7438 - val_loss: 415.4757 - val_mae: 415.4757\n",
      "Epoch 122/145\n",
      "63/63 - 0s - loss: 361.9974 - mae: 361.9974 - val_loss: 415.5759 - val_mae: 415.5759\n",
      "Epoch 123/145\n",
      "63/63 - 1s - loss: 361.4975 - mae: 361.4975 - val_loss: 415.7784 - val_mae: 415.7784\n",
      "Epoch 124/145\n",
      "63/63 - 0s - loss: 361.8966 - mae: 361.8966 - val_loss: 414.8815 - val_mae: 414.8815\n",
      "Epoch 125/145\n",
      "63/63 - 0s - loss: 361.2491 - mae: 361.2491 - val_loss: 415.3564 - val_mae: 415.3564\n",
      "Epoch 126/145\n",
      "63/63 - 0s - loss: 361.5816 - mae: 361.5816 - val_loss: 414.9987 - val_mae: 414.9987\n",
      "Epoch 127/145\n",
      "63/63 - 0s - loss: 361.1516 - mae: 361.1516 - val_loss: 415.3340 - val_mae: 415.3340\n",
      "Epoch 128/145\n",
      "63/63 - 0s - loss: 361.0261 - mae: 361.0261 - val_loss: 414.9377 - val_mae: 414.9377\n",
      "Epoch 129/145\n",
      "63/63 - 1s - loss: 360.4642 - mae: 360.4642 - val_loss: 414.6042 - val_mae: 414.6042\n",
      "Epoch 130/145\n",
      "63/63 - 1s - loss: 360.3410 - mae: 360.3410 - val_loss: 415.2934 - val_mae: 415.2934\n",
      "Epoch 131/145\n",
      "63/63 - 0s - loss: 360.6944 - mae: 360.6944 - val_loss: 414.6766 - val_mae: 414.6766\n",
      "Epoch 132/145\n",
      "63/63 - 0s - loss: 360.3181 - mae: 360.3181 - val_loss: 415.1160 - val_mae: 415.1160\n",
      "Epoch 133/145\n",
      "63/63 - 0s - loss: 359.9726 - mae: 359.9726 - val_loss: 416.6493 - val_mae: 416.6493\n",
      "Epoch 134/145\n",
      "63/63 - 0s - loss: 359.8832 - mae: 359.8832 - val_loss: 415.2147 - val_mae: 415.2147\n",
      "Epoch 135/145\n",
      "63/63 - 0s - loss: 359.9493 - mae: 359.9493 - val_loss: 414.7636 - val_mae: 414.7636\n",
      "Epoch 136/145\n",
      "63/63 - 0s - loss: 359.7312 - mae: 359.7312 - val_loss: 416.0260 - val_mae: 416.0260\n",
      "Epoch 137/145\n",
      "63/63 - 1s - loss: 360.1621 - mae: 360.1621 - val_loss: 414.4870 - val_mae: 414.4870\n",
      "Epoch 138/145\n",
      "63/63 - 0s - loss: 359.8663 - mae: 359.8663 - val_loss: 415.8549 - val_mae: 415.8549\n",
      "Epoch 139/145\n",
      "63/63 - 0s - loss: 359.0115 - mae: 359.0115 - val_loss: 415.1658 - val_mae: 415.1658\n",
      "Epoch 140/145\n",
      "63/63 - 0s - loss: 359.0403 - mae: 359.0403 - val_loss: 415.7366 - val_mae: 415.7366\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 0s - loss: 358.0228 - mae: 358.0228 - val_loss: 414.5443 - val_mae: 414.5443\n",
      "Epoch 142/145\n",
      "63/63 - 0s - loss: 357.4435 - mae: 357.4435 - val_loss: 414.7316 - val_mae: 414.7316\n",
      "Epoch 143/145\n",
      "63/63 - 1s - loss: 357.6202 - mae: 357.6202 - val_loss: 414.8813 - val_mae: 414.8813\n",
      "Epoch 144/145\n",
      "63/63 - 0s - loss: 357.1096 - mae: 357.1096 - val_loss: 414.6907 - val_mae: 414.6907\n",
      "Epoch 145/145\n",
      "63/63 - 0s - loss: 357.2737 - mae: 357.2737 - val_loss: 414.8862 - val_mae: 414.8862\n",
      "\n",
      "val_mae is:414.8861972711182\n",
      "\n",
      "fold: 4\n",
      "X_train shape: (125000, 146)\n",
      "y_train shape: (125000,)\n",
      "X_val shape: (25000, 146)\n",
      "y_val shape: (25000,)\n",
      "Epoch 1/145\n",
      "63/63 - 2s - loss: 2695.3408 - mae: 2695.3408 - val_loss: 930.2769 - val_mae: 930.2769\n",
      "Epoch 2/145\n",
      "63/63 - 0s - loss: 802.8652 - mae: 802.8652 - val_loss: 705.8276 - val_mae: 705.8276\n",
      "Epoch 3/145\n",
      "63/63 - 1s - loss: 683.6895 - mae: 683.6895 - val_loss: 762.8995 - val_mae: 762.8995\n",
      "Epoch 4/145\n",
      "63/63 - 0s - loss: 657.8894 - mae: 657.8894 - val_loss: 621.4523 - val_mae: 621.4523\n",
      "Epoch 5/145\n",
      "63/63 - 0s - loss: 579.0417 - mae: 579.0417 - val_loss: 655.1327 - val_mae: 655.1327\n",
      "Epoch 6/145\n",
      "63/63 - 0s - loss: 559.0083 - mae: 559.0083 - val_loss: 562.2577 - val_mae: 562.2577\n",
      "Epoch 7/145\n",
      "63/63 - 0s - loss: 536.9224 - mae: 536.9224 - val_loss: 513.3889 - val_mae: 513.3889\n",
      "Epoch 8/145\n",
      "63/63 - 0s - loss: 518.9767 - mae: 518.9767 - val_loss: 526.0699 - val_mae: 526.0699\n",
      "Epoch 9/145\n",
      "63/63 - 0s - loss: 515.1666 - mae: 515.1666 - val_loss: 515.9343 - val_mae: 515.9343\n",
      "Epoch 10/145\n",
      "63/63 - 0s - loss: 534.4733 - mae: 534.4733 - val_loss: 490.5325 - val_mae: 490.5325\n",
      "Epoch 11/145\n",
      "63/63 - 0s - loss: 569.2864 - mae: 569.2864 - val_loss: 588.7605 - val_mae: 588.7605\n",
      "Epoch 12/145\n",
      "63/63 - 0s - loss: 519.2358 - mae: 519.2358 - val_loss: 484.8375 - val_mae: 484.8375\n",
      "Epoch 13/145\n",
      "63/63 - 0s - loss: 487.9098 - mae: 487.9098 - val_loss: 488.2092 - val_mae: 488.2092\n",
      "Epoch 14/145\n",
      "63/63 - 1s - loss: 490.8151 - mae: 490.8151 - val_loss: 483.6229 - val_mae: 483.6229\n",
      "Epoch 15/145\n",
      "63/63 - 0s - loss: 502.6461 - mae: 502.6461 - val_loss: 673.2281 - val_mae: 673.2281\n",
      "Epoch 16/145\n",
      "63/63 - 0s - loss: 537.8382 - mae: 537.8382 - val_loss: 474.1591 - val_mae: 474.1591\n",
      "Epoch 17/145\n",
      "63/63 - 0s - loss: 487.6316 - mae: 487.6316 - val_loss: 470.1555 - val_mae: 470.1555\n",
      "Epoch 18/145\n",
      "63/63 - 0s - loss: 467.7318 - mae: 467.7318 - val_loss: 464.3732 - val_mae: 464.3732\n",
      "Epoch 19/145\n",
      "63/63 - 0s - loss: 470.9773 - mae: 470.9773 - val_loss: 462.9726 - val_mae: 462.9726\n",
      "Epoch 20/145\n",
      "63/63 - 0s - loss: 497.5365 - mae: 497.5365 - val_loss: 492.2391 - val_mae: 492.2391\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 0s - loss: 451.6372 - mae: 451.6372 - val_loss: 456.7160 - val_mae: 456.7160\n",
      "Epoch 22/145\n",
      "63/63 - 1s - loss: 438.7016 - mae: 438.7016 - val_loss: 448.2666 - val_mae: 448.2666\n",
      "Epoch 23/145\n",
      "63/63 - 0s - loss: 442.3683 - mae: 442.3683 - val_loss: 445.1717 - val_mae: 445.1717\n",
      "Epoch 24/145\n",
      "63/63 - 0s - loss: 437.6275 - mae: 437.6275 - val_loss: 441.2184 - val_mae: 441.2184\n",
      "Epoch 25/145\n",
      "63/63 - 0s - loss: 437.7911 - mae: 437.7911 - val_loss: 442.2295 - val_mae: 442.2295\n",
      "Epoch 26/145\n",
      "63/63 - 1s - loss: 436.5673 - mae: 436.5673 - val_loss: 460.3521 - val_mae: 460.3521\n",
      "Epoch 27/145\n",
      "63/63 - 0s - loss: 436.9818 - mae: 436.9818 - val_loss: 448.7675 - val_mae: 448.7675\n",
      "Epoch 28/145\n",
      "63/63 - 0s - loss: 435.8164 - mae: 435.8164 - val_loss: 458.0979 - val_mae: 458.0979\n",
      "Epoch 29/145\n",
      "63/63 - 0s - loss: 446.1346 - mae: 446.1346 - val_loss: 478.7701 - val_mae: 478.7701\n",
      "Epoch 30/145\n",
      "63/63 - 1s - loss: 434.1503 - mae: 434.1503 - val_loss: 438.6531 - val_mae: 438.6531\n",
      "Epoch 31/145\n",
      "63/63 - 1s - loss: 429.4790 - mae: 429.4790 - val_loss: 443.0733 - val_mae: 443.0733\n",
      "Epoch 32/145\n",
      "63/63 - 0s - loss: 432.2618 - mae: 432.2618 - val_loss: 448.5574 - val_mae: 448.5574\n",
      "Epoch 33/145\n",
      "63/63 - 0s - loss: 466.6056 - mae: 466.6056 - val_loss: 505.3218 - val_mae: 505.3218\n",
      "Epoch 34/145\n",
      "63/63 - 0s - loss: 458.9716 - mae: 458.9716 - val_loss: 459.9514 - val_mae: 459.9514\n",
      "Epoch 35/145\n",
      "63/63 - 0s - loss: 426.9999 - mae: 426.9999 - val_loss: 440.3117 - val_mae: 440.3117\n",
      "Epoch 36/145\n",
      "63/63 - 0s - loss: 429.0345 - mae: 429.0345 - val_loss: 440.1388 - val_mae: 440.1388\n",
      "Epoch 37/145\n",
      "63/63 - 0s - loss: 441.1645 - mae: 441.1645 - val_loss: 438.7693 - val_mae: 438.7693\n",
      "Epoch 38/145\n",
      "63/63 - 1s - loss: 421.9257 - mae: 421.9257 - val_loss: 452.8204 - val_mae: 452.8204\n",
      "Epoch 39/145\n"
     ]
    }
   ],
   "source": [
    "n_splits = 6\n",
    "kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "import keras \n",
    "\n",
    "b_size = 2000\n",
    "max_epochs = 145\n",
    "oof_pred = np.zeros((len(X_pca), ))\n",
    "\n",
    "sub = pd.read_csv('./data/used_car_testB_20200421.csv',sep = ' ')[['SaleID']].copy()\n",
    "sub['price'] = 0\n",
    "reduce_lr = LearningRateScheduler(scheduler)\n",
    "\n",
    "avg_mae = 0\n",
    "for fold, (trn_idx, val_idx) in enumerate(kf.split(X_pca, y)):\n",
    "    print('fold:', fold)\n",
    "    X_train, y_train = X_pca[trn_idx], y[trn_idx]\n",
    "    X_val, y_val = X_pca[val_idx], y[val_idx]\n",
    "    \n",
    "    model = NN_model(X_train.shape[1])\n",
    "    simple_adam = Adam(lr = 0.015)\n",
    "    model.compile(loss='mae', optimizer=simple_adam,metrics=['mae'])\n",
    "    es = EarlyStopping(monitor='val_score', patience=10, verbose=2, mode='min', restore_best_weights=True,)\n",
    "    es.set_model(model)\n",
    "    metric = Metric(model, [es], [(X_train, y_train), (X_val, y_val)])\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    print(\"X_val shape:\", X_val.shape)\n",
    "    print(\"y_val shape:\", y_val.shape)\n",
    "    model.fit(X_train, y_train, batch_size=b_size, epochs=max_epochs, \n",
    "          validation_data=(X_val, y_val),  # Use a tuple here\n",
    "          callbacks=[reduce_lr], shuffle=True, verbose=2)\n",
    "    y_pred3 = model.predict(X_val)\n",
    "    y_pred = np.zeros((len(y_pred3), ))\n",
    "    sub['price'] += model.predict(test).reshape(-1,)/n_splits\n",
    "    for i in range(len(y_pred3)):\n",
    "        y_pred[i] = y_pred3[i]\n",
    "        \n",
    "    oof_pred[val_idx] = y_pred\n",
    "    val_mae = mean_absolute_error(y[val_idx], y_pred)\n",
    "    avg_mae += val_mae/n_splits\n",
    "    print()\n",
    "    print('val_mae is:{}'.format(val_mae))\n",
    "    print()\n",
    "mean_absolute_error(y, oof_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-16T23:45:18.039130600Z"
    }
   },
   "id": "2ff52be0a8bc6f4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub.to_csv('nn_sub_{}_{}.csv'.format('mae', sub['price'].mean()), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d6e978e8c477656e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a17d37c98adde4f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
